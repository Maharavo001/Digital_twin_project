{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847381ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ctown_lstm_tf.py\n",
    "import pandas as pd, numpy as np, tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "CSV_PATH = \"./dataset/leakage/ctown_leak_dataset.csv\"\n",
    "WINDOW = 12          # 12 steps (e.g., 1h if 5-min step)\n",
    "STRIDE = 3\n",
    "BATCH = 64\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "RANDOM = 42\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "# recover/ensure time order\n",
    "leak_df = df[df.leak == 1]\n",
    "noleak_df = df[df.leak == 0].sample(len(leak_df), random_state=42)\n",
    "balanced = pd.concat([leak_df, noleak_df]).sample(frac=1, random_state=42)\n",
    "df = balanced.copy()\n",
    "\n",
    "if 'Unnamed: 0' in df.columns: df = df.rename(columns={'Unnamed: 0':'t'})\n",
    "if 't' not in df.columns:\n",
    "    df['t'] = df.groupby('scenario_id').cumcount()\n",
    "\n",
    "# Identify boundaries where a new scenario begins\n",
    "meta_cols = [\"leak\", \"node\", \"area\", \"Cd\", \"tstart\", \"tend\", \"bin\"]\n",
    "# each unique combo of these = one scenario\n",
    "# df[\"scenario_id\"] = (df[meta_cols] != df[meta_cols].shift()).any(axis=1).cumsum()\n",
    "\n",
    "feat_cols = [c for c in df.columns if c.startswith(('P_','Q_','H_'))]\n",
    "label_col = 'leak'\n",
    "\n",
    "\n",
    "# split by scenario_id (no leakage)\n",
    "sc_ids = df['scenario_id'].dropna().unique()\n",
    "tr_ids, te_ids = train_test_split(sc_ids, test_size=0.2, random_state=RANDOM, shuffle=True)\n",
    "tr_ids, va_ids = train_test_split(tr_ids, test_size=0.2, random_state=RANDOM, shuffle=True)\n",
    "\n",
    "def make_windows(sub):\n",
    "    sub = sub.sort_values('t')\n",
    "    X, y = [], []\n",
    "    Xraw, yraw = sub[feat_cols].values, sub[label_col].values\n",
    "    for i in range(0, len(sub)-WINDOW+1, STRIDE):\n",
    "        X.append(Xraw[i:i+WINDOW])\n",
    "        y.append(yraw[i+WINDOW-1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def pack(ids):\n",
    "    partsX, partsy = [], []\n",
    "    for sid in ids:\n",
    "        Xc, yc = make_windows(df[df.scenario_id==sid])\n",
    "        if len(Xc): partsX.append(Xc); partsy.append(yc)\n",
    "    return np.vstack(partsX), np.hstack(partsy)\n",
    "\n",
    "Xtr, ytr = pack(tr_ids); Xva, yva = pack(va_ids); Xte, yte = pack(te_ids)\n",
    "\n",
    "# scale features (fit on train only)\n",
    "scaler = StandardScaler().fit(Xtr.reshape(-1, Xtr.shape[-1]))\n",
    "def scale(X): return scaler.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
    "Xtr, Xva, Xte = scale(Xtr), scale(Xva), scale(Xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e8787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# build model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(WINDOW, Xtr.shape[-1])),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(LR),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc'),\n",
    "                       tf.keras.metrics.Precision(name='prec'),\n",
    "                       tf.keras.metrics.Recall(name='rec')])\n",
    "\n",
    "cb = [tf.keras.callbacks.EarlyStopping(monitor='val_auc', mode='max',\n",
    "                                       patience=5, restore_best_weights=True)]\n",
    "model.fit(Xtr, ytr, validation_data=(Xva, yva),\n",
    "          epochs=EPOCHS, batch_size=BATCH, callbacks=cb, verbose=1)\n",
    "\n",
    "print(\"\\nTest metrics:\", model.evaluate(Xte, yte, batch_size=BATCH, verbose=0))\n",
    "# optional: save\n",
    "model.save(\"./model_trained/leakage_lstm_1.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6503ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build stacked LSTM/GRU model\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks, metrics\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(WINDOW, Xtr.shape[-1])),\n",
    "\n",
    "    # Stacked recurrent layers\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.LSTM(64, return_sequences=False),  # last layer outputs sequence summary\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    # Or try GRUs instead:\n",
    "    # layers.GRU(128, return_sequences=True),\n",
    "    # layers.Dropout(0.3),\n",
    "    # layers.GRU(64, return_sequences=False),\n",
    "    # layers.Dropout(0.3),\n",
    "\n",
    "    # Dense head\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(LR),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        metrics.AUC(name=\"auc\"),\n",
    "        metrics.Precision(name=\"prec\"),\n",
    "        metrics.Recall(name=\"rec\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "cb = [callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\",\n",
    "                              patience=6, restore_best_weights=True)]\n",
    "model.fit(Xtr, ytr, validation_data=(Xva, yva),\n",
    "          epochs=EPOCHS, batch_size=BATCH, callbacks=cb, verbose=1)\n",
    "\n",
    "print(\"\\nTest metrics:\", model.evaluate(Xte, yte, batch_size=BATCH, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabdb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, callbacks, optimizers, metrics\n",
    "\n",
    "# Stacked GRU (using previous dataset variables)\n",
    "model = models.Sequential()\n",
    "model.add(layers.GRU(64, return_sequences=True, input_shape=(Xtr.shape[1], Xtr.shape[2])))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.GRU(32, return_sequences=False))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        metrics.AUC(name=\"auc\"),\n",
    "        metrics.Precision(name=\"prec\"),\n",
    "        metrics.Recall(name=\"rec\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor=\"val_auc\", patience=6, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    Xtr, ytr,\n",
    "    validation_data=(Xva, yva),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_metrics = model.evaluate(Xte, yte, verbose=0)\n",
    "print(\"Test metrics:\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aca982",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model_trained/leakage_gru_1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc95dd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file = os.listdir('/home/zayd/Desktop/Digital_twin_project/machine_learning/model_trained/LightGBM_0.0.2_moving_avg')\n",
    "print(len(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aeb302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 11:49:34,813 - INFO - Processing junction J511\n",
      "2025-08-26 11:49:34,941 - INFO - Merged J511 from ./dataset/junction_elevation-0-static_input.parquet\n",
      "2025-08-26 11:49:35,114 - INFO - Merged node J503 for junction J511\n",
      "2025-08-26 11:49:36,420 - INFO - Merged node J580 for junction J511\n",
      "2025-08-26 11:49:38,242 - INFO - Merged pipe P349 for junction J511\n",
      "2025-08-26 11:49:40,039 - INFO - Merged pipe P524 for junction J511\n",
      "2025-08-26 11:49:42,082 - INFO - Merged pipe P349 for junction J511\n",
      "2025-08-26 11:49:43,768 - INFO - Merged pipe P524 for junction J511\n",
      "2025-08-26 11:49:45,944 - INFO - Merged pipe P349 for junction J511\n",
      "2025-08-26 11:49:48,779 - INFO - Merged pipe P524 for junction J511\n",
      "2025-08-26 11:49:51,929 - INFO - Merged pipe P349 for junction J511\n",
      "2025-08-26 11:49:56,946 - INFO - Merged pipe P524 for junction J511\n",
      "2025-08-26 11:50:02,770 - INFO - Merged pipe P349 for junction J511\n",
      "2025-08-26 11:50:07,479 - INFO - Merged pipe P524 for junction J511\n",
      "2025-08-26 11:50:17,698 - INFO - J511 saved to ./dataset/junctions/J511.parquet\n",
      "2025-08-26 11:50:17,797 - INFO - Processing junction J411\n",
      "2025-08-26 11:50:17,964 - INFO - Merged J411 from ./dataset/junction_elevation-0-static_input.parquet\n",
      "2025-08-26 11:50:18,271 - INFO - Merged node J1025 for junction J411\n",
      "2025-08-26 11:50:19,825 - INFO - Merged node J1056 for junction J411\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "# Load main data\n",
    "demand = pd.read_parquet('./dataset/junction_base_demand-0-dynamic_input.parquet')\n",
    "statics = [\n",
    "    './dataset/junction_elevation-0-static_input.parquet',\n",
    "    './dataset/pipe_diameter-0-static_input.parquet',\n",
    "    './dataset/pipe_initial_status-0-static_input.parquet',\n",
    "    './dataset/pipe_length-0-static_input.parquet',\n",
    "    './dataset/pipe_minor_loss-0-static_input.parquet',\n",
    "    './dataset/pipe_roughness-0-static_input.parquet'\n",
    "]\n",
    "links = pd.read_parquet('./dataset/pipes.parquet')\n",
    "\n",
    "# Ensure output folder exists\n",
    "Path(\"./dataset/junctions/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for junction in demand.columns[demand.columns.str.startswith(\"J\")]:\n",
    "    demnd = demand[['scenario_id', junction, 'time_id']]\n",
    "    logging.info(f\"Processing junction {junction}\")\n",
    "\n",
    "    for static_path in statics:\n",
    "        stat = pd.read_parquet(static_path)\n",
    "        if junction in stat.columns:\n",
    "            stat = stat[[\"scenario_id\", junction]].rename(columns={junction: f'{junction}_elevation'})\n",
    "            demnd = pd.merge(demnd, stat, on=[\"scenario_id\"], how=\"inner\")\n",
    "            logging.info(f\"Merged {junction} from {static_path}\")\n",
    "\n",
    "            near_nodes = [node[\"start\"] for idx , node in links.iterrows() if node['end'] == junction]\n",
    "            near_nodes += [node[\"end\"] for idx , node in links.iterrows() if node['start'] == junction]\n",
    "            for node in near_nodes:\n",
    "                if node in demand.columns:\n",
    "                    node_demands = demand[[\"scenario_id\", node]].rename(columns={node: f'{node}_demand'})\n",
    "                    demnd = pd.merge(demnd, node_demands, on=[\"scenario_id\"], how=\"inner\")\n",
    "                    logging.info(f\"Merged node {node} for junction {junction}\")\n",
    "                else:\n",
    "                    logging.warning(f\"Merge impossible for node {node} and junction {junction}\")            \n",
    "        else:\n",
    "            pipes = [row[\"pipe_id\"] for idx, row in links.iterrows()\n",
    "                     if row[\"start\"] == junction or row[\"end\"] == junction]            \n",
    "            for pipe in pipes:\n",
    "                if pipe in stat.columns:\n",
    "                    #####################################\n",
    "                    # Get new columns name and rename it: \n",
    "                    #####################################\n",
    "                    filename = os.path.basename(static_path)  \n",
    "                    variable_name = filename.split('-')[0]  \n",
    "                    variable_name = variable_name.replace(\"pipe_\", \"\")\n",
    "                    stat_pipe = stat[[\"scenario_id\", pipe]].rename(columns={pipe: f'{pipe}_{variable_name}'})\n",
    "                    ########################\n",
    "                    # Merging the dataframes\n",
    "                    ########################\n",
    "                    demnd = pd.merge(demnd, stat_pipe, on=[\"scenario_id\"], how=\"inner\")\n",
    "                    logging.info(f\"Merged pipe {pipe} for junction {junction}\")\n",
    "                else:\n",
    "                    logging.warning(f\"Merge impossible for pipe {pipe} and junction {junction}\")\n",
    "\n",
    "\n",
    "    output_path = f\"./dataset/junctions/{junction}.parquet\"\n",
    "    demnd.to_parquet(output_path, engine=\"pyarrow\", index=False)\n",
    "    logging.info(f\"{junction} saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digitaltwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
