{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9cffc9a",
   "metadata": {},
   "source": [
    "LTSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb3967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 12:03:51.542006: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-24 12:03:51.870931: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-24 12:03:52.239323: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756033432.548534   43681 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756033432.648210   43681 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756033433.350599   43681 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756033433.350626   43681 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756033433.350629   43681 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756033433.350633   43681 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-24 12:03:53.448757: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/zayd/.config/matplotlib is not a writable directory\n",
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-e9_afcqr because there was an issue with the default path (/home/zayd/.config/matplotlib); it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "Matplotlib is building the font cache; this may take a moment.\n",
      "2025-08-24 12:04:34.807276: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 76ms/step - loss: 37638.3867 - val_loss: 218634.2188\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 50042.9297 - val_loss: 59047.2812\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 13630.9502 - val_loss: 234.5295\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 158.8276 - val_loss: 417.4804\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 377.4199 - val_loss: 50.7521\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 122.7127 - val_loss: 162.9412\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 170.8882 - val_loss: 8.5410\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 19.9097 - val_loss: 65.8870\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 19.8858 - val_loss: 1.8814\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 8.8540 - val_loss: 2.7085\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 13.7027 - val_loss: 1.5567\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 7.4594 - val_loss: 1.4751\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 8.1647 - val_loss: 1.3580\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 6.4340 - val_loss: 1.2688\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 5.9610 - val_loss: 1.2244\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 6.4070 - val_loss: 1.1680\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 6.0768 - val_loss: 1.1654\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 5.1461 - val_loss: 1.2275\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 5.9042 - val_loss: 1.3222\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - loss: 6.3907 - val_loss: 1.6534\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 6.8992 - val_loss: 1.8321\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 4.1250 - val_loss: 2.2291\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 4.7848 - val_loss: 2.7671\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 4.7717 - val_loss: 3.5011\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 3.5460 - val_loss: 3.8780\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 2.0455 - val_loss: 3.7422\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 2.1958 - val_loss: 4.8039\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 2.0357 - val_loss: 4.8360\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1.6243 - val_loss: 2.6841\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 1.0273 - val_loss: 2.8690\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.9682 - val_loss: 4.0824\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 1.5274 - val_loss: 4.1296\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 1.0434 - val_loss: 3.2157\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 0.9583 - val_loss: 2.3016\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.9888 - val_loss: 2.3365\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 1.0217 - val_loss: 2.5500\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 1.3825 - val_loss: 2.5632\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 1.4682 - val_loss: 3.1603\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.9051 - val_loss: 2.8009\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.8964 - val_loss: 1.8876\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.7622 - val_loss: 2.3747\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.8262 - val_loss: 2.3902\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.6266 - val_loss: 2.3620\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.8553 - val_loss: 2.4661\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.6391 - val_loss: 2.1070\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.9373 - val_loss: 2.5443\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.7764 - val_loss: 3.9424\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.6121 - val_loss: 1.8720\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.7571 - val_loss: 3.7257\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.7216 - val_loss: 1.5444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43681/1802571160.py:183: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
      "2025/08/24 12:05:42 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0046\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J434' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J434'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - loss: 25815.7285 - val_loss: 24431.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 4489.5435 - val_loss: 1456.2429\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 386.6884 - val_loss: 153.3228\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 109.5299 - val_loss: 83.6553\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 57.8025 - val_loss: 26.0967\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 44.4053 - val_loss: 65.0845\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 29.9839 - val_loss: 104.4928\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 39.8296 - val_loss: 0.9106\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 14.9185 - val_loss: 59.7776\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 37.4869 - val_loss: 3.4653\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 10.3664 - val_loss: 20.7005\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 8.3867 - val_loss: 0.1899\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 9.0877 - val_loss: 3.1168\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 7.0667 - val_loss: 36.2818\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 31.5612 - val_loss: 20.8100\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 5.9430 - val_loss: 1.8993\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 2.7055 - val_loss: 1.3977\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 5.2031 - val_loss: 1.7107\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 15.8870 - val_loss: 35.2950\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 10.7376 - val_loss: 35.2788\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 136.9581 - val_loss: 226.2216\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 25.9272 - val_loss: 9.6240\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 10.2129 - val_loss: 7.7775\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 8.8302 - val_loss: 0.8222\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 4.5881 - val_loss: 6.9214\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 4.7598 - val_loss: 1.9154\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 5.6255 - val_loss: 0.3846\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 5.4106 - val_loss: 1.9178\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 3.5062 - val_loss: 0.9643\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 3.1230 - val_loss: 1.5456\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 6.0958 - val_loss: 0.1077\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 3.2207 - val_loss: 5.8920\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 4.1748 - val_loss: 1.0934\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 2.4836 - val_loss: 1.1416\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 2.1369 - val_loss: 17.8524\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 30.7524 - val_loss: 0.1394\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 6.3438 - val_loss: 2.3462\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 4.4728 - val_loss: 1.4166\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 9.8106 - val_loss: 116.9875\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 55.1449 - val_loss: 33.9451\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 7.9186 - val_loss: 6.3779\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 5.1477 - val_loss: 0.3433\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 5.7595 - val_loss: 2.3941\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - loss: 7.6979 - val_loss: 1.6039\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 4.4919 - val_loss: 4.1247\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 2.4521 - val_loss: 1.0333\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 2.5035 - val_loss: 0.1290\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 2.2276 - val_loss: 0.1603\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 2.3901 - val_loss: 0.2344\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 1.7784 - val_loss: 0.3864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/7\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step WARNING:tensorflow:5 out of the last 16 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x703a6d202340> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 16 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x703a6d202340> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 167ms/step\n",
      "MAE: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:07:25 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x703a6d2ab1a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x703a6d2ab1a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 503ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J246' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J246'.\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 68ms/step - loss: 767912.1875 - val_loss: 131823.7656\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 8999.4473 - val_loss: 204.1915\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 36.3261 - val_loss: 392.3046\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 55.3385 - val_loss: 8.3118\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 5.4001 - val_loss: 2.4763\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 4.7301 - val_loss: 2.9417\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 3.6685 - val_loss: 2.7624\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 3.3639 - val_loss: 1.7734\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 2.6525 - val_loss: 0.9244\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 2.3964 - val_loss: 0.3746\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 1.9667 - val_loss: 0.1453\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 1.4708 - val_loss: 0.2198\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 1.4086 - val_loss: 0.3273\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 1.2788 - val_loss: 0.1485\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 1.0103 - val_loss: 0.3103\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 1.0580 - val_loss: 0.1577\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.9176 - val_loss: 0.1676\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.8220 - val_loss: 0.1713\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.7714 - val_loss: 0.1715\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.8501 - val_loss: 0.1939\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.8514 - val_loss: 0.2000\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.8878 - val_loss: 0.2155\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.7261 - val_loss: 0.2515\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.7030 - val_loss: 0.2778\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.6568 - val_loss: 0.2812\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.6168 - val_loss: 0.3219\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.5489 - val_loss: 0.3522\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.6605 - val_loss: 0.3918\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.5669 - val_loss: 0.3527\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.5257 - val_loss: 0.4301\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.4381 - val_loss: 0.3819\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.4421 - val_loss: 0.5133\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.7827 - val_loss: 0.4195\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.6360 - val_loss: 0.3673\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.6336 - val_loss: 0.4278\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.4501 - val_loss: 0.4427\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.3307 - val_loss: 0.5481\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.3289 - val_loss: 0.6686\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3007 - val_loss: 0.8268\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.2588 - val_loss: 0.7383\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.3201 - val_loss: 1.0523\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.2752 - val_loss: 0.6210\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.2921 - val_loss: 0.7623\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.2786 - val_loss: 0.6843\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.2779 - val_loss: 0.6655\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.3209 - val_loss: 0.7203\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.2086 - val_loss: 0.7181\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.2192 - val_loss: 0.6856\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.5989 - val_loss: 0.4206\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.3600 - val_loss: 0.3995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:08:45 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0021\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 617ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J82' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J82'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 55ms/step - loss: 1161.5756 - val_loss: 6413.7676\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 677.4192 - val_loss: 20789.9902\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 391.7570 - val_loss: 718.6604\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 207.7578 - val_loss: 472.1024\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 100.9222 - val_loss: 267.0866\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 79.5403 - val_loss: 200.5755\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - loss: 58.9257 - val_loss: 183.5967\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 42.9450 - val_loss: 172.0171\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 40.7525 - val_loss: 175.9097\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 42.4971 - val_loss: 170.4877\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 27.4322 - val_loss: 189.3104\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 31.3895 - val_loss: 209.9906\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 37.7206 - val_loss: 207.3204\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 26.1950 - val_loss: 236.0841\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 29.5221 - val_loss: 212.3814\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 25.0908 - val_loss: 208.8075\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 23.5916 - val_loss: 197.9639\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 30.3928 - val_loss: 178.4133\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 23.5031 - val_loss: 218.4874\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 29.8537 - val_loss: 191.4542\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 23.0683 - val_loss: 207.5895\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 20.0262 - val_loss: 197.4467\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 25.0142 - val_loss: 199.5491\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 18.2577 - val_loss: 201.3966\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 22.7659 - val_loss: 221.0292\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 20.3705 - val_loss: 221.0959\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 17.2002 - val_loss: 219.0259\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 17.8506 - val_loss: 226.0522\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 20.6932 - val_loss: 224.3146\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 20.4609 - val_loss: 230.7525\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 10.9251 - val_loss: 217.8540\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 24.7458 - val_loss: 235.9482\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 13.0350 - val_loss: 232.2861\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 13.6467 - val_loss: 229.6072\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 14.7980 - val_loss: 238.9030\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 11.5991 - val_loss: 164.4909\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 12.8263 - val_loss: 135.9478\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 11.7959 - val_loss: 142.9596\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 10.1358 - val_loss: 125.9488\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 14.8297 - val_loss: 124.7743\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 9.0359 - val_loss: 128.2793\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 11.6285 - val_loss: 143.5537\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 8.8109 - val_loss: 151.4878\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 14.9215 - val_loss: 71.3472\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 9.9377 - val_loss: 53.8089\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 11.9526 - val_loss: 60.2916\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 13.3083 - val_loss: 57.2674\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 9.2537 - val_loss: 61.3350\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 7.3046 - val_loss: 61.1288\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 9.1599 - val_loss: 59.7609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:10:00 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0227\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 502ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J231' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J231'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 87ms/step - loss: 4950.5864 - val_loss: 3933.4980\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 718.7205 - val_loss: 245.9017\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 27.6607 - val_loss: 14.1871\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 4.3641 - val_loss: 43.5577\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 2.1106 - val_loss: 56.3097\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 1.4639 - val_loss: 43.6334\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 1.4757 - val_loss: 41.3790\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 1.2977 - val_loss: 50.2689\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 1.3512 - val_loss: 43.7198\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 1.2415 - val_loss: 41.0081\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 1.1543 - val_loss: 36.4159\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 1.1149 - val_loss: 32.7390\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.9940 - val_loss: 38.3588\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.0958 - val_loss: 37.7130\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.0391 - val_loss: 37.2522\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 1.0743 - val_loss: 36.8372\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.9840 - val_loss: 44.6852\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 1.8208 - val_loss: 39.0145\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 2.9412 - val_loss: 35.6607\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 2.2066 - val_loss: 31.3591\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 1.7932 - val_loss: 31.7357\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.6031 - val_loss: 42.5311\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 1.6243 - val_loss: 32.2065\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.4804 - val_loss: 49.3151\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 1.5863 - val_loss: 41.0838\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.5180 - val_loss: 40.0769\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.4193 - val_loss: 38.2886\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 1.3814 - val_loss: 30.7305\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.2958 - val_loss: 37.7576\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 1.3431 - val_loss: 32.6894\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.2094 - val_loss: 42.0286\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.2784 - val_loss: 34.3960\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.1362 - val_loss: 39.1998\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.2444 - val_loss: 30.5134\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.0998 - val_loss: 33.7768\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.0898 - val_loss: 32.7256\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.0772 - val_loss: 20.0908\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.3630 - val_loss: 19.2234\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.2680 - val_loss: 29.4576\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 1.0093 - val_loss: 31.4676\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.1166 - val_loss: 22.1939\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 1.0157 - val_loss: 34.7045\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.9275 - val_loss: 36.1447\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 1.0501 - val_loss: 29.9957\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.8796 - val_loss: 22.1988\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.8895 - val_loss: 13.3761\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 1.1473 - val_loss: 28.8501\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.9181 - val_loss: 34.5801\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.8801 - val_loss: 25.2743\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.8875 - val_loss: 8.0707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:11:11 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0094\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J274' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J274'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 98ms/step - loss: 3708.7683 - val_loss: 23171.0547\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 1797.6147 - val_loss: 8940.8633\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 3202.6785 - val_loss: 17083.2930\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 3437.9089 - val_loss: 146.7281\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 727.0603 - val_loss: 0.1691\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 29.9340 - val_loss: 0.3142\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 21.7072 - val_loss: 0.9013\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 28.6198 - val_loss: 49.2365\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 118.5455 - val_loss: 3.5740\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 13.3634 - val_loss: 0.8105\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 11.3648 - val_loss: 0.2544\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 14.8412 - val_loss: 0.4917\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 11.7928 - val_loss: 0.4130\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 15.1650 - val_loss: 0.1014\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 11.9734 - val_loss: 0.0880\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - loss: 8.1434 - val_loss: 0.0650\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 6.0279 - val_loss: 0.0698\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 3.3543 - val_loss: 0.2831\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 6.5858 - val_loss: 0.2540\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 4.7326 - val_loss: 1.5035\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 2.4379 - val_loss: 1.0718\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 1.4310 - val_loss: 0.9448\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 2.5306 - val_loss: 1.1094\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 2.4204 - val_loss: 0.7187\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 3.1329 - val_loss: 0.6865\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 1.8013 - val_loss: 0.8779\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 1.5342 - val_loss: 0.7148\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 1.1807 - val_loss: 0.5078\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 1.5609 - val_loss: 0.5973\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 1.5429 - val_loss: 0.7977\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 1.6483 - val_loss: 0.2479\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 1.7416 - val_loss: 0.0986\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 3.3139 - val_loss: 0.3381\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 2.2032 - val_loss: 0.2045\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 1.2472 - val_loss: 0.1348\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 3.0573 - val_loss: 0.5521\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 1.9292 - val_loss: 0.3799\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 1.0453 - val_loss: 0.2477\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 1.2445 - val_loss: 0.3047\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 1.0643 - val_loss: 0.3559\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 1.1381 - val_loss: 0.1591\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 1.1384 - val_loss: 0.3182\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.9008 - val_loss: 0.1231\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 1.1683 - val_loss: 0.2647\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.9273 - val_loss: 0.1647\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.7897 - val_loss: 0.1782\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.7702 - val_loss: 0.1272\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 1.2482 - val_loss: 0.3467\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 2.0052 - val_loss: 0.6548\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 321.9021 - val_loss: 3.3174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:12:32 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0071\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 740ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J330' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J330'.\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - loss: 26703.6680 - val_loss: 44.2786\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 1899.7673 - val_loss: 19801.5020\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 2415.8760 - val_loss: 2692.6704\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 4589.6328 - val_loss: 183.7222\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 1433.3588 - val_loss: 1749.2264\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - loss: 1554.1111 - val_loss: 728.9684\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 4135.7007 - val_loss: 3557.8508\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 1188.0197 - val_loss: 1546.7255\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 1063.9384 - val_loss: 603.2561\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 671.0141 - val_loss: 1400.0105\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 498.6622 - val_loss: 1856.6293\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 231.2316 - val_loss: 1477.3567\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 163.2976 - val_loss: 830.3156\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 161.5246 - val_loss: 2122.9282\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 469.1819 - val_loss: 5262.6987\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 290.6930 - val_loss: 408.3812\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 174.3987 - val_loss: 585.6163\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 124.4953 - val_loss: 431.1370\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 63.1992 - val_loss: 134.7768\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 13.5233 - val_loss: 333.3502\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 11.3087 - val_loss: 425.5919\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 11.3716 - val_loss: 2127.9109\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 6.3552 - val_loss: 2388.4661\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 9.9927 - val_loss: 3776.6775\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 39.5108 - val_loss: 3950.3938\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 67.1598 - val_loss: 1991.7170\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 41.6812 - val_loss: 1365.1466\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 42.3068 - val_loss: 703.3198\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 33.1795 - val_loss: 459.0819\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 28.8293 - val_loss: 233.8626\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 21.1021 - val_loss: 155.1606\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 20.6724 - val_loss: 93.2597\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 22.2227 - val_loss: 120.6857\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 25.9145 - val_loss: 152.9511\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 16.5565 - val_loss: 278.2096\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 35.1602 - val_loss: 254.4817\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 33.9950 - val_loss: 190.6891\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 22.1358 - val_loss: 14.1558\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 45.0646 - val_loss: 202.6884\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 57.8420 - val_loss: 379.9068\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 71.3656 - val_loss: 417.2048\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 69.3513 - val_loss: 102.5795\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 143.0689 - val_loss: 521.4670\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 131.5163 - val_loss: 118.1088\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 14.0118 - val_loss: 5.5646\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 14.0472 - val_loss: 36.0773\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 8.0400 - val_loss: 26.9144\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 5.9091 - val_loss: 28.4560\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 3.7217 - val_loss: 28.1032\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 3.9671 - val_loss: 30.1172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step\n",
      "MAE: 0.0205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:13:47 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J308' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J308'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 350273.9688 - val_loss: 1710253.8750\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 467185.0000 - val_loss: 485545.2500\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 425993.9062 - val_loss: 746970.6250\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 107919.4766 - val_loss: 43923.4805\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 26238.9883 - val_loss: 2347.8386\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2996.4231 - val_loss: 213.2780\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3377.5371 - val_loss: 0.7615\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5634.3096 - val_loss: 293197.2500\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 211625.7031 - val_loss: 2254.3997\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1018.9998 - val_loss: 63.7469\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 99.1231 - val_loss: 136.7283\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 99.8525 - val_loss: 69.6299\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 80.7416 - val_loss: 68.8463\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 98.5502 - val_loss: 49.7234\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 82.1691 - val_loss: 150.3853\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 58.0268 - val_loss: 163.3875\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 52.1598 - val_loss: 36.9256\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 49.6579 - val_loss: 27.0710\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 36.1093 - val_loss: 28.2519\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 42.4348 - val_loss: 24.2851\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 55.6229 - val_loss: 26.3647\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 35.3624 - val_loss: 28.0162\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 39.3720 - val_loss: 29.2762\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 34.6211 - val_loss: 35.2785\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 34.5763 - val_loss: 28.8223\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 42.0191 - val_loss: 30.6922\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 39.8838 - val_loss: 33.9819\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 34.4783 - val_loss: 33.9097\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 36.8374 - val_loss: 36.4024\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 30.7865 - val_loss: 36.1339\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 28.7126 - val_loss: 35.0082\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 33.0837 - val_loss: 32.4150\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 32.3205 - val_loss: 35.8695\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 29.7291 - val_loss: 33.9992\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 27.4372 - val_loss: 33.6866\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 30.5452 - val_loss: 31.9860\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 23.2268 - val_loss: 38.6102\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 28.5853 - val_loss: 32.5141\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 32.6858 - val_loss: 32.2666\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 25.5599 - val_loss: 38.3355\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 20.7206 - val_loss: 35.5482\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 23.4226 - val_loss: 34.8737\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 20.9309 - val_loss: 35.2222\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 22.4257 - val_loss: 33.8660\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 24.3720 - val_loss: 32.8881\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 24.5771 - val_loss: 35.0677\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 24.6801 - val_loss: 34.7434\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 25.1080 - val_loss: 36.8092\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 24.3046 - val_loss: 28.1009\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 24.2345 - val_loss: 29.6423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:14:40 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0181\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J492' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J492'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 243.9272 - val_loss: 51.9423\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.8654 - val_loss: 3.0224\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2743 - val_loss: 0.1232\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1030 - val_loss: 0.1271\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0712 - val_loss: 0.0992\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0785 - val_loss: 0.0844\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0704 - val_loss: 0.0980\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0710 - val_loss: 0.1141\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0684 - val_loss: 0.0769\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0718 - val_loss: 0.0781\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0708 - val_loss: 0.0957\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0659 - val_loss: 0.1101\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0658 - val_loss: 0.1017\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0690 - val_loss: 0.0854\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0697 - val_loss: 0.0776\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0652 - val_loss: 0.0859\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0665 - val_loss: 0.0724\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0649 - val_loss: 0.0840\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0674 - val_loss: 0.0855\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0664 - val_loss: 0.0710\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0701 - val_loss: 0.0660\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0679 - val_loss: 0.0617\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0750 - val_loss: 0.0614\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0591 - val_loss: 0.0899\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0719 - val_loss: 0.0645\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0673 - val_loss: 0.0657\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0649 - val_loss: 0.0886\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0680 - val_loss: 0.0702\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0575 - val_loss: 0.0834\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0660 - val_loss: 0.0764\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0701 - val_loss: 0.0754\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0616 - val_loss: 0.0867\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0607 - val_loss: 0.0740\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0629 - val_loss: 0.0742\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0664 - val_loss: 0.0609\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0704 - val_loss: 0.0756\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0671 - val_loss: 0.0713\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0658 - val_loss: 0.0786\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0654 - val_loss: 0.0698\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0619 - val_loss: 0.0823\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0613 - val_loss: 0.0744\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0672 - val_loss: 0.0737\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0662 - val_loss: 0.1100\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0686 - val_loss: 0.0748\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0653 - val_loss: 0.0608\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0657 - val_loss: 0.0623\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0587 - val_loss: 0.0836\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0632 - val_loss: 0.0608\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0652 - val_loss: 0.0644\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0566 - val_loss: 0.0890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:15:27 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0010\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J436' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J436'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 50669.2695 - val_loss: 15989.5527\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 14557.3145 - val_loss: 109468.1094\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 7062.9590 - val_loss: 3908.0950\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1852.8560 - val_loss: 3557.3413\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 282.9284 - val_loss: 20.2737\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 11.1886 - val_loss: 23.6056\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 19.1574 - val_loss: 3.5975\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 5.9791 - val_loss: 0.5665\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 2.6105 - val_loss: 3.2108\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 5.7540 - val_loss: 20482.8594\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 367.6962 - val_loss: 30152.6191\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 2557.5044 - val_loss: 18680.0098\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 2862.6897 - val_loss: 50.7716\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 205.7456 - val_loss: 1679.0604\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 239.9614 - val_loss: 296.9659\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 32.5365 - val_loss: 2.5068\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 689.2639 - val_loss: 68.6105\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 2983.3477 - val_loss: 5554.5508\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 944.2897 - val_loss: 1.9634\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 78.5182 - val_loss: 35.3099\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 16.6087 - val_loss: 26.7932\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 8.8896 - val_loss: 16.0669\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 8.0734 - val_loss: 7.2016\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 8.5292 - val_loss: 9.7990\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.4371 - val_loss: 12.3870\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 7.3741 - val_loss: 10.5233\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 7.7764 - val_loss: 9.8519\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 6.9364 - val_loss: 10.8447\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.6126 - val_loss: 11.3305\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.5428 - val_loss: 11.5383\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.1641 - val_loss: 6.8165\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 5.0527 - val_loss: 6.5280\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.0765 - val_loss: 9.5115\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 5.0363 - val_loss: 15.0788\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.2421 - val_loss: 10.5927\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 3.6025 - val_loss: 8.5184\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 3.8631 - val_loss: 8.3101\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.4996 - val_loss: 9.2290\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 3.1396 - val_loss: 8.2047\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.0993 - val_loss: 7.8852\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 2.9298 - val_loss: 5.0024\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.1489 - val_loss: 11.2279\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.8495 - val_loss: 4.8439\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.0251 - val_loss: 3.4182\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.9931 - val_loss: 12.9763\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 2.8091 - val_loss: 5.2856\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 2.8677 - val_loss: 5.3921\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.7122 - val_loss: 2.8550\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 3.0382 - val_loss: 3.0945\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0075 - val_loss: 2.5061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:16:22 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0053\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 430ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J501' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J501'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - loss: 14.5243 - val_loss: 4.1534\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.0894 - val_loss: 1.7029\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.2470 - val_loss: 0.3571\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1315 - val_loss: 0.6452\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1190 - val_loss: 0.2938\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.1283 - val_loss: 0.5293\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1141 - val_loss: 0.2803\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1093 - val_loss: 0.3189\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1045 - val_loss: 0.5550\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.1074 - val_loss: 0.3636\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0977 - val_loss: 0.3374\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.0982 - val_loss: 0.4446\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.1040 - val_loss: 0.4588\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0948 - val_loss: 0.3337\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 0.1014 - val_loss: 0.2497\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0961 - val_loss: 0.6448\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.0935 - val_loss: 0.8403\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0966 - val_loss: 0.6135\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.1029 - val_loss: 0.1961\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0942 - val_loss: 0.7454\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1031 - val_loss: 0.2359\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0874 - val_loss: 0.2422\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0905 - val_loss: 0.1841\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0909 - val_loss: 0.2552\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0889 - val_loss: 0.1797\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0916 - val_loss: 0.5061\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0896 - val_loss: 0.1989\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0919 - val_loss: 0.1704\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0864 - val_loss: 0.4634\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0949 - val_loss: 0.1573\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0794 - val_loss: 0.3470\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0767 - val_loss: 0.3231\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0814 - val_loss: 0.0802\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0921 - val_loss: 0.2502\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0802 - val_loss: 0.4507\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0908 - val_loss: 0.2305\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.0867 - val_loss: 0.3253\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0831 - val_loss: 0.1807\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0903 - val_loss: 0.0993\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0846 - val_loss: 0.2036\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0703 - val_loss: 0.1614\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0797 - val_loss: 0.3186\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0769 - val_loss: 0.3855\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0884 - val_loss: 0.3565\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0868 - val_loss: 0.3727\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0819 - val_loss: 0.5395\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0866 - val_loss: 0.1809\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0761 - val_loss: 0.1696\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0766 - val_loss: 0.0828\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0772 - val_loss: 0.1446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step\n",
      "MAE: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:17:34 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J86' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J86'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - loss: 3937.5442 - val_loss: 844.5540\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1153.9336 - val_loss: 1946.7417\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 4817.8291 - val_loss: 261.1583\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1092.2661 - val_loss: 597.9750\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 143.2884 - val_loss: 7.7666\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 27.1179 - val_loss: 43.1391\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 4.5619 - val_loss: 8.3301\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 1.0410 - val_loss: 5.6137\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.4275 - val_loss: 4.4353\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2447 - val_loss: 3.6998\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2254 - val_loss: 3.3829\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1993 - val_loss: 3.0263\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1835 - val_loss: 2.9695\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1795 - val_loss: 2.5458\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1804 - val_loss: 2.4953\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.1827 - val_loss: 2.3074\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1670 - val_loss: 2.1949\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1557 - val_loss: 1.9932\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1596 - val_loss: 1.7756\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1720 - val_loss: 1.7666\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1680 - val_loss: 1.3424\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1542 - val_loss: 1.2261\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1513 - val_loss: 1.3270\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1404 - val_loss: 1.3390\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1560 - val_loss: 1.2941\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1319 - val_loss: 1.1027\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.1655 - val_loss: 1.1926\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.1414 - val_loss: 1.2500\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1721 - val_loss: 1.1025\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1417 - val_loss: 0.9279\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1313 - val_loss: 0.8575\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1402 - val_loss: 0.8444\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1454 - val_loss: 0.9891\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1337 - val_loss: 1.1588\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1377 - val_loss: 0.7399\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1395 - val_loss: 0.7922\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1384 - val_loss: 0.6551\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1424 - val_loss: 0.9220\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1387 - val_loss: 0.7890\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1310 - val_loss: 0.6734\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1129 - val_loss: 0.6659\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1270 - val_loss: 0.5549\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1282 - val_loss: 0.6970\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1547 - val_loss: 0.6540\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1416 - val_loss: 0.6750\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1267 - val_loss: 0.5593\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1258 - val_loss: 0.5761\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2062 - val_loss: 0.6648\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.1422 - val_loss: 0.6315\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1302 - val_loss: 0.5085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step\n",
      "MAE: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:18:32 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J175' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J175'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 23915.4160 - val_loss: 41004.9609\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4368.5835 - val_loss: 29472.0879\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3094.0483 - val_loss: 2768.5825\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 603.2853 - val_loss: 36.9034\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 15.4311 - val_loss: 2.1479\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 5.6278 - val_loss: 27.5655\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 1.8007 - val_loss: 16.6579\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.2753 - val_loss: 14.2060\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 1.0692 - val_loss: 11.9131\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.9689 - val_loss: 10.7855\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.8240 - val_loss: 10.5063\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6909 - val_loss: 8.4094\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6818 - val_loss: 7.2936\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.5795 - val_loss: 7.5820\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5690 - val_loss: 6.4959\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.5916 - val_loss: 6.9411\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.6053 - val_loss: 4.9955\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.6100 - val_loss: 5.4161\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.5103 - val_loss: 5.2167\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.5603 - val_loss: 5.6405\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.5242 - val_loss: 4.5196\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.2023 - val_loss: 2.1599\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.6108 - val_loss: 3.9000\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.5134 - val_loss: 3.4338\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.4670 - val_loss: 3.9221\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.4421 - val_loss: 2.8328\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4199 - val_loss: 3.4227\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4518 - val_loss: 2.5642\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4251 - val_loss: 2.3340\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.4453 - val_loss: 2.2190\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.4274 - val_loss: 1.6149\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3941 - val_loss: 1.4439\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.4013 - val_loss: 1.3128\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.4231 - val_loss: 1.3253\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.4252 - val_loss: 1.6314\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 0.3777 - val_loss: 0.9944\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.3901 - val_loss: 1.1534\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.3679 - val_loss: 0.8343\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.4442 - val_loss: 0.8224\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.4240 - val_loss: 0.8435\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.3827 - val_loss: 0.8769\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.4028 - val_loss: 0.8617\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.4173 - val_loss: 1.2524\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.4296 - val_loss: 0.9354\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.3293 - val_loss: 0.8954\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.4234 - val_loss: 0.8587\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.3512 - val_loss: 0.8065\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.3662 - val_loss: 0.7950\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.3776 - val_loss: 0.7696\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.3384 - val_loss: 0.6720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step\n",
      "MAE: 0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:19:47 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J239' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J239'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 6956.6519 - val_loss: 4033.3047\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7258.0205 - val_loss: 3851.8904\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1234.4539 - val_loss: 3489.4712\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 322.9068 - val_loss: 214.4541\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 225.3427 - val_loss: 1962.5546\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 616.5822 - val_loss: 1216.6726\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 165.8897 - val_loss: 486.9956\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 450.1122 - val_loss: 398.8351\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 129.9847 - val_loss: 139.3326\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 60.2902 - val_loss: 92.1105\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 47.5708 - val_loss: 44.9545\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 39.6378 - val_loss: 27.2446\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 42.7264 - val_loss: 17.0077\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 38.7417 - val_loss: 9.3285\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 27.0486 - val_loss: 6.0595\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 24.0978 - val_loss: 4.0634\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 18.3792 - val_loss: 2.3099\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 10.8656 - val_loss: 1.5262\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 9.1930 - val_loss: 1.3893\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 12.4405 - val_loss: 0.9822\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 8.6407 - val_loss: 0.7850\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 13.6350 - val_loss: 0.7109\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 11.0705 - val_loss: 0.7056\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 14.2280 - val_loss: 0.9637\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 24.8253 - val_loss: 2.0513\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 21.3288 - val_loss: 3.1064\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 21.1211 - val_loss: 4.2513\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 22.4613 - val_loss: 4.5397\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 19.0905 - val_loss: 4.8798\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 18.7047 - val_loss: 2.0886\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 11.1801 - val_loss: 1.3726\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.4995 - val_loss: 1.0641\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.6177 - val_loss: 0.8323\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.9446 - val_loss: 1.1880\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.0850 - val_loss: 1.2541\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.0843 - val_loss: 1.3405\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 3.2173 - val_loss: 2.0778\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.8706 - val_loss: 4.7308\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 3.9919 - val_loss: 6.9871\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 3.2713 - val_loss: 7.5404\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.6087 - val_loss: 8.3177\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 3.2671 - val_loss: 8.4182\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.5994 - val_loss: 8.6576\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.9687 - val_loss: 7.1195\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.3982 - val_loss: 7.7047\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.0847 - val_loss: 7.1835\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 2.5326 - val_loss: 5.4832\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.2996 - val_loss: 4.1018\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.3758 - val_loss: 3.1877\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.2645 - val_loss: 2.8425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step\n",
      "MAE: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:21:31 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J355' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J355'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 1567.8748 - val_loss: 102.4510\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 334.2320 - val_loss: 1008.5352\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2148.6211 - val_loss: 401.9771\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 671.5626 - val_loss: 1248.1519\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 479.2627 - val_loss: 544.0432\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 197.9734 - val_loss: 164.2391\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 88.9881 - val_loss: 64.0690\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 69.6560 - val_loss: 26.3977\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 13.9977 - val_loss: 1.2784\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.7644 - val_loss: 0.3476\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.3036 - val_loss: 0.4407\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.7293 - val_loss: 7.7017\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 9.6113 - val_loss: 6.4391\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.4412 - val_loss: 0.6420\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6019 - val_loss: 0.2955\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.3889 - val_loss: 0.3488\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3103 - val_loss: 0.3395\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3007 - val_loss: 0.2623\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.2270 - val_loss: 0.1581\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.2327 - val_loss: 0.1668\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1928 - val_loss: 0.1404\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1683 - val_loss: 0.1156\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1864 - val_loss: 0.1242\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.1626 - val_loss: 0.1162\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.1683 - val_loss: 0.1194\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1629 - val_loss: 0.1598\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1545 - val_loss: 0.1048\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.1475 - val_loss: 0.1012\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1312 - val_loss: 0.1442\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.1376 - val_loss: 0.1030\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1565 - val_loss: 0.0996\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.1560 - val_loss: 0.1103\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1738 - val_loss: 0.1050\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.1499 - val_loss: 0.1091\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1566 - val_loss: 0.1044\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1282 - val_loss: 0.0979\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1181 - val_loss: 0.1038\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.1402 - val_loss: 0.0937\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1187 - val_loss: 0.0976\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.1301 - val_loss: 0.1106\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1259 - val_loss: 0.1055\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1323 - val_loss: 0.0979\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1252 - val_loss: 0.0985\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1405 - val_loss: 0.1242\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1478 - val_loss: 0.1016\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1061 - val_loss: 0.0900\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1140 - val_loss: 0.1609\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1205 - val_loss: 0.0968\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0994 - val_loss: 0.0975\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1075 - val_loss: 0.0939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step\n",
      "MAE: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:23:46 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J216' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J216'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 110204.0312 - val_loss: 89357.7969\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 19809.5430 - val_loss: 85223.8203\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 21819.6406 - val_loss: 39870.7539\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7686.1714 - val_loss: 18.8071\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 162.4579 - val_loss: 180.2453\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 73.6354 - val_loss: 18.9222\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 29.1403 - val_loss: 15.4665\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 11.4334 - val_loss: 7.1918\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 22.1406 - val_loss: 5.7975\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 7.5474 - val_loss: 2.9776\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.9689 - val_loss: 6.1300\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 3.1816 - val_loss: 4.1334\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 3.5511 - val_loss: 3.2961\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.7907 - val_loss: 5.7036\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.6466 - val_loss: 5.3568\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 2.4632 - val_loss: 3.8895\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 2.5414 - val_loss: 2.9690\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 1.9937 - val_loss: 3.7594\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 2.1863 - val_loss: 2.8376\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 2.1302 - val_loss: 3.1198\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.9119 - val_loss: 3.1351\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 2.7378 - val_loss: 3.3791\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 2.1538 - val_loss: 3.7464\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 1.5930 - val_loss: 3.9241\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.7195 - val_loss: 3.3645\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 2.0740 - val_loss: 6.4410\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 1.7263 - val_loss: 2.9135\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 1.4034 - val_loss: 6.1785\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 2.1354 - val_loss: 2.7908\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 1.7028 - val_loss: 3.8528\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.6080 - val_loss: 2.6234\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 1.5397 - val_loss: 2.5215\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1.2157 - val_loss: 2.7865\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.5103 - val_loss: 2.5212\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 1.3208 - val_loss: 2.8546\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.1861 - val_loss: 2.3363\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.2148 - val_loss: 2.3353\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.9801 - val_loss: 2.3321\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.9297 - val_loss: 2.4395\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.9287 - val_loss: 2.4337\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.9212 - val_loss: 2.4088\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.9032 - val_loss: 2.3738\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.8879 - val_loss: 2.3626\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.8532 - val_loss: 2.4065\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.8171 - val_loss: 2.4134\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.8489 - val_loss: 2.4248\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.9359 - val_loss: 2.4763\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.8580 - val_loss: 2.3935\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.7873 - val_loss: 2.4423\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.8038 - val_loss: 2.6566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step\n",
      "MAE: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:27:59 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 658ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J291' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J291'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 132980.7500 - val_loss: 285605.5312\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 14371.1553 - val_loss: 34343.8984\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 17374.8418 - val_loss: 2625.6760\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 396.1084 - val_loss: 541.9045\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 147.8737 - val_loss: 29.7118\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 58.8069 - val_loss: 0.5011\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 48.6084 - val_loss: 0.9943\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 45.5646 - val_loss: 16.4495\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 48.8954 - val_loss: 8.1311\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 38.2866 - val_loss: 0.4253\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 21.9231 - val_loss: 0.4961\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 26.4062 - val_loss: 0.3304\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 16.7145 - val_loss: 0.6420\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 35.4138 - val_loss: 15.9171\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 40.1929 - val_loss: 0.4150\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 21.1552 - val_loss: 0.5571\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 19.2912 - val_loss: 0.9090\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 20.1899 - val_loss: 0.3973\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 19.0727 - val_loss: 0.6694\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 17.1510 - val_loss: 1.3135\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 15.0479 - val_loss: 0.5702\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 16.7891 - val_loss: 0.3931\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 14.6283 - val_loss: 2.4548\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 15.5336 - val_loss: 1.2481\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 9.3160 - val_loss: 0.3893\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 8.9282 - val_loss: 1.0699\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.2666 - val_loss: 0.4988\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.6202 - val_loss: 0.6848\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 10.4533 - val_loss: 2.7433\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 5.7214 - val_loss: 0.4055\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 4.6333 - val_loss: 0.4844\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 4.6537 - val_loss: 0.4491\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.5666 - val_loss: 1.1676\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 5.7498 - val_loss: 0.3548\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.0298 - val_loss: 0.3221\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.0120 - val_loss: 0.8659\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 12.9244 - val_loss: 0.6244\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 8.8727 - val_loss: 0.6441\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 8.0181 - val_loss: 0.2919\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 9.7487 - val_loss: 0.3445\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 6.4601 - val_loss: 0.3096\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 78.6377 - val_loss: 71.1540\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 70.5231 - val_loss: 4.9916\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 2.7973 - val_loss: 1.4366\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 2.4285 - val_loss: 1.2267\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 1.4868 - val_loss: 2.7754\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.5809 - val_loss: 1.2793\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 2.1025 - val_loss: 1.1319\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 1.1862 - val_loss: 4.0795\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.8314 - val_loss: 1.7924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 140ms/step\n",
      "MAE: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:36:29 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J54' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J54'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 26015.2246 - val_loss: 134.2187\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 104.3127 - val_loss: 7.8724\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 40.3500 - val_loss: 5.3610\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 23.2093 - val_loss: 3.7505\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 12.9642 - val_loss: 1.0218\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.5759 - val_loss: 1.6011\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.3283 - val_loss: 1.2871\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.4896 - val_loss: 1.4654\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.2487 - val_loss: 2.0226\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5187 - val_loss: 1.8287\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.7372 - val_loss: 1.7500\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.6075 - val_loss: 2.1010\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5442 - val_loss: 0.7759\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5523 - val_loss: 1.0045\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.6188 - val_loss: 1.4898\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5482 - val_loss: 1.4177\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4944 - val_loss: 1.2478\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.4748 - val_loss: 1.4910\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4493 - val_loss: 1.1197\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4651 - val_loss: 0.5765\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.4483 - val_loss: 1.1177\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.4378 - val_loss: 0.8516\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.4305 - val_loss: 0.7024\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.4440 - val_loss: 0.7327\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4150 - val_loss: 0.9867\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3936 - val_loss: 1.0292\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4036 - val_loss: 0.9486\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4164 - val_loss: 0.7887\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.3846 - val_loss: 0.6654\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.4030 - val_loss: 0.4345\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.3822 - val_loss: 0.7867\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4066 - val_loss: 0.4609\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.3877 - val_loss: 1.4192\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3899 - val_loss: 0.5580\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3649 - val_loss: 0.4149\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3691 - val_loss: 0.3806\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3409 - val_loss: 1.6182\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4936 - val_loss: 0.6129\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3414 - val_loss: 0.4481\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3781 - val_loss: 0.3723\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3683 - val_loss: 0.3897\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.4148 - val_loss: 0.7550\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3341 - val_loss: 0.6259\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3255 - val_loss: 1.0113\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3846 - val_loss: 0.4675\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3284 - val_loss: 0.6176\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3560 - val_loss: 0.9812\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3207 - val_loss: 2.0703\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4291 - val_loss: 1.4907\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4742 - val_loss: 3.1348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "MAE: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 12:50:44 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'J255'.\n",
      "Created version '1' of model 'J255'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 64911.6172 - val_loss: 163083.8438\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 20375.7871 - val_loss: 29100.2969\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 4386.8521 - val_loss: 2861.4824\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6942.5288 - val_loss: 18085.7910\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3902.7686 - val_loss: 150243.8594\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8117.3286 - val_loss: 33845.2539\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 4024.3823 - val_loss: 1623.5437\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 971.1032 - val_loss: 3.0783\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 137.6329 - val_loss: 114.8189\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 237.6510 - val_loss: 2.6388\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 84.6742 - val_loss: 3.6082\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 55.4079 - val_loss: 6.7605\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 33.9744 - val_loss: 6.6674\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 28.2117 - val_loss: 10.3481\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 34.6375 - val_loss: 10.4745\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 43.7431 - val_loss: 7.6192\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 22.0929 - val_loss: 2.9646\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 33.3767 - val_loss: 18.2047\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 25.8228 - val_loss: 37.3733\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 24.0567 - val_loss: 32.3426\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1219.1473 - val_loss: 5482.6436\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2947.8611 - val_loss: 41333.6992\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 10345.1006 - val_loss: 114.9659\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 742.0919 - val_loss: 36626.8789\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 2512.2393 - val_loss: 1181.7974\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 218.3327 - val_loss: 0.9179\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 8.5767 - val_loss: 2.2197\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.7969 - val_loss: 1.3963\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.4286 - val_loss: 0.4186\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.3595 - val_loss: 0.2327\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.3394 - val_loss: 0.2094\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.2867 - val_loss: 0.2570\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.3184 - val_loss: 0.1223\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.2141 - val_loss: 2.1849\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.2098 - val_loss: 2.8194\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1920 - val_loss: 0.6204\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1755 - val_loss: 0.1432\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2004 - val_loss: 2.5285\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1372 - val_loss: 2.0633\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1293 - val_loss: 0.6574\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1333 - val_loss: 1.9347\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.1346 - val_loss: 0.5635\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.1301 - val_loss: 1.1644\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.1140 - val_loss: 0.1574\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1286 - val_loss: 1.5010\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1093 - val_loss: 1.2371\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1087 - val_loss: 0.5327\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1171 - val_loss: 0.9528\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1073 - val_loss: 0.6832\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.1132 - val_loss: 0.2366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step\n",
      "MAE: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 13:24:02 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x703b08ddff50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import time\n",
    "import json \n",
    "\n",
    "#usefull functions:\n",
    "def create_sequences_by_scenario(df, seq_length, feature_cols, target_col):\n",
    "    X, y, seq_scenario_ids = [], [], []\n",
    "    for scenario_id, group in df.groupby('scenario_id'):\n",
    "        group = group.sort_index()\n",
    "        data = group[feature_cols + [target_col]].values\n",
    "        if len(data) <= seq_length:\n",
    "            continue\n",
    "        for i in range(seq_length, len(data)):\n",
    "            X.append(data[i-seq_length:i, :-1])\n",
    "            y.append(data[i, -1])\n",
    "            seq_scenario_ids.append(scenario_id)\n",
    "    return np.array(X), np.array(y), np.array(seq_scenario_ids)\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    # Avoid division by zero\n",
    "    y_true = np.where(y_true == 0, np.finfo(float).eps, y_true)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
    "    denominator = np.where(denominator == 0, np.finfo(float).eps, denominator)\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / denominator)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_value = np.percentile(y_true, percentile)\n",
    "    peak_indices = y_true >= peak_value\n",
    "    if np.sum(peak_indices) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_indices] - y_pred[peak_indices]))\n",
    "# Load data\n",
    "feature_and_target ={}\n",
    "version = '0.0.1'\n",
    "os.makedirs(f'./model_trained/LSTM_{version}',exist_ok=True)\n",
    "root = './dataset/junctions/'\n",
    "metrics_df = pd.DataFrame(columns=['junction', 'MAE', 'RMSE', 'MAPE', 'SMAPE', 'R2', 'Peak_Error'])\n",
    "mlflow.set_tracking_uri(\"file:model_trained/mlruns\")\n",
    "\n",
    "seq_length = 23\n",
    "n_features = 0\n",
    "n_targets = 1\n",
    "with mlflow.start_run(run_name=f\"LSTM_{version}\"):\n",
    "\n",
    "        # Log parameters\n",
    "    mlflow.log_param(\"seq_length\", seq_length)\n",
    "    mlflow.log_param(\"n_targets\", n_targets)\n",
    "    mlflow.log_param(\"epochs\", 50)\n",
    "    mlflow.log_param(\"batch_size\", 32)\n",
    "\n",
    "    for filename in os.listdir(root):\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        raw_cols = [col for col in df.columns]\n",
    "        junction = os.path.splitext(filename)[0]\n",
    "        df[f'{junction}_lag1'] = df[junction].shift(1)\n",
    "        # df[f'{junction}_lag7'] = df[junction].shift(7)\n",
    "        # df[f'{junction}_roll_mean_7'] = df[junction].rolling(window=7).mean()\n",
    "\n",
    "        # Drop rows with NaN\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        # Features and targets\n",
    "        target = junction\n",
    "        features = [col for col in df.columns if col != target]\n",
    "        feature_and_target[junction] = {\n",
    "            \"target\": target,\n",
    "            \"features\": features,\n",
    "            \"raw_cols\": raw_cols\n",
    "        }\n",
    "\n",
    "\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        target_scaler = MinMaxScaler()\n",
    "\n",
    "        scaled_features = feature_scaler.fit_transform(df[features])\n",
    "        scaled_target = target_scaler.fit_transform(df[[target]])\n",
    "\n",
    "        # Save the scalers for later use\n",
    "        with open(f'model_trained/LSTM_{version}/{junction}_feature_scaler.save', 'wb') as f:\n",
    "            pickle.dump(feature_scaler, f)\n",
    "\n",
    "        with open(f'model_trained/LSTM_{version}/{junction}_target_scaler.save', 'wb') as f:\n",
    "            pickle.dump(target_scaler, f)\n",
    "\n",
    "\n",
    "        # # Log dataset file names, number of scenarios, rows, and columns\n",
    "        # mlflow.log_param(\"num_scenarios\", len(scenario_ids))\n",
    "        # mlflow.log_param(\"num_rows\", df.shape[0])\n",
    "        # mlflow.log_param(\"num_features\", len(features))\n",
    "\n",
    "        scaled_df = pd.DataFrame(np.hstack((scaled_features, scaled_target)), columns=features + [target])\n",
    "        scaled_df['scenario_id'] = df['scenario_id'].values\n",
    "\n",
    "        X, y, seq_scenario_ids = create_sequences_by_scenario(scaled_df, seq_length, features, target)\n",
    "\n",
    "        scenario_ids = df['scenario_id'].unique()\n",
    "        split = int(0.8 * len(scenario_ids))\n",
    "        train_scenarios = scenario_ids[:split]\n",
    "        test_scenarios = scenario_ids[split:]\n",
    "\n",
    "        X_train = X[np.isin(seq_scenario_ids, train_scenarios)]\n",
    "        y_train = y[np.isin(seq_scenario_ids, train_scenarios)]\n",
    "        X_test = X[np.isin(seq_scenario_ids, test_scenarios)]\n",
    "        y_test = y[np.isin(seq_scenario_ids, test_scenarios)]\n",
    "\n",
    "        n_features = len(features)\n",
    "        mlflow.log_param(f\"n_features_{junction}\", n_features)\n",
    "        n_targets = 1\n",
    "\n",
    "        # Build multi-output LSTM model\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.LSTM(50, activation='relu', input_shape=(seq_length, n_features)),\n",
    "            tf.keras.layers.Dense(n_targets)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "\n",
    "        # Train\n",
    "        start = time.time()\n",
    "\n",
    "        history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "        elapsed_sec = time.time() - start\n",
    "        minutes, rem = divmod(elapsed_sec, 60)\n",
    "        seconds, milliseconds = divmod(rem, 1)\n",
    "        milliseconds = int(milliseconds * 1000)\n",
    "        formatted_time = f\"{int(minutes)}m:{int(seconds)}s:{milliseconds}ms\"\n",
    "        mlflow.log_param(f\"{junction}_training_time\", formatted_time)\n",
    "\n",
    "        # Save the model\n",
    "        model.save(f'model_trained/LSTM_{version}/{junction}.h5')\n",
    "\n",
    "        model_summary = []\n",
    "        model.summary(print_fn=lambda x: model_summary.append(x))\n",
    "        mlflow.log_text(\n",
    "            \"\\n\".join(model_summary),\n",
    "            artifact_file=f\"{junction}/model_summary.txt\"\n",
    "        )\n",
    "\n",
    "        # Log training and validation loss curves\n",
    "        plt.plot(history.history['loss'], label='train_loss')\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "        plt.legend(); plt.title('Loss over epochs')\n",
    "        plt.savefig(f'model_trained/LSTM_{version}/{junction}_loss_curve.png')\n",
    "        mlflow.log_artifact(\n",
    "            f'model_trained/LSTM_{version}/{junction}_loss_curve.png',\n",
    "            artifact_path=f\"{junction}/plots\"\n",
    "        )\n",
    "\n",
    "        # Predict & inverse scale\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_test_inv = target_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "        y_pred_inv = target_scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "        # Simple evaluation per node\n",
    "        for i in range(y_test_inv.shape[1]):\n",
    "            mae = mean_absolute_error(y_test_inv[:, i], y_pred_inv[:, i])\n",
    "            print(f'MAE: {mae:.4f}')\n",
    "\n",
    "\n",
    "        mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "        mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv)\n",
    "        smape_val = smape(y_test_inv, y_pred_inv)\n",
    "        r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "        peak_err = peak_error(y_test_inv, y_pred_inv)\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
    "            'junction': [junction],\n",
    "            'MAE': [mae],\n",
    "            'RMSE': [rmse],\n",
    "            'MAPE': [mape],\n",
    "            'SMAPE': [smape_val],\n",
    "            'R2': [r2],\n",
    "            'Peak_Error': [peak_err]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "            # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"MAE\": mae,\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAPE\": mape,\n",
    "            \"SMAPE\": smape_val,\n",
    "            \"R2\": r2,\n",
    "            \"Peak_Error\": peak_err\n",
    "        })\n",
    "        mlflow.log_artifact(\n",
    "            f'model_trained/LSTM_{version}/{junction}_feature_scaler.save',\n",
    "            artifact_path=f\"{junction}/scalers\"\n",
    "        )\n",
    "        mlflow.log_artifact(\n",
    "            f'model_trained/LSTM_{version}/{junction}_target_scaler.save',\n",
    "            artifact_path=f\"{junction}/scalers\"\n",
    "        )\n",
    "        input_example = np.random.rand(1, seq_length, n_features)\n",
    "        mlflow.tensorflow.log_model(\n",
    "            model,\n",
    "            artifact_path=junction,       # dossier interne pour stocker les fichiers\n",
    "            registered_model_name=junction, # nom du modèle dans MLflow\n",
    "            input_example=input_example\n",
    "        )\n",
    "\n",
    "\n",
    "metrics_df.set_index('junction', inplace=True)\n",
    "for metric in metrics_df.columns:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    metrics_df[metric].plot(kind='bar', color='skyblue')\n",
    "    plt.title(f'LSTM Model {metric} Comparison Across Junctions')\n",
    "    plt.ylabel(metric)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plot_path = f'model_trained/LSTM_{version}/{metric}_comparison.png'\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # log the plot to MLflow\n",
    "    mlflow.log_artifact(plot_path)\n",
    "with open(\"feature_and_target.json\", \"w\") as f:\n",
    "    json.dump(feature_and_target, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d14c793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>J434_elevation</th>\n",
       "      <th>P298_diameter</th>\n",
       "      <th>P301_diameter</th>\n",
       "      <th>P298_initial_status</th>\n",
       "      <th>P301_initial_status</th>\n",
       "      <th>P298_length</th>\n",
       "      <th>P301_length</th>\n",
       "      <th>P298_minor_loss</th>\n",
       "      <th>P301_minor_loss</th>\n",
       "      <th>P298_roughness</th>\n",
       "      <th>P301_roughness</th>\n",
       "      <th>J434_lag1</th>\n",
       "      <th>J434</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.355601</td>\n",
       "      <td>0.318596</td>\n",
       "      <td>0.318596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304077</td>\n",
       "      <td>0.072027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.465571</td>\n",
       "      <td>0.836645</td>\n",
       "      <td>0.667289</td>\n",
       "      <td>0.028461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.809477</td>\n",
       "      <td>0.068133</td>\n",
       "      <td>0.068133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.679583</td>\n",
       "      <td>0.064813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.060956</td>\n",
       "      <td>0.028461</td>\n",
       "      <td>0.245149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.821379</td>\n",
       "      <td>0.356280</td>\n",
       "      <td>0.356280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>0.919407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036234</td>\n",
       "      <td>0.367950</td>\n",
       "      <td>0.245149</td>\n",
       "      <td>0.622423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.245046</td>\n",
       "      <td>0.410557</td>\n",
       "      <td>0.410557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176097</td>\n",
       "      <td>0.338755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.881240</td>\n",
       "      <td>0.274330</td>\n",
       "      <td>0.622423</td>\n",
       "      <td>0.770930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582206</td>\n",
       "      <td>0.319809</td>\n",
       "      <td>0.319809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.437278</td>\n",
       "      <td>0.330571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111404</td>\n",
       "      <td>0.184478</td>\n",
       "      <td>0.770930</td>\n",
       "      <td>0.864857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scenario_id  time_id  J434_elevation  P298_diameter  P301_diameter  \\\n",
       "0          1.0      0.0        0.355601       0.318596       0.318596   \n",
       "1          2.0      0.0        0.809477       0.068133       0.068133   \n",
       "2          3.0      0.0        0.821379       0.356280       0.356280   \n",
       "3          4.0      0.0        0.245046       0.410557       0.410557   \n",
       "4          5.0      0.0        0.582206       0.319809       0.319809   \n",
       "\n",
       "   P298_initial_status  P301_initial_status  P298_length  P301_length  \\\n",
       "0                  0.0                  0.0     0.304077     0.072027   \n",
       "1                  0.0                  0.0     0.679583     0.064813   \n",
       "2                  0.0                  0.0     0.948148     0.919407   \n",
       "3                  0.0                  0.0     0.176097     0.338755   \n",
       "4                  0.0                  0.0     0.437278     0.330571   \n",
       "\n",
       "   P298_minor_loss  P301_minor_loss  P298_roughness  P301_roughness  \\\n",
       "0              0.0              0.0        0.465571        0.836645   \n",
       "1              0.0              0.0        0.100005        0.060956   \n",
       "2              0.0              0.0        0.036234        0.367950   \n",
       "3              0.0              0.0        0.881240        0.274330   \n",
       "4              0.0              0.0        0.111404        0.184478   \n",
       "\n",
       "   J434_lag1      J434  \n",
       "0   0.667289  0.028461  \n",
       "1   0.028461  0.245149  \n",
       "2   0.245149  0.622423  \n",
       "3   0.622423  0.770930  \n",
       "4   0.770930  0.864857  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3364e46",
   "metadata": {},
   "source": [
    "LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27fdc4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/.config/matplotlib is not a writable directory\n",
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-k2_esvgw because there was an issue with the default path (/home/zayd/.config/matplotlib); it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.236709\tvalid_1's rmse: 0.241072\n",
      "[200]\ttraining's rmse: 0.229331\tvalid_1's rmse: 0.236764\n",
      "[300]\ttraining's rmse: 0.225019\tvalid_1's rmse: 0.235763\n",
      "[400]\ttraining's rmse: 0.221995\tvalid_1's rmse: 0.235552\n",
      "[500]\ttraining's rmse: 0.219323\tvalid_1's rmse: 0.235457\n",
      "[600]\ttraining's rmse: 0.217026\tvalid_1's rmse: 0.235466\n",
      "Early stopping, best iteration is:\n",
      "[553]\ttraining's rmse: 0.218071\tvalid_1's rmse: 0.235411\n",
      "J434 training time: 6.40 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4136/372010587.py:144: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  metrics_df = pd.concat([metrics_df, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.241274\tvalid_1's rmse: 0.244168\n",
      "[200]\ttraining's rmse: 0.235198\tvalid_1's rmse: 0.241447\n",
      "[300]\ttraining's rmse: 0.231663\tvalid_1's rmse: 0.240862\n",
      "[400]\ttraining's rmse: 0.228899\tvalid_1's rmse: 0.240889\n",
      "Early stopping, best iteration is:\n",
      "[333]\ttraining's rmse: 0.230696\tvalid_1's rmse: 0.240827\n",
      "J246 training time: 5.75 sec\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.237435\tvalid_1's rmse: 0.238924\n",
      "[200]\ttraining's rmse: 0.230412\tvalid_1's rmse: 0.235455\n",
      "[300]\ttraining's rmse: 0.226454\tvalid_1's rmse: 0.234866\n",
      "[400]\ttraining's rmse: 0.223183\tvalid_1's rmse: 0.23461\n",
      "Early stopping, best iteration is:\n",
      "[385]\ttraining's rmse: 0.223633\tvalid_1's rmse: 0.234589\n",
      "J82 training time: 3.89 sec\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.227363\tvalid_1's rmse: 0.228167\n",
      "[200]\ttraining's rmse: 0.219543\tvalid_1's rmse: 0.22368\n",
      "[300]\ttraining's rmse: 0.215692\tvalid_1's rmse: 0.222742\n",
      "[400]\ttraining's rmse: 0.212839\tvalid_1's rmse: 0.222597\n",
      "Early stopping, best iteration is:\n",
      "[350]\ttraining's rmse: 0.214157\tvalid_1's rmse: 0.222531\n",
      "J231 training time: 2.31 sec\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.241829\tvalid_1's rmse: 0.244007\n",
      "[200]\ttraining's rmse: 0.234807\tvalid_1's rmse: 0.240649\n",
      "[300]\ttraining's rmse: 0.230901\tvalid_1's rmse: 0.240106\n",
      "[400]\ttraining's rmse: 0.22784\tvalid_1's rmse: 0.240027\n",
      "[500]\ttraining's rmse: 0.225176\tvalid_1's rmse: 0.240127\n",
      "Early stopping, best iteration is:\n",
      "[410]\ttraining's rmse: 0.227536\tvalid_1's rmse: 0.240004\n",
      "J274 training time: 3.79 sec\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.228335\tvalid_1's rmse: 0.229362\n",
      "[200]\ttraining's rmse: 0.221345\tvalid_1's rmse: 0.225274\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    101\u001b[39m params = {\n\u001b[32m    102\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mregression\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    103\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmetric\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m'\u001b[39m: -\u001b[32m1\u001b[39m\n\u001b[32m    110\u001b[39m }\n\u001b[32m    112\u001b[39m start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m model = \u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m elapsed_sec = time.time() - start_time\n\u001b[32m    124\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjunction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_sec\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sec\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/lightgbm/engine.py:322\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[32m    311\u001b[39m     cb(\n\u001b[32m    312\u001b[39m         callback.CallbackEnv(\n\u001b[32m    313\u001b[39m             model=booster,\n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m         )\n\u001b[32m    320\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43mbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = []\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/lightgbm/basic.py:4155\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, train_set, fobj)\u001b[39m\n\u001b[32m   4152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__set_objective_to_none:\n\u001b[32m   4153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[33m\"\u001b[39m\u001b[33mCannot update due to null objective function.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4154\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4159\u001b[39m )\n\u001b[32m   4160\u001b[39m \u001b[38;5;28mself\u001b[39m.__is_predicted_cur_iter = [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.__num_dataset)]\n\u001b[32m   4161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished.value == \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pickle\n",
    "import os\n",
    "import mlflow\n",
    "import json\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true = np.where(y_true == 0, np.finfo(float).eps, y_true)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    denominator = np.where(denominator == 0, np.finfo(float).eps, denominator)\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / denominator)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_value = np.percentile(y_true, percentile)\n",
    "    peak_indices = y_true >= peak_value\n",
    "    if np.sum(peak_indices) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_indices] - y_pred[peak_indices]))\n",
    "\n",
    "# -----------------------------\n",
    "# Paths and parameters\n",
    "# -----------------------------\n",
    "root = './dataset/junctions/'\n",
    "version = '0.0.1'\n",
    "os.makedirs(f'./model_trained/LightGBM_{version}', exist_ok=True)\n",
    "mlflow.set_tracking_uri(\"file:model_trained/mlruns\")\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=['junction', 'MAE', 'RMSE', 'MAPE', 'SMAPE', 'R2', 'Peak_Error'])\n",
    "feature_and_target = {}\n",
    "\n",
    "lag_steps = [1,2,3]  \n",
    "\n",
    "# lists to store global predictions\n",
    "global_y_true, global_y_pred = [], []\n",
    "with mlflow.start_run(run_name=f\"LightGBM_{version}\"):\n",
    "\n",
    "    for filename in os.listdir(root)[:70]:\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        junction = os.path.splitext(filename)[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Create lag features\n",
    "        # -----------------------------\n",
    "        for lag in lag_steps:\n",
    "            df[f'{junction}_lag{lag}'] = df[junction].shift(lag)\n",
    "\n",
    "        # Fill NaNs with zero\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        features = [col for col in df.columns if col != junction]\n",
    "        target = junction\n",
    "        feature_and_target[junction] = {\"target\": target, \"features\": features,'lags':lag_steps }\n",
    "\n",
    "        # -----------------------------\n",
    "        # Scaling\n",
    "        # -----------------------------\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        target_scaler = MinMaxScaler()\n",
    "\n",
    "        df[features] = feature_scaler.fit_transform(df[features])\n",
    "        df[[target]] = target_scaler.fit_transform(df[[target]])\n",
    "\n",
    "        # Save scalers\n",
    "        os.makedirs(f'model_trained/LightGBM_{version}/scalers', exist_ok=True)\n",
    "        with open(f'model_trained/LightGBM_{version}/scalers/{junction}_feature_scaler.save', 'wb') as f:\n",
    "            pickle.dump(feature_scaler, f)\n",
    "        with open(f'model_trained/LightGBM_{version}/scalers/{junction}_target_scaler.save', 'wb') as f:\n",
    "            pickle.dump(target_scaler, f)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Train/test split by scenario_id\n",
    "        # -----------------------------\n",
    "        scenario_ids = df['scenario_id'].unique()\n",
    "        split = int(0.8 * len(scenario_ids))\n",
    "        train_scenarios = scenario_ids[:split]\n",
    "        test_scenarios = scenario_ids[split:]\n",
    "\n",
    "        train_df = df[df['scenario_id'].isin(train_scenarios)]\n",
    "        test_df = df[df['scenario_id'].isin(test_scenarios)]\n",
    "\n",
    "        X_train, y_train = train_df[features], train_df[target]\n",
    "        X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Train LightGBM Regressor\n",
    "        # -----------------------------\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'learning_rate': 0.01,\n",
    "            'num_leaves': 31,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1\n",
    "        }\n",
    "\n",
    "        start_time = time.time()\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=5000,\n",
    "            valid_sets=[train_data, valid_data],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=100),\n",
    "                lgb.log_evaluation(period=100)\n",
    "            ]\n",
    "        )\n",
    "        elapsed_sec = time.time() - start_time\n",
    "        print(f\"{junction} training time: {elapsed_sec:.2f} sec\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Prediction & evaluation\n",
    "        # -----------------------------\n",
    "        y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "        y_test_inv = target_scaler.inverse_transform(y_test.values.reshape(-1,1))\n",
    "        y_pred_inv = target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
    "\n",
    "        # store for global metrics\n",
    "        global_y_true.append(y_test_inv)\n",
    "        global_y_pred.append(y_pred_inv)\n",
    "\n",
    "        mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "        mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv)\n",
    "        smape_val = smape(y_test_inv, y_pred_inv)\n",
    "        r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "        peak_err = peak_error(y_test_inv, y_pred_inv)\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
    "            'junction': [junction],\n",
    "            'MAE': [mae],\n",
    "            'RMSE': [rmse],\n",
    "            'MAPE': [mape],\n",
    "            'SMAPE': [smape_val],\n",
    "            'R2': [r2],\n",
    "            'Peak_Error': [peak_err]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "        # Save model\n",
    "        model.save_model(f'model_trained/LightGBM_{version}/{junction}.txt')\n",
    "\n",
    "    # -----------------------------\n",
    "    # Global metrics across all junctions\n",
    "    # -----------------------------\n",
    "    all_y_true = np.vstack(global_y_true)\n",
    "    all_y_pred = np.vstack(global_y_pred)\n",
    "\n",
    "    global_metrics = {\n",
    "        'MAE': mean_absolute_error(all_y_true, all_y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(all_y_true, all_y_pred)),\n",
    "        'MAPE': mean_absolute_percentage_error(all_y_true, all_y_pred),\n",
    "        'SMAPE': smape(all_y_true, all_y_pred),\n",
    "        'R2': r2_score(all_y_true, all_y_pred),\n",
    "        'Peak_Error': peak_error(all_y_true, all_y_pred)\n",
    "    }\n",
    "\n",
    "    print(\"\\n✅ Global Metrics:\")\n",
    "    for k,v in global_metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    mlflow.log_metrics(global_metrics)\n",
    "    \n",
    "# Save feature/target mapping\n",
    "with open(f'model_trained/LightGBM_{version}/feature_and_target.json', 'w') as f:\n",
    "    json.dump(feature_and_target, f, indent=4)\n",
    "\n",
    "print(\"✅ LightGBM training completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c7477b",
   "metadata": {},
   "source": [
    "LightGBM with engineered features (e.g., lags, moving averages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4801530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.144887\tvalid_1's rmse: 0.148378\n",
      "[200]\ttraining's rmse: 0.0926397\tvalid_1's rmse: 0.0973174\n",
      "[300]\ttraining's rmse: 0.0637599\tvalid_1's rmse: 0.0688354\n",
      "[400]\ttraining's rmse: 0.0500982\tvalid_1's rmse: 0.0552162\n",
      "[500]\ttraining's rmse: 0.0413909\tvalid_1's rmse: 0.0464797\n",
      "[600]\ttraining's rmse: 0.0354732\tvalid_1's rmse: 0.0405495\n",
      "[700]\ttraining's rmse: 0.0308796\tvalid_1's rmse: 0.0358675\n",
      "[800]\ttraining's rmse: 0.027495\tvalid_1's rmse: 0.0324845\n",
      "[900]\ttraining's rmse: 0.0248267\tvalid_1's rmse: 0.0297928\n",
      "[1000]\ttraining's rmse: 0.022751\tvalid_1's rmse: 0.0277243\n",
      "[1100]\ttraining's rmse: 0.0211293\tvalid_1's rmse: 0.0261031\n",
      "[1200]\ttraining's rmse: 0.0198349\tvalid_1's rmse: 0.0248209\n",
      "[1300]\ttraining's rmse: 0.0186921\tvalid_1's rmse: 0.0236598\n",
      "[1400]\ttraining's rmse: 0.0177362\tvalid_1's rmse: 0.0227033\n",
      "[1500]\ttraining's rmse: 0.0168984\tvalid_1's rmse: 0.0218919\n",
      "[1600]\ttraining's rmse: 0.0162107\tvalid_1's rmse: 0.0212617\n",
      "[1700]\ttraining's rmse: 0.015614\tvalid_1's rmse: 0.0207316\n",
      "[1800]\ttraining's rmse: 0.015075\tvalid_1's rmse: 0.0202798\n",
      "[1900]\ttraining's rmse: 0.0145947\tvalid_1's rmse: 0.0198613\n",
      "[2000]\ttraining's rmse: 0.0141532\tvalid_1's rmse: 0.0194725\n",
      "[2100]\ttraining's rmse: 0.013748\tvalid_1's rmse: 0.0191664\n",
      "[2200]\ttraining's rmse: 0.0133779\tvalid_1's rmse: 0.0188999\n",
      "[2300]\ttraining's rmse: 0.0130658\tvalid_1's rmse: 0.0187075\n",
      "[2400]\ttraining's rmse: 0.0127807\tvalid_1's rmse: 0.0185425\n",
      "[2500]\ttraining's rmse: 0.0124919\tvalid_1's rmse: 0.0183649\n",
      "[2600]\ttraining's rmse: 0.0122066\tvalid_1's rmse: 0.0181836\n",
      "[2700]\ttraining's rmse: 0.0119441\tvalid_1's rmse: 0.0180279\n",
      "[2800]\ttraining's rmse: 0.0117282\tvalid_1's rmse: 0.0179151\n",
      "[2900]\ttraining's rmse: 0.0115023\tvalid_1's rmse: 0.0177783\n",
      "[3000]\ttraining's rmse: 0.0112796\tvalid_1's rmse: 0.0176371\n",
      "[3100]\ttraining's rmse: 0.0110915\tvalid_1's rmse: 0.0175475\n",
      "[3200]\ttraining's rmse: 0.0108919\tvalid_1's rmse: 0.0174419\n",
      "[3300]\ttraining's rmse: 0.0106853\tvalid_1's rmse: 0.0173385\n",
      "[3400]\ttraining's rmse: 0.0105138\tvalid_1's rmse: 0.0172612\n",
      "[3500]\ttraining's rmse: 0.0103512\tvalid_1's rmse: 0.0171992\n",
      "[3600]\ttraining's rmse: 0.0101885\tvalid_1's rmse: 0.0171239\n",
      "[3700]\ttraining's rmse: 0.0100401\tvalid_1's rmse: 0.0170812\n",
      "[3800]\ttraining's rmse: 0.009885\tvalid_1's rmse: 0.0170141\n",
      "[3900]\ttraining's rmse: 0.00973442\tvalid_1's rmse: 0.0169502\n",
      "[4000]\ttraining's rmse: 0.00960544\tvalid_1's rmse: 0.0169006\n",
      "[4100]\ttraining's rmse: 0.00946788\tvalid_1's rmse: 0.0168556\n",
      "[4200]\ttraining's rmse: 0.00931209\tvalid_1's rmse: 0.0167817\n",
      "[4300]\ttraining's rmse: 0.00919317\tvalid_1's rmse: 0.0167379\n",
      "[4400]\ttraining's rmse: 0.00906765\tvalid_1's rmse: 0.0166906\n",
      "[4500]\ttraining's rmse: 0.0089352\tvalid_1's rmse: 0.0166264\n",
      "[4600]\ttraining's rmse: 0.00881418\tvalid_1's rmse: 0.0165823\n",
      "[4700]\ttraining's rmse: 0.00869534\tvalid_1's rmse: 0.0165373\n",
      "[4800]\ttraining's rmse: 0.00858259\tvalid_1's rmse: 0.0164971\n",
      "[4900]\ttraining's rmse: 0.00847819\tvalid_1's rmse: 0.0164558\n",
      "[5000]\ttraining's rmse: 0.00837749\tvalid_1's rmse: 0.0164214\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4998]\ttraining's rmse: 0.00837877\tvalid_1's rmse: 0.0164212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4136/1409757792.py:146: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  metrics_df = pd.concat([metrics_df, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.150359\tvalid_1's rmse: 0.151748\n",
      "[200]\ttraining's rmse: 0.0975872\tvalid_1's rmse: 0.100216\n",
      "[300]\ttraining's rmse: 0.0667845\tvalid_1's rmse: 0.0701986\n",
      "[400]\ttraining's rmse: 0.0516372\tvalid_1's rmse: 0.0555259\n",
      "[500]\ttraining's rmse: 0.0416702\tvalid_1's rmse: 0.0457117\n",
      "[600]\ttraining's rmse: 0.0350652\tvalid_1's rmse: 0.0392248\n",
      "[700]\ttraining's rmse: 0.0301276\tvalid_1's rmse: 0.0342822\n",
      "[800]\ttraining's rmse: 0.0266377\tvalid_1's rmse: 0.0308288\n",
      "[900]\ttraining's rmse: 0.0239211\tvalid_1's rmse: 0.0281673\n",
      "[1000]\ttraining's rmse: 0.021835\tvalid_1's rmse: 0.0261088\n",
      "[1100]\ttraining's rmse: 0.0202622\tvalid_1's rmse: 0.0245822\n",
      "[1200]\ttraining's rmse: 0.018947\tvalid_1's rmse: 0.0233032\n",
      "[1300]\ttraining's rmse: 0.0178416\tvalid_1's rmse: 0.0222347\n",
      "[1400]\ttraining's rmse: 0.0169509\tvalid_1's rmse: 0.0213508\n",
      "[1500]\ttraining's rmse: 0.0161952\tvalid_1's rmse: 0.0206344\n",
      "[1600]\ttraining's rmse: 0.0155724\tvalid_1's rmse: 0.020074\n",
      "[1700]\ttraining's rmse: 0.0150538\tvalid_1's rmse: 0.0196137\n",
      "[1800]\ttraining's rmse: 0.0145803\tvalid_1's rmse: 0.0192279\n",
      "[1900]\ttraining's rmse: 0.0141577\tvalid_1's rmse: 0.018897\n",
      "[2000]\ttraining's rmse: 0.0137493\tvalid_1's rmse: 0.0185901\n",
      "[2100]\ttraining's rmse: 0.0134098\tvalid_1's rmse: 0.0183369\n",
      "[2200]\ttraining's rmse: 0.0130949\tvalid_1's rmse: 0.0181158\n",
      "[2300]\ttraining's rmse: 0.0128285\tvalid_1's rmse: 0.0179481\n",
      "[2400]\ttraining's rmse: 0.0125575\tvalid_1's rmse: 0.0177795\n",
      "[2500]\ttraining's rmse: 0.0122959\tvalid_1's rmse: 0.0176088\n",
      "[2600]\ttraining's rmse: 0.0120332\tvalid_1's rmse: 0.0174465\n",
      "[2700]\ttraining's rmse: 0.0117894\tvalid_1's rmse: 0.0172999\n",
      "[2800]\ttraining's rmse: 0.0115802\tvalid_1's rmse: 0.0171914\n",
      "[2900]\ttraining's rmse: 0.0113824\tvalid_1's rmse: 0.0170846\n",
      "[3000]\ttraining's rmse: 0.0111899\tvalid_1's rmse: 0.0169774\n",
      "[3100]\ttraining's rmse: 0.0110223\tvalid_1's rmse: 0.0169067\n",
      "[3200]\ttraining's rmse: 0.010832\tvalid_1's rmse: 0.0168041\n",
      "[3300]\ttraining's rmse: 0.0106545\tvalid_1's rmse: 0.0167195\n",
      "[3400]\ttraining's rmse: 0.0105012\tvalid_1's rmse: 0.0166585\n",
      "[3500]\ttraining's rmse: 0.0103554\tvalid_1's rmse: 0.0166001\n",
      "[3600]\ttraining's rmse: 0.0102086\tvalid_1's rmse: 0.0165359\n",
      "[3700]\ttraining's rmse: 0.010075\tvalid_1's rmse: 0.0164861\n",
      "[3800]\ttraining's rmse: 0.00993338\tvalid_1's rmse: 0.0164258\n",
      "[3900]\ttraining's rmse: 0.00977309\tvalid_1's rmse: 0.0163492\n",
      "[4000]\ttraining's rmse: 0.00963293\tvalid_1's rmse: 0.0162838\n",
      "[4100]\ttraining's rmse: 0.00951108\tvalid_1's rmse: 0.0162429\n",
      "[4200]\ttraining's rmse: 0.00939127\tvalid_1's rmse: 0.0161931\n",
      "[4300]\ttraining's rmse: 0.00926307\tvalid_1's rmse: 0.0161314\n",
      "[4400]\ttraining's rmse: 0.00914559\tvalid_1's rmse: 0.0160977\n",
      "[4500]\ttraining's rmse: 0.00901517\tvalid_1's rmse: 0.0160413\n",
      "[4600]\ttraining's rmse: 0.00889546\tvalid_1's rmse: 0.0159927\n",
      "[4700]\ttraining's rmse: 0.0088038\tvalid_1's rmse: 0.0159723\n",
      "[4800]\ttraining's rmse: 0.00870768\tvalid_1's rmse: 0.0159453\n",
      "[4900]\ttraining's rmse: 0.00861821\tvalid_1's rmse: 0.0159263\n",
      "[5000]\ttraining's rmse: 0.00851782\tvalid_1's rmse: 0.015906\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00851782\tvalid_1's rmse: 0.015906\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.146365\tvalid_1's rmse: 0.14768\n",
      "[200]\ttraining's rmse: 0.0949382\tvalid_1's rmse: 0.0982492\n",
      "[300]\ttraining's rmse: 0.0660119\tvalid_1's rmse: 0.0702823\n",
      "[400]\ttraining's rmse: 0.0519003\tvalid_1's rmse: 0.0566036\n",
      "[500]\ttraining's rmse: 0.0426858\tvalid_1's rmse: 0.0476438\n",
      "[600]\ttraining's rmse: 0.0364333\tvalid_1's rmse: 0.041569\n",
      "[700]\ttraining's rmse: 0.0315025\tvalid_1's rmse: 0.036702\n",
      "[800]\ttraining's rmse: 0.0278771\tvalid_1's rmse: 0.0331279\n",
      "[900]\ttraining's rmse: 0.0250101\tvalid_1's rmse: 0.0303228\n",
      "[1000]\ttraining's rmse: 0.0227557\tvalid_1's rmse: 0.0280918\n",
      "[1100]\ttraining's rmse: 0.0210121\tvalid_1's rmse: 0.0263754\n",
      "[1200]\ttraining's rmse: 0.0195969\tvalid_1's rmse: 0.0249697\n",
      "[1300]\ttraining's rmse: 0.0183673\tvalid_1's rmse: 0.0237552\n",
      "[1400]\ttraining's rmse: 0.0173514\tvalid_1's rmse: 0.0227204\n",
      "[1500]\ttraining's rmse: 0.0164845\tvalid_1's rmse: 0.0218621\n",
      "[1600]\ttraining's rmse: 0.0157769\tvalid_1's rmse: 0.0211656\n",
      "[1700]\ttraining's rmse: 0.0151823\tvalid_1's rmse: 0.020589\n",
      "[1800]\ttraining's rmse: 0.0146642\tvalid_1's rmse: 0.0201267\n",
      "[1900]\ttraining's rmse: 0.0141846\tvalid_1's rmse: 0.0197005\n",
      "[2000]\ttraining's rmse: 0.0137304\tvalid_1's rmse: 0.0192862\n",
      "[2100]\ttraining's rmse: 0.0133486\tvalid_1's rmse: 0.0189585\n",
      "[2200]\ttraining's rmse: 0.0130045\tvalid_1's rmse: 0.0186801\n",
      "[2300]\ttraining's rmse: 0.0126943\tvalid_1's rmse: 0.0184766\n",
      "[2400]\ttraining's rmse: 0.0124204\tvalid_1's rmse: 0.0182829\n",
      "[2500]\ttraining's rmse: 0.0121549\tvalid_1's rmse: 0.0181072\n",
      "[2600]\ttraining's rmse: 0.0118813\tvalid_1's rmse: 0.0179082\n",
      "[2700]\ttraining's rmse: 0.011633\tvalid_1's rmse: 0.0177411\n",
      "[2800]\ttraining's rmse: 0.0114073\tvalid_1's rmse: 0.0176087\n",
      "[2900]\ttraining's rmse: 0.0111979\tvalid_1's rmse: 0.0174727\n",
      "[3000]\ttraining's rmse: 0.0110123\tvalid_1's rmse: 0.0173689\n",
      "[3100]\ttraining's rmse: 0.0108275\tvalid_1's rmse: 0.0172785\n",
      "[3200]\ttraining's rmse: 0.010627\tvalid_1's rmse: 0.0171607\n",
      "[3300]\ttraining's rmse: 0.0104416\tvalid_1's rmse: 0.0170497\n",
      "[3400]\ttraining's rmse: 0.0102824\tvalid_1's rmse: 0.0169866\n",
      "[3500]\ttraining's rmse: 0.0101252\tvalid_1's rmse: 0.0169111\n",
      "[3600]\ttraining's rmse: 0.00996909\tvalid_1's rmse: 0.0168398\n",
      "[3700]\ttraining's rmse: 0.00983357\tvalid_1's rmse: 0.0167951\n",
      "[3800]\ttraining's rmse: 0.00966981\tvalid_1's rmse: 0.0166971\n",
      "[3900]\ttraining's rmse: 0.00952451\tvalid_1's rmse: 0.0166371\n",
      "[4000]\ttraining's rmse: 0.00939939\tvalid_1's rmse: 0.0165871\n",
      "[4100]\ttraining's rmse: 0.00926652\tvalid_1's rmse: 0.0165263\n",
      "[4200]\ttraining's rmse: 0.00914223\tvalid_1's rmse: 0.0164759\n",
      "[4300]\ttraining's rmse: 0.00902752\tvalid_1's rmse: 0.0164352\n",
      "[4400]\ttraining's rmse: 0.00890284\tvalid_1's rmse: 0.0163916\n",
      "[4500]\ttraining's rmse: 0.00877844\tvalid_1's rmse: 0.0163297\n",
      "[4600]\ttraining's rmse: 0.0086636\tvalid_1's rmse: 0.0162842\n",
      "[4700]\ttraining's rmse: 0.0085664\tvalid_1's rmse: 0.0162515\n",
      "[4800]\ttraining's rmse: 0.00845471\tvalid_1's rmse: 0.0162095\n",
      "[4900]\ttraining's rmse: 0.00835015\tvalid_1's rmse: 0.0161701\n",
      "[5000]\ttraining's rmse: 0.00825359\tvalid_1's rmse: 0.0161354\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00825359\tvalid_1's rmse: 0.0161354\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.134519\tvalid_1's rmse: 0.135113\n",
      "[200]\ttraining's rmse: 0.0828897\tvalid_1's rmse: 0.0852598\n",
      "[300]\ttraining's rmse: 0.0581408\tvalid_1's rmse: 0.0614166\n",
      "[400]\ttraining's rmse: 0.045081\tvalid_1's rmse: 0.0487666\n",
      "[500]\ttraining's rmse: 0.0365991\tvalid_1's rmse: 0.0405632\n",
      "[600]\ttraining's rmse: 0.031089\tvalid_1's rmse: 0.035209\n",
      "[700]\ttraining's rmse: 0.0273669\tvalid_1's rmse: 0.0315918\n",
      "[800]\ttraining's rmse: 0.0245619\tvalid_1's rmse: 0.0289062\n",
      "[900]\ttraining's rmse: 0.0223367\tvalid_1's rmse: 0.0267553\n",
      "[1000]\ttraining's rmse: 0.0204695\tvalid_1's rmse: 0.0248943\n",
      "[1100]\ttraining's rmse: 0.0188895\tvalid_1's rmse: 0.0233066\n",
      "[1200]\ttraining's rmse: 0.017688\tvalid_1's rmse: 0.0221412\n",
      "[1300]\ttraining's rmse: 0.0166852\tvalid_1's rmse: 0.0211477\n",
      "[1400]\ttraining's rmse: 0.0158621\tvalid_1's rmse: 0.0203526\n",
      "[1500]\ttraining's rmse: 0.0151868\tvalid_1's rmse: 0.0197126\n",
      "[1600]\ttraining's rmse: 0.0145858\tvalid_1's rmse: 0.0191734\n",
      "[1700]\ttraining's rmse: 0.0140959\tvalid_1's rmse: 0.0187623\n",
      "[1800]\ttraining's rmse: 0.0136382\tvalid_1's rmse: 0.0183793\n",
      "[1900]\ttraining's rmse: 0.013223\tvalid_1's rmse: 0.018037\n",
      "[2000]\ttraining's rmse: 0.012846\tvalid_1's rmse: 0.0177417\n",
      "[2100]\ttraining's rmse: 0.0124882\tvalid_1's rmse: 0.0174789\n",
      "[2200]\ttraining's rmse: 0.0121708\tvalid_1's rmse: 0.0172595\n",
      "[2300]\ttraining's rmse: 0.0118921\tvalid_1's rmse: 0.0170846\n",
      "[2400]\ttraining's rmse: 0.0116235\tvalid_1's rmse: 0.0169024\n",
      "[2500]\ttraining's rmse: 0.0113754\tvalid_1's rmse: 0.0167496\n",
      "[2600]\ttraining's rmse: 0.0111103\tvalid_1's rmse: 0.016554\n",
      "[2700]\ttraining's rmse: 0.0108948\tvalid_1's rmse: 0.0164378\n",
      "[2800]\ttraining's rmse: 0.0106804\tvalid_1's rmse: 0.016312\n",
      "[2900]\ttraining's rmse: 0.0104722\tvalid_1's rmse: 0.0161887\n",
      "[3000]\ttraining's rmse: 0.0102703\tvalid_1's rmse: 0.0160591\n",
      "[3100]\ttraining's rmse: 0.0100766\tvalid_1's rmse: 0.0159473\n",
      "[3200]\ttraining's rmse: 0.00990289\tvalid_1's rmse: 0.0158558\n",
      "[3300]\ttraining's rmse: 0.0097402\tvalid_1's rmse: 0.0157768\n",
      "[3400]\ttraining's rmse: 0.00958696\tvalid_1's rmse: 0.0157176\n",
      "[3500]\ttraining's rmse: 0.00942709\tvalid_1's rmse: 0.0156341\n",
      "[3600]\ttraining's rmse: 0.00927991\tvalid_1's rmse: 0.0155652\n",
      "[3700]\ttraining's rmse: 0.00913857\tvalid_1's rmse: 0.0155017\n",
      "[3800]\ttraining's rmse: 0.00900563\tvalid_1's rmse: 0.0154496\n",
      "[3900]\ttraining's rmse: 0.00886196\tvalid_1's rmse: 0.0153716\n",
      "[4000]\ttraining's rmse: 0.00873908\tvalid_1's rmse: 0.0153261\n",
      "[4100]\ttraining's rmse: 0.00862217\tvalid_1's rmse: 0.0152772\n",
      "[4200]\ttraining's rmse: 0.0084845\tvalid_1's rmse: 0.0152057\n",
      "[4300]\ttraining's rmse: 0.00836157\tvalid_1's rmse: 0.0151542\n",
      "[4400]\ttraining's rmse: 0.00825424\tvalid_1's rmse: 0.0151124\n",
      "[4500]\ttraining's rmse: 0.00814438\tvalid_1's rmse: 0.0150682\n",
      "[4600]\ttraining's rmse: 0.00804959\tvalid_1's rmse: 0.0150382\n",
      "[4700]\ttraining's rmse: 0.00795351\tvalid_1's rmse: 0.0150109\n",
      "[4800]\ttraining's rmse: 0.00784067\tvalid_1's rmse: 0.0149647\n",
      "[4900]\ttraining's rmse: 0.00773522\tvalid_1's rmse: 0.0149209\n",
      "[5000]\ttraining's rmse: 0.00762437\tvalid_1's rmse: 0.0148616\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00762437\tvalid_1's rmse: 0.0148616\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.13948\tvalid_1's rmse: 0.140974\n",
      "[200]\ttraining's rmse: 0.0853296\tvalid_1's rmse: 0.0889658\n",
      "[300]\ttraining's rmse: 0.0596055\tvalid_1's rmse: 0.0640179\n",
      "[400]\ttraining's rmse: 0.046268\tvalid_1's rmse: 0.050946\n",
      "[500]\ttraining's rmse: 0.0377129\tvalid_1's rmse: 0.0424661\n",
      "[600]\ttraining's rmse: 0.0321267\tvalid_1's rmse: 0.0369585\n",
      "[700]\ttraining's rmse: 0.028339\tvalid_1's rmse: 0.0332932\n",
      "[800]\ttraining's rmse: 0.0254964\tvalid_1's rmse: 0.0305137\n",
      "[900]\ttraining's rmse: 0.023288\tvalid_1's rmse: 0.0283198\n",
      "[1000]\ttraining's rmse: 0.0214099\tvalid_1's rmse: 0.0264257\n",
      "[1100]\ttraining's rmse: 0.0198671\tvalid_1's rmse: 0.0248663\n",
      "[1200]\ttraining's rmse: 0.0186719\tvalid_1's rmse: 0.0236488\n",
      "[1300]\ttraining's rmse: 0.0177098\tvalid_1's rmse: 0.0226812\n",
      "[1400]\ttraining's rmse: 0.0169008\tvalid_1's rmse: 0.0218958\n",
      "[1500]\ttraining's rmse: 0.0162081\tvalid_1's rmse: 0.0212276\n",
      "[1600]\ttraining's rmse: 0.0155831\tvalid_1's rmse: 0.0206655\n",
      "[1700]\ttraining's rmse: 0.0150563\tvalid_1's rmse: 0.0202132\n",
      "[1800]\ttraining's rmse: 0.0145906\tvalid_1's rmse: 0.0198197\n",
      "[1900]\ttraining's rmse: 0.0141827\tvalid_1's rmse: 0.0194835\n",
      "[2000]\ttraining's rmse: 0.0137839\tvalid_1's rmse: 0.0191707\n",
      "[2100]\ttraining's rmse: 0.0134281\tvalid_1's rmse: 0.0188699\n",
      "[2200]\ttraining's rmse: 0.0130695\tvalid_1's rmse: 0.0185945\n",
      "[2300]\ttraining's rmse: 0.0127568\tvalid_1's rmse: 0.0183887\n",
      "[2400]\ttraining's rmse: 0.0124688\tvalid_1's rmse: 0.0181857\n",
      "[2500]\ttraining's rmse: 0.0122219\tvalid_1's rmse: 0.0180039\n",
      "[2600]\ttraining's rmse: 0.0119509\tvalid_1's rmse: 0.0177991\n",
      "[2700]\ttraining's rmse: 0.0117105\tvalid_1's rmse: 0.0176334\n",
      "[2800]\ttraining's rmse: 0.0114886\tvalid_1's rmse: 0.017493\n",
      "[2900]\ttraining's rmse: 0.0112525\tvalid_1's rmse: 0.0173509\n",
      "[3000]\ttraining's rmse: 0.0110618\tvalid_1's rmse: 0.0172437\n",
      "[3100]\ttraining's rmse: 0.0108533\tvalid_1's rmse: 0.0171021\n",
      "[3200]\ttraining's rmse: 0.0106696\tvalid_1's rmse: 0.0169969\n",
      "[3300]\ttraining's rmse: 0.0104864\tvalid_1's rmse: 0.0169052\n",
      "[3400]\ttraining's rmse: 0.0103189\tvalid_1's rmse: 0.0168222\n",
      "[3500]\ttraining's rmse: 0.0101577\tvalid_1's rmse: 0.0167271\n",
      "[3600]\ttraining's rmse: 0.0100176\tvalid_1's rmse: 0.0166665\n",
      "[3700]\ttraining's rmse: 0.0098604\tvalid_1's rmse: 0.0165911\n",
      "[3800]\ttraining's rmse: 0.00972019\tvalid_1's rmse: 0.0165063\n",
      "[3900]\ttraining's rmse: 0.00958196\tvalid_1's rmse: 0.0164295\n",
      "[4000]\ttraining's rmse: 0.00944615\tvalid_1's rmse: 0.0163735\n",
      "[4100]\ttraining's rmse: 0.0093139\tvalid_1's rmse: 0.016295\n",
      "[4200]\ttraining's rmse: 0.00917462\tvalid_1's rmse: 0.016214\n",
      "[4300]\ttraining's rmse: 0.00904351\tvalid_1's rmse: 0.0161473\n",
      "[4400]\ttraining's rmse: 0.00892603\tvalid_1's rmse: 0.0160996\n",
      "[4500]\ttraining's rmse: 0.00879647\tvalid_1's rmse: 0.0160363\n",
      "[4600]\ttraining's rmse: 0.00865641\tvalid_1's rmse: 0.0159596\n",
      "[4700]\ttraining's rmse: 0.00853896\tvalid_1's rmse: 0.0159004\n",
      "[4800]\ttraining's rmse: 0.00842383\tvalid_1's rmse: 0.0158488\n",
      "[4900]\ttraining's rmse: 0.00830718\tvalid_1's rmse: 0.0157975\n",
      "[5000]\ttraining's rmse: 0.00820604\tvalid_1's rmse: 0.0157633\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4995]\ttraining's rmse: 0.00821038\tvalid_1's rmse: 0.0157631\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.142111\tvalid_1's rmse: 0.143604\n",
      "[200]\ttraining's rmse: 0.0921783\tvalid_1's rmse: 0.0957281\n",
      "[300]\ttraining's rmse: 0.0639405\tvalid_1's rmse: 0.0684431\n",
      "[400]\ttraining's rmse: 0.0498002\tvalid_1's rmse: 0.0545999\n",
      "[500]\ttraining's rmse: 0.0404051\tvalid_1's rmse: 0.0453616\n",
      "[600]\ttraining's rmse: 0.0340808\tvalid_1's rmse: 0.039161\n",
      "[700]\ttraining's rmse: 0.0291701\tvalid_1's rmse: 0.0342269\n",
      "[800]\ttraining's rmse: 0.0256944\tvalid_1's rmse: 0.0307389\n",
      "[900]\ttraining's rmse: 0.0228873\tvalid_1's rmse: 0.0279392\n",
      "[1000]\ttraining's rmse: 0.0207435\tvalid_1's rmse: 0.0257901\n",
      "[1100]\ttraining's rmse: 0.0190951\tvalid_1's rmse: 0.0241485\n",
      "[1200]\ttraining's rmse: 0.017753\tvalid_1's rmse: 0.0228115\n",
      "[1300]\ttraining's rmse: 0.0165937\tvalid_1's rmse: 0.0216837\n",
      "[1400]\ttraining's rmse: 0.01563\tvalid_1's rmse: 0.0206926\n",
      "[1500]\ttraining's rmse: 0.0148352\tvalid_1's rmse: 0.0198659\n",
      "[1600]\ttraining's rmse: 0.0141853\tvalid_1's rmse: 0.01921\n",
      "[1700]\ttraining's rmse: 0.0136152\tvalid_1's rmse: 0.0186709\n",
      "[1800]\ttraining's rmse: 0.0131311\tvalid_1's rmse: 0.0181886\n",
      "[1900]\ttraining's rmse: 0.0127093\tvalid_1's rmse: 0.0178017\n",
      "[2000]\ttraining's rmse: 0.0123194\tvalid_1's rmse: 0.0174647\n",
      "[2100]\ttraining's rmse: 0.0119584\tvalid_1's rmse: 0.0171671\n",
      "[2200]\ttraining's rmse: 0.0116445\tvalid_1's rmse: 0.016915\n",
      "[2300]\ttraining's rmse: 0.0113572\tvalid_1's rmse: 0.0166989\n",
      "[2400]\ttraining's rmse: 0.0110809\tvalid_1's rmse: 0.0165044\n",
      "[2500]\ttraining's rmse: 0.0108245\tvalid_1's rmse: 0.0163214\n",
      "[2600]\ttraining's rmse: 0.0105769\tvalid_1's rmse: 0.0161502\n",
      "[2700]\ttraining's rmse: 0.0103644\tvalid_1's rmse: 0.0160321\n",
      "[2800]\ttraining's rmse: 0.0101419\tvalid_1's rmse: 0.0158808\n",
      "[2900]\ttraining's rmse: 0.00994201\tvalid_1's rmse: 0.0157509\n",
      "[3000]\ttraining's rmse: 0.00976105\tvalid_1's rmse: 0.0156597\n",
      "[3100]\ttraining's rmse: 0.00958976\tvalid_1's rmse: 0.0155653\n",
      "[3200]\ttraining's rmse: 0.00942\tvalid_1's rmse: 0.0154571\n",
      "[3300]\ttraining's rmse: 0.00925516\tvalid_1's rmse: 0.0153731\n",
      "[3400]\ttraining's rmse: 0.00908845\tvalid_1's rmse: 0.0152707\n",
      "[3500]\ttraining's rmse: 0.00893962\tvalid_1's rmse: 0.0151968\n",
      "[3600]\ttraining's rmse: 0.00879021\tvalid_1's rmse: 0.0151208\n",
      "[3700]\ttraining's rmse: 0.00865693\tvalid_1's rmse: 0.0150543\n",
      "[3800]\ttraining's rmse: 0.00851972\tvalid_1's rmse: 0.01498\n",
      "[3900]\ttraining's rmse: 0.00839195\tvalid_1's rmse: 0.0149194\n",
      "[4000]\ttraining's rmse: 0.00827902\tvalid_1's rmse: 0.0148708\n",
      "[4100]\ttraining's rmse: 0.00815186\tvalid_1's rmse: 0.0148057\n",
      "[4200]\ttraining's rmse: 0.00803081\tvalid_1's rmse: 0.0147486\n",
      "[4300]\ttraining's rmse: 0.0079252\tvalid_1's rmse: 0.0147116\n",
      "[4400]\ttraining's rmse: 0.00781926\tvalid_1's rmse: 0.0146885\n",
      "[4500]\ttraining's rmse: 0.00771391\tvalid_1's rmse: 0.0146449\n",
      "[4600]\ttraining's rmse: 0.00762372\tvalid_1's rmse: 0.0146102\n",
      "[4700]\ttraining's rmse: 0.00751988\tvalid_1's rmse: 0.0145551\n",
      "[4800]\ttraining's rmse: 0.00742799\tvalid_1's rmse: 0.0145228\n",
      "[4900]\ttraining's rmse: 0.00733734\tvalid_1's rmse: 0.0144836\n",
      "[5000]\ttraining's rmse: 0.00725098\tvalid_1's rmse: 0.0144497\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00725098\tvalid_1's rmse: 0.0144497\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.153531\tvalid_1's rmse: 0.15418\n",
      "[200]\ttraining's rmse: 0.0998379\tvalid_1's rmse: 0.102516\n",
      "[300]\ttraining's rmse: 0.0682879\tvalid_1's rmse: 0.0720493\n",
      "[400]\ttraining's rmse: 0.0527223\tvalid_1's rmse: 0.057017\n",
      "[500]\ttraining's rmse: 0.0425396\tvalid_1's rmse: 0.0470734\n",
      "[600]\ttraining's rmse: 0.0357663\tvalid_1's rmse: 0.0404194\n",
      "[700]\ttraining's rmse: 0.0305738\tvalid_1's rmse: 0.0352746\n",
      "[800]\ttraining's rmse: 0.0268939\tvalid_1's rmse: 0.0316298\n",
      "[900]\ttraining's rmse: 0.0240577\tvalid_1's rmse: 0.0288054\n",
      "[1000]\ttraining's rmse: 0.0218773\tvalid_1's rmse: 0.0266205\n",
      "[1100]\ttraining's rmse: 0.0202638\tvalid_1's rmse: 0.0250174\n",
      "[1200]\ttraining's rmse: 0.0189726\tvalid_1's rmse: 0.0237372\n",
      "[1300]\ttraining's rmse: 0.0178782\tvalid_1's rmse: 0.0226646\n",
      "[1400]\ttraining's rmse: 0.0169736\tvalid_1's rmse: 0.0217589\n",
      "[1500]\ttraining's rmse: 0.0162388\tvalid_1's rmse: 0.0210441\n",
      "[1600]\ttraining's rmse: 0.0156311\tvalid_1's rmse: 0.0204885\n",
      "[1700]\ttraining's rmse: 0.0151196\tvalid_1's rmse: 0.0200287\n",
      "[1800]\ttraining's rmse: 0.0146511\tvalid_1's rmse: 0.0196375\n",
      "[1900]\ttraining's rmse: 0.0142394\tvalid_1's rmse: 0.0192973\n",
      "[2000]\ttraining's rmse: 0.0138846\tvalid_1's rmse: 0.0190319\n",
      "[2100]\ttraining's rmse: 0.013555\tvalid_1's rmse: 0.0187988\n",
      "[2200]\ttraining's rmse: 0.0132543\tvalid_1's rmse: 0.0186009\n",
      "[2300]\ttraining's rmse: 0.0129725\tvalid_1's rmse: 0.018421\n",
      "[2400]\ttraining's rmse: 0.0127095\tvalid_1's rmse: 0.0182601\n",
      "[2500]\ttraining's rmse: 0.0124595\tvalid_1's rmse: 0.0181045\n",
      "[2600]\ttraining's rmse: 0.0122069\tvalid_1's rmse: 0.0179407\n",
      "[2700]\ttraining's rmse: 0.0119716\tvalid_1's rmse: 0.0178045\n",
      "[2800]\ttraining's rmse: 0.0117612\tvalid_1's rmse: 0.0176902\n",
      "[2900]\ttraining's rmse: 0.011559\tvalid_1's rmse: 0.0175736\n",
      "[3000]\ttraining's rmse: 0.0113599\tvalid_1's rmse: 0.0174557\n",
      "[3100]\ttraining's rmse: 0.0111948\tvalid_1's rmse: 0.0173855\n",
      "[3200]\ttraining's rmse: 0.0109983\tvalid_1's rmse: 0.0172798\n",
      "[3300]\ttraining's rmse: 0.0108413\tvalid_1's rmse: 0.0172177\n",
      "[3400]\ttraining's rmse: 0.010674\tvalid_1's rmse: 0.0171288\n",
      "[3500]\ttraining's rmse: 0.0105322\tvalid_1's rmse: 0.017084\n",
      "[3600]\ttraining's rmse: 0.010387\tvalid_1's rmse: 0.0170199\n",
      "[3700]\ttraining's rmse: 0.0102572\tvalid_1's rmse: 0.0169719\n",
      "[3800]\ttraining's rmse: 0.0101169\tvalid_1's rmse: 0.0169141\n",
      "[3900]\ttraining's rmse: 0.009986\tvalid_1's rmse: 0.016864\n",
      "[4000]\ttraining's rmse: 0.00984825\tvalid_1's rmse: 0.0168088\n",
      "[4100]\ttraining's rmse: 0.00970266\tvalid_1's rmse: 0.0167343\n",
      "[4200]\ttraining's rmse: 0.00956777\tvalid_1's rmse: 0.0166802\n",
      "[4300]\ttraining's rmse: 0.00943955\tvalid_1's rmse: 0.0166273\n",
      "[4400]\ttraining's rmse: 0.00931954\tvalid_1's rmse: 0.0165812\n",
      "[4500]\ttraining's rmse: 0.00921411\tvalid_1's rmse: 0.0165439\n",
      "[4600]\ttraining's rmse: 0.00909784\tvalid_1's rmse: 0.0164897\n",
      "[4700]\ttraining's rmse: 0.0089966\tvalid_1's rmse: 0.0164691\n",
      "[4800]\ttraining's rmse: 0.00888913\tvalid_1's rmse: 0.0164305\n",
      "[4900]\ttraining's rmse: 0.00879375\tvalid_1's rmse: 0.0164013\n",
      "[5000]\ttraining's rmse: 0.00869467\tvalid_1's rmse: 0.0163733\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4997]\ttraining's rmse: 0.00869691\tvalid_1's rmse: 0.016372\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.14798\tvalid_1's rmse: 0.149589\n",
      "[200]\ttraining's rmse: 0.0949505\tvalid_1's rmse: 0.0986497\n",
      "[300]\ttraining's rmse: 0.0672476\tvalid_1's rmse: 0.0718972\n",
      "[400]\ttraining's rmse: 0.0522372\tvalid_1's rmse: 0.0574426\n",
      "[500]\ttraining's rmse: 0.0436374\tvalid_1's rmse: 0.0491446\n",
      "[600]\ttraining's rmse: 0.0374675\tvalid_1's rmse: 0.0432456\n",
      "[700]\ttraining's rmse: 0.0326169\tvalid_1's rmse: 0.038625\n",
      "[800]\ttraining's rmse: 0.0291078\tvalid_1's rmse: 0.0352702\n",
      "[900]\ttraining's rmse: 0.0264242\tvalid_1's rmse: 0.0327298\n",
      "[1000]\ttraining's rmse: 0.0242345\tvalid_1's rmse: 0.0306099\n",
      "[1100]\ttraining's rmse: 0.022289\tvalid_1's rmse: 0.0286852\n",
      "[1200]\ttraining's rmse: 0.0207385\tvalid_1's rmse: 0.0271665\n",
      "[1300]\ttraining's rmse: 0.0194334\tvalid_1's rmse: 0.0258672\n",
      "[1400]\ttraining's rmse: 0.0183077\tvalid_1's rmse: 0.0247517\n",
      "[1500]\ttraining's rmse: 0.0174223\tvalid_1's rmse: 0.0239287\n",
      "[1600]\ttraining's rmse: 0.0166378\tvalid_1's rmse: 0.0231839\n",
      "[1700]\ttraining's rmse: 0.0159493\tvalid_1's rmse: 0.02253\n",
      "[1800]\ttraining's rmse: 0.0153071\tvalid_1's rmse: 0.0219007\n",
      "[1900]\ttraining's rmse: 0.0147783\tvalid_1's rmse: 0.0214449\n",
      "[2000]\ttraining's rmse: 0.0142917\tvalid_1's rmse: 0.0209964\n",
      "[2100]\ttraining's rmse: 0.0138945\tvalid_1's rmse: 0.0206635\n",
      "[2200]\ttraining's rmse: 0.0134958\tvalid_1's rmse: 0.0203353\n",
      "[2300]\ttraining's rmse: 0.0131459\tvalid_1's rmse: 0.0200589\n",
      "[2400]\ttraining's rmse: 0.0128099\tvalid_1's rmse: 0.0197809\n",
      "[2500]\ttraining's rmse: 0.0125082\tvalid_1's rmse: 0.0195507\n",
      "[2600]\ttraining's rmse: 0.0122172\tvalid_1's rmse: 0.0193277\n",
      "[2700]\ttraining's rmse: 0.0119308\tvalid_1's rmse: 0.0191252\n",
      "[2800]\ttraining's rmse: 0.0116995\tvalid_1's rmse: 0.0189583\n",
      "[2900]\ttraining's rmse: 0.011465\tvalid_1's rmse: 0.0187966\n",
      "[3000]\ttraining's rmse: 0.011248\tvalid_1's rmse: 0.0186631\n",
      "[3100]\ttraining's rmse: 0.0110291\tvalid_1's rmse: 0.0185361\n",
      "[3200]\ttraining's rmse: 0.0108282\tvalid_1's rmse: 0.018427\n",
      "[3300]\ttraining's rmse: 0.0106242\tvalid_1's rmse: 0.0183045\n",
      "[3400]\ttraining's rmse: 0.0104505\tvalid_1's rmse: 0.018214\n",
      "[3500]\ttraining's rmse: 0.010258\tvalid_1's rmse: 0.0180965\n",
      "[3600]\ttraining's rmse: 0.0100787\tvalid_1's rmse: 0.0179969\n",
      "[3700]\ttraining's rmse: 0.0099162\tvalid_1's rmse: 0.0179132\n",
      "[3800]\ttraining's rmse: 0.00974932\tvalid_1's rmse: 0.0178257\n",
      "[3900]\ttraining's rmse: 0.00960042\tvalid_1's rmse: 0.0177644\n",
      "[4000]\ttraining's rmse: 0.00945806\tvalid_1's rmse: 0.017704\n",
      "[4100]\ttraining's rmse: 0.00932745\tvalid_1's rmse: 0.017646\n",
      "[4200]\ttraining's rmse: 0.00918164\tvalid_1's rmse: 0.017563\n",
      "[4300]\ttraining's rmse: 0.00905637\tvalid_1's rmse: 0.0175086\n",
      "[4400]\ttraining's rmse: 0.00892856\tvalid_1's rmse: 0.0174486\n",
      "[4500]\ttraining's rmse: 0.00881256\tvalid_1's rmse: 0.0174104\n",
      "[4600]\ttraining's rmse: 0.00868974\tvalid_1's rmse: 0.0173473\n",
      "[4700]\ttraining's rmse: 0.00858635\tvalid_1's rmse: 0.0173103\n",
      "[4800]\ttraining's rmse: 0.00846275\tvalid_1's rmse: 0.0172568\n",
      "[4900]\ttraining's rmse: 0.00835723\tvalid_1's rmse: 0.0172201\n",
      "[5000]\ttraining's rmse: 0.00823542\tvalid_1's rmse: 0.0171726\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00823542\tvalid_1's rmse: 0.0171726\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.141662\tvalid_1's rmse: 0.144969\n",
      "[200]\ttraining's rmse: 0.091248\tvalid_1's rmse: 0.0956718\n",
      "[300]\ttraining's rmse: 0.0631758\tvalid_1's rmse: 0.0680265\n",
      "[400]\ttraining's rmse: 0.0494484\tvalid_1's rmse: 0.0546436\n",
      "[500]\ttraining's rmse: 0.0404562\tvalid_1's rmse: 0.0458133\n",
      "[600]\ttraining's rmse: 0.0343718\tvalid_1's rmse: 0.0397567\n",
      "[700]\ttraining's rmse: 0.0296409\tvalid_1's rmse: 0.0349976\n",
      "[800]\ttraining's rmse: 0.0262565\tvalid_1's rmse: 0.0316474\n",
      "[900]\ttraining's rmse: 0.0235597\tvalid_1's rmse: 0.0289412\n",
      "[1000]\ttraining's rmse: 0.0214734\tvalid_1's rmse: 0.0268254\n",
      "[1100]\ttraining's rmse: 0.0198746\tvalid_1's rmse: 0.025196\n",
      "[1200]\ttraining's rmse: 0.0185809\tvalid_1's rmse: 0.0238792\n",
      "[1300]\ttraining's rmse: 0.0174678\tvalid_1's rmse: 0.0227341\n",
      "[1400]\ttraining's rmse: 0.0165305\tvalid_1's rmse: 0.0217808\n",
      "[1500]\ttraining's rmse: 0.0157394\tvalid_1's rmse: 0.0209647\n",
      "[1600]\ttraining's rmse: 0.0151103\tvalid_1's rmse: 0.0203245\n",
      "[1700]\ttraining's rmse: 0.0145401\tvalid_1's rmse: 0.0197942\n",
      "[1800]\ttraining's rmse: 0.0140471\tvalid_1's rmse: 0.0193291\n",
      "[1900]\ttraining's rmse: 0.0136064\tvalid_1's rmse: 0.018915\n",
      "[2000]\ttraining's rmse: 0.0132032\tvalid_1's rmse: 0.0185728\n",
      "[2100]\ttraining's rmse: 0.0128442\tvalid_1's rmse: 0.0182969\n",
      "[2200]\ttraining's rmse: 0.0125425\tvalid_1's rmse: 0.0180712\n",
      "[2300]\ttraining's rmse: 0.0122534\tvalid_1's rmse: 0.0178675\n",
      "[2400]\ttraining's rmse: 0.011983\tvalid_1's rmse: 0.017665\n",
      "[2500]\ttraining's rmse: 0.0117127\tvalid_1's rmse: 0.0174633\n",
      "[2600]\ttraining's rmse: 0.0114585\tvalid_1's rmse: 0.0172774\n",
      "[2700]\ttraining's rmse: 0.0112411\tvalid_1's rmse: 0.0171433\n",
      "[2800]\ttraining's rmse: 0.0110171\tvalid_1's rmse: 0.0169999\n",
      "[2900]\ttraining's rmse: 0.0108101\tvalid_1's rmse: 0.0168669\n",
      "[3000]\ttraining's rmse: 0.0106229\tvalid_1's rmse: 0.0167534\n",
      "[3100]\ttraining's rmse: 0.0104452\tvalid_1's rmse: 0.0166593\n",
      "[3200]\ttraining's rmse: 0.0102722\tvalid_1's rmse: 0.0165704\n",
      "[3300]\ttraining's rmse: 0.0100946\tvalid_1's rmse: 0.0164671\n",
      "[3400]\ttraining's rmse: 0.00994279\tvalid_1's rmse: 0.0164027\n",
      "[3500]\ttraining's rmse: 0.00978828\tvalid_1's rmse: 0.0163237\n",
      "[3600]\ttraining's rmse: 0.00964611\tvalid_1's rmse: 0.0162544\n",
      "[3700]\ttraining's rmse: 0.00951392\tvalid_1's rmse: 0.0162026\n",
      "[3800]\ttraining's rmse: 0.00936233\tvalid_1's rmse: 0.0161334\n",
      "[3900]\ttraining's rmse: 0.00923743\tvalid_1's rmse: 0.0160934\n",
      "[4000]\ttraining's rmse: 0.00911417\tvalid_1's rmse: 0.0160455\n",
      "[4100]\ttraining's rmse: 0.00898179\tvalid_1's rmse: 0.0159988\n",
      "[4200]\ttraining's rmse: 0.00885937\tvalid_1's rmse: 0.0159467\n",
      "[4300]\ttraining's rmse: 0.00873144\tvalid_1's rmse: 0.0158963\n",
      "[4400]\ttraining's rmse: 0.00861794\tvalid_1's rmse: 0.0158469\n",
      "[4500]\ttraining's rmse: 0.00850902\tvalid_1's rmse: 0.01582\n",
      "[4600]\ttraining's rmse: 0.0083939\tvalid_1's rmse: 0.0157738\n",
      "[4700]\ttraining's rmse: 0.00827762\tvalid_1's rmse: 0.0157195\n",
      "[4800]\ttraining's rmse: 0.00816168\tvalid_1's rmse: 0.0156739\n",
      "[4900]\ttraining's rmse: 0.00806752\tvalid_1's rmse: 0.0156434\n",
      "[5000]\ttraining's rmse: 0.00796286\tvalid_1's rmse: 0.0156027\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4998]\ttraining's rmse: 0.00796417\tvalid_1's rmse: 0.0156025\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.153772\tvalid_1's rmse: 0.155755\n",
      "[200]\ttraining's rmse: 0.100309\tvalid_1's rmse: 0.104057\n",
      "[300]\ttraining's rmse: 0.0695778\tvalid_1's rmse: 0.0741617\n",
      "[400]\ttraining's rmse: 0.0519976\tvalid_1's rmse: 0.0568944\n",
      "[500]\ttraining's rmse: 0.0418786\tvalid_1's rmse: 0.0468468\n",
      "[600]\ttraining's rmse: 0.0349264\tvalid_1's rmse: 0.0398813\n",
      "[700]\ttraining's rmse: 0.0297832\tvalid_1's rmse: 0.0346059\n",
      "[800]\ttraining's rmse: 0.0262846\tvalid_1's rmse: 0.0310659\n",
      "[900]\ttraining's rmse: 0.0237576\tvalid_1's rmse: 0.0285011\n",
      "[1000]\ttraining's rmse: 0.0218113\tvalid_1's rmse: 0.0265241\n",
      "[1100]\ttraining's rmse: 0.0201936\tvalid_1's rmse: 0.024838\n",
      "[1200]\ttraining's rmse: 0.0189274\tvalid_1's rmse: 0.0235287\n",
      "[1300]\ttraining's rmse: 0.0179168\tvalid_1's rmse: 0.0224753\n",
      "[1400]\ttraining's rmse: 0.0170837\tvalid_1's rmse: 0.0216289\n",
      "[1500]\ttraining's rmse: 0.0164421\tvalid_1's rmse: 0.0210149\n",
      "[1600]\ttraining's rmse: 0.015863\tvalid_1's rmse: 0.0204893\n",
      "[1700]\ttraining's rmse: 0.0153257\tvalid_1's rmse: 0.0200217\n",
      "[1800]\ttraining's rmse: 0.0148397\tvalid_1's rmse: 0.0196062\n",
      "[1900]\ttraining's rmse: 0.0144512\tvalid_1's rmse: 0.019315\n",
      "[2000]\ttraining's rmse: 0.0140679\tvalid_1's rmse: 0.0190104\n",
      "[2100]\ttraining's rmse: 0.0137503\tvalid_1's rmse: 0.0188134\n",
      "[2200]\ttraining's rmse: 0.0134621\tvalid_1's rmse: 0.0186285\n",
      "[2300]\ttraining's rmse: 0.0131682\tvalid_1's rmse: 0.0184563\n",
      "[2400]\ttraining's rmse: 0.0128869\tvalid_1's rmse: 0.0182741\n",
      "[2500]\ttraining's rmse: 0.0126125\tvalid_1's rmse: 0.0180911\n",
      "[2600]\ttraining's rmse: 0.0123533\tvalid_1's rmse: 0.0179424\n",
      "[2700]\ttraining's rmse: 0.0121197\tvalid_1's rmse: 0.0178204\n",
      "[2800]\ttraining's rmse: 0.0119275\tvalid_1's rmse: 0.0177478\n",
      "[2900]\ttraining's rmse: 0.0116994\tvalid_1's rmse: 0.0176231\n",
      "[3000]\ttraining's rmse: 0.0115165\tvalid_1's rmse: 0.0175449\n",
      "[3100]\ttraining's rmse: 0.0113305\tvalid_1's rmse: 0.0174502\n",
      "[3200]\ttraining's rmse: 0.0111579\tvalid_1's rmse: 0.0173742\n",
      "[3300]\ttraining's rmse: 0.010974\tvalid_1's rmse: 0.0172709\n",
      "[3400]\ttraining's rmse: 0.0108358\tvalid_1's rmse: 0.0172417\n",
      "[3500]\ttraining's rmse: 0.0106956\tvalid_1's rmse: 0.0172019\n",
      "[3600]\ttraining's rmse: 0.0105673\tvalid_1's rmse: 0.0171617\n",
      "[3700]\ttraining's rmse: 0.0104134\tvalid_1's rmse: 0.0170959\n",
      "[3800]\ttraining's rmse: 0.0102629\tvalid_1's rmse: 0.0170306\n",
      "[3900]\ttraining's rmse: 0.0101263\tvalid_1's rmse: 0.0169777\n",
      "[4000]\ttraining's rmse: 0.00999659\tvalid_1's rmse: 0.0169297\n",
      "[4100]\ttraining's rmse: 0.00987714\tvalid_1's rmse: 0.0168933\n",
      "[4200]\ttraining's rmse: 0.00972773\tvalid_1's rmse: 0.0168096\n",
      "[4300]\ttraining's rmse: 0.00959212\tvalid_1's rmse: 0.016754\n",
      "[4400]\ttraining's rmse: 0.00944869\tvalid_1's rmse: 0.016688\n",
      "[4500]\ttraining's rmse: 0.00933357\tvalid_1's rmse: 0.0166513\n",
      "[4600]\ttraining's rmse: 0.00922533\tvalid_1's rmse: 0.016624\n",
      "[4700]\ttraining's rmse: 0.00909182\tvalid_1's rmse: 0.0165661\n",
      "[4800]\ttraining's rmse: 0.00898174\tvalid_1's rmse: 0.0165258\n",
      "[4900]\ttraining's rmse: 0.00887427\tvalid_1's rmse: 0.0164804\n",
      "[5000]\ttraining's rmse: 0.00877575\tvalid_1's rmse: 0.0164564\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00877575\tvalid_1's rmse: 0.0164564\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.1508\tvalid_1's rmse: 0.151744\n",
      "[200]\ttraining's rmse: 0.0985777\tvalid_1's rmse: 0.101677\n",
      "[300]\ttraining's rmse: 0.0705679\tvalid_1's rmse: 0.0744627\n",
      "[400]\ttraining's rmse: 0.0551548\tvalid_1's rmse: 0.0594593\n",
      "[500]\ttraining's rmse: 0.0461716\tvalid_1's rmse: 0.0506924\n",
      "[600]\ttraining's rmse: 0.0396418\tvalid_1's rmse: 0.044323\n",
      "[700]\ttraining's rmse: 0.0343613\tvalid_1's rmse: 0.0391564\n",
      "[800]\ttraining's rmse: 0.0305427\tvalid_1's rmse: 0.0355082\n",
      "[900]\ttraining's rmse: 0.0275973\tvalid_1's rmse: 0.0327124\n",
      "[1000]\ttraining's rmse: 0.0251705\tvalid_1's rmse: 0.0303664\n",
      "[1100]\ttraining's rmse: 0.0230251\tvalid_1's rmse: 0.0283087\n",
      "[1200]\ttraining's rmse: 0.0213039\tvalid_1's rmse: 0.0266467\n",
      "[1300]\ttraining's rmse: 0.0198841\tvalid_1's rmse: 0.0252488\n",
      "[1400]\ttraining's rmse: 0.0186539\tvalid_1's rmse: 0.0240481\n",
      "[1500]\ttraining's rmse: 0.0177249\tvalid_1's rmse: 0.0231815\n",
      "[1600]\ttraining's rmse: 0.0168761\tvalid_1's rmse: 0.0223505\n",
      "[1700]\ttraining's rmse: 0.0161389\tvalid_1's rmse: 0.0216605\n",
      "[1800]\ttraining's rmse: 0.0154556\tvalid_1's rmse: 0.0209995\n",
      "[1900]\ttraining's rmse: 0.0149246\tvalid_1's rmse: 0.0205295\n",
      "[2000]\ttraining's rmse: 0.0144159\tvalid_1's rmse: 0.02008\n",
      "[2100]\ttraining's rmse: 0.013981\tvalid_1's rmse: 0.0197158\n",
      "[2200]\ttraining's rmse: 0.0135924\tvalid_1's rmse: 0.0194065\n",
      "[2300]\ttraining's rmse: 0.0132173\tvalid_1's rmse: 0.0191064\n",
      "[2400]\ttraining's rmse: 0.0128587\tvalid_1's rmse: 0.0188047\n",
      "[2500]\ttraining's rmse: 0.0125556\tvalid_1's rmse: 0.0185853\n",
      "[2600]\ttraining's rmse: 0.0122594\tvalid_1's rmse: 0.0183581\n",
      "[2700]\ttraining's rmse: 0.0119644\tvalid_1's rmse: 0.0181353\n",
      "[2800]\ttraining's rmse: 0.0117284\tvalid_1's rmse: 0.0179873\n",
      "[2900]\ttraining's rmse: 0.0114854\tvalid_1's rmse: 0.0178215\n",
      "[3000]\ttraining's rmse: 0.0112794\tvalid_1's rmse: 0.0176999\n",
      "[3100]\ttraining's rmse: 0.0110736\tvalid_1's rmse: 0.0175781\n",
      "[3200]\ttraining's rmse: 0.0108752\tvalid_1's rmse: 0.0174722\n",
      "[3300]\ttraining's rmse: 0.0106845\tvalid_1's rmse: 0.0173615\n",
      "[3400]\ttraining's rmse: 0.0105038\tvalid_1's rmse: 0.0172655\n",
      "[3500]\ttraining's rmse: 0.0103158\tvalid_1's rmse: 0.0171638\n",
      "[3600]\ttraining's rmse: 0.0101577\tvalid_1's rmse: 0.0170899\n",
      "[3700]\ttraining's rmse: 0.00999284\tvalid_1's rmse: 0.0170078\n",
      "[3800]\ttraining's rmse: 0.00984603\tvalid_1's rmse: 0.0169443\n",
      "[3900]\ttraining's rmse: 0.00970957\tvalid_1's rmse: 0.0168887\n",
      "[4000]\ttraining's rmse: 0.00958053\tvalid_1's rmse: 0.016842\n",
      "[4100]\ttraining's rmse: 0.00944011\tvalid_1's rmse: 0.0167846\n",
      "[4200]\ttraining's rmse: 0.00932253\tvalid_1's rmse: 0.0167418\n",
      "[4300]\ttraining's rmse: 0.00920048\tvalid_1's rmse: 0.0166969\n",
      "[4400]\ttraining's rmse: 0.00907439\tvalid_1's rmse: 0.016637\n",
      "[4500]\ttraining's rmse: 0.00895939\tvalid_1's rmse: 0.0165969\n",
      "[4600]\ttraining's rmse: 0.00884102\tvalid_1's rmse: 0.0165466\n",
      "[4700]\ttraining's rmse: 0.00871115\tvalid_1's rmse: 0.0164897\n",
      "[4800]\ttraining's rmse: 0.00859009\tvalid_1's rmse: 0.0164317\n",
      "[4900]\ttraining's rmse: 0.00847851\tvalid_1's rmse: 0.0163838\n",
      "[5000]\ttraining's rmse: 0.00834046\tvalid_1's rmse: 0.0163152\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00834046\tvalid_1's rmse: 0.0163152\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.151283\tvalid_1's rmse: 0.15291\n",
      "[200]\ttraining's rmse: 0.0986328\tvalid_1's rmse: 0.102323\n",
      "[300]\ttraining's rmse: 0.0692839\tvalid_1's rmse: 0.0738287\n",
      "[400]\ttraining's rmse: 0.052794\tvalid_1's rmse: 0.0577047\n",
      "[500]\ttraining's rmse: 0.0433956\tvalid_1's rmse: 0.0485595\n",
      "[600]\ttraining's rmse: 0.0367489\tvalid_1's rmse: 0.0420774\n",
      "[700]\ttraining's rmse: 0.0316149\tvalid_1's rmse: 0.0369803\n",
      "[800]\ttraining's rmse: 0.0280313\tvalid_1's rmse: 0.0334519\n",
      "[900]\ttraining's rmse: 0.0253561\tvalid_1's rmse: 0.0308436\n",
      "[1000]\ttraining's rmse: 0.023187\tvalid_1's rmse: 0.0287015\n",
      "[1100]\ttraining's rmse: 0.0213517\tvalid_1's rmse: 0.0269017\n",
      "[1200]\ttraining's rmse: 0.0198803\tvalid_1's rmse: 0.0254473\n",
      "[1300]\ttraining's rmse: 0.0186844\tvalid_1's rmse: 0.0242622\n",
      "[1400]\ttraining's rmse: 0.0176489\tvalid_1's rmse: 0.0232257\n",
      "[1500]\ttraining's rmse: 0.0168706\tvalid_1's rmse: 0.0225118\n",
      "[1600]\ttraining's rmse: 0.0161759\tvalid_1's rmse: 0.0218772\n",
      "[1700]\ttraining's rmse: 0.0155407\tvalid_1's rmse: 0.0212945\n",
      "[1800]\ttraining's rmse: 0.0149581\tvalid_1's rmse: 0.0207725\n",
      "[1900]\ttraining's rmse: 0.0144775\tvalid_1's rmse: 0.0203781\n",
      "[2000]\ttraining's rmse: 0.0140231\tvalid_1's rmse: 0.0200061\n",
      "[2100]\ttraining's rmse: 0.013636\tvalid_1's rmse: 0.0197315\n",
      "[2200]\ttraining's rmse: 0.0133101\tvalid_1's rmse: 0.019504\n",
      "[2300]\ttraining's rmse: 0.0129759\tvalid_1's rmse: 0.0192675\n",
      "[2400]\ttraining's rmse: 0.0126796\tvalid_1's rmse: 0.0190506\n",
      "[2500]\ttraining's rmse: 0.0124192\tvalid_1's rmse: 0.0188868\n",
      "[2600]\ttraining's rmse: 0.0121711\tvalid_1's rmse: 0.0187473\n",
      "[2700]\ttraining's rmse: 0.0119106\tvalid_1's rmse: 0.0185895\n",
      "[2800]\ttraining's rmse: 0.0116784\tvalid_1's rmse: 0.0184758\n",
      "[2900]\ttraining's rmse: 0.0114682\tvalid_1's rmse: 0.018368\n",
      "[3000]\ttraining's rmse: 0.0112717\tvalid_1's rmse: 0.0182741\n",
      "[3100]\ttraining's rmse: 0.0110886\tvalid_1's rmse: 0.0181786\n",
      "[3200]\ttraining's rmse: 0.0109034\tvalid_1's rmse: 0.0180998\n",
      "[3300]\ttraining's rmse: 0.0106961\tvalid_1's rmse: 0.0179692\n",
      "[3400]\ttraining's rmse: 0.0105423\tvalid_1's rmse: 0.0179135\n",
      "[3500]\ttraining's rmse: 0.010391\tvalid_1's rmse: 0.0178505\n",
      "[3600]\ttraining's rmse: 0.010233\tvalid_1's rmse: 0.0177829\n",
      "[3700]\ttraining's rmse: 0.0100915\tvalid_1's rmse: 0.0177229\n",
      "[3800]\ttraining's rmse: 0.00996008\tvalid_1's rmse: 0.0176886\n",
      "[3900]\ttraining's rmse: 0.00980682\tvalid_1's rmse: 0.017621\n",
      "[4000]\ttraining's rmse: 0.00967719\tvalid_1's rmse: 0.0175628\n",
      "[4100]\ttraining's rmse: 0.00954258\tvalid_1's rmse: 0.0175135\n",
      "[4200]\ttraining's rmse: 0.00942125\tvalid_1's rmse: 0.0174634\n",
      "[4300]\ttraining's rmse: 0.00929813\tvalid_1's rmse: 0.0174251\n",
      "[4400]\ttraining's rmse: 0.00919278\tvalid_1's rmse: 0.0173998\n",
      "[4500]\ttraining's rmse: 0.00907361\tvalid_1's rmse: 0.0173616\n",
      "[4600]\ttraining's rmse: 0.00893944\tvalid_1's rmse: 0.0172989\n",
      "[4700]\ttraining's rmse: 0.00881713\tvalid_1's rmse: 0.0172495\n",
      "[4800]\ttraining's rmse: 0.00870347\tvalid_1's rmse: 0.0172022\n",
      "[4900]\ttraining's rmse: 0.00858446\tvalid_1's rmse: 0.0171721\n",
      "[5000]\ttraining's rmse: 0.00848919\tvalid_1's rmse: 0.0171484\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\ttraining's rmse: 0.00848989\tvalid_1's rmse: 0.0171482\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.152569\tvalid_1's rmse: 0.154214\n",
      "[200]\ttraining's rmse: 0.0985832\tvalid_1's rmse: 0.101585\n",
      "[300]\ttraining's rmse: 0.0668629\tvalid_1's rmse: 0.0705858\n",
      "[400]\ttraining's rmse: 0.0513818\tvalid_1's rmse: 0.055509\n",
      "[500]\ttraining's rmse: 0.0413383\tvalid_1's rmse: 0.0456441\n",
      "[600]\ttraining's rmse: 0.0347624\tvalid_1's rmse: 0.0392708\n",
      "[700]\ttraining's rmse: 0.0297958\tvalid_1's rmse: 0.0343687\n",
      "[800]\ttraining's rmse: 0.0263474\tvalid_1's rmse: 0.0309569\n",
      "[900]\ttraining's rmse: 0.0236656\tvalid_1's rmse: 0.0283284\n",
      "[1000]\ttraining's rmse: 0.0216752\tvalid_1's rmse: 0.0264223\n",
      "[1100]\ttraining's rmse: 0.020199\tvalid_1's rmse: 0.0250019\n",
      "[1200]\ttraining's rmse: 0.0190076\tvalid_1's rmse: 0.0238605\n",
      "[1300]\ttraining's rmse: 0.018\tvalid_1's rmse: 0.0228964\n",
      "[1400]\ttraining's rmse: 0.0171769\tvalid_1's rmse: 0.0220988\n",
      "[1500]\ttraining's rmse: 0.0164963\tvalid_1's rmse: 0.0214433\n",
      "[1600]\ttraining's rmse: 0.0159149\tvalid_1's rmse: 0.0209574\n",
      "[1700]\ttraining's rmse: 0.0153908\tvalid_1's rmse: 0.0205187\n",
      "[1800]\ttraining's rmse: 0.0149493\tvalid_1's rmse: 0.0201985\n",
      "[1900]\ttraining's rmse: 0.0145616\tvalid_1's rmse: 0.0199152\n",
      "[2000]\ttraining's rmse: 0.0141965\tvalid_1's rmse: 0.0196508\n",
      "[2100]\ttraining's rmse: 0.0138499\tvalid_1's rmse: 0.0194209\n",
      "[2200]\ttraining's rmse: 0.0135683\tvalid_1's rmse: 0.0192656\n",
      "[2300]\ttraining's rmse: 0.0132821\tvalid_1's rmse: 0.0190901\n",
      "[2400]\ttraining's rmse: 0.013011\tvalid_1's rmse: 0.018932\n",
      "[2500]\ttraining's rmse: 0.0127394\tvalid_1's rmse: 0.0187616\n",
      "[2600]\ttraining's rmse: 0.012495\tvalid_1's rmse: 0.0186246\n",
      "[2700]\ttraining's rmse: 0.0122742\tvalid_1's rmse: 0.0185148\n",
      "[2800]\ttraining's rmse: 0.0120382\tvalid_1's rmse: 0.0183781\n",
      "[2900]\ttraining's rmse: 0.0118215\tvalid_1's rmse: 0.0182673\n",
      "[3000]\ttraining's rmse: 0.0116173\tvalid_1's rmse: 0.0181659\n",
      "[3100]\ttraining's rmse: 0.01144\tvalid_1's rmse: 0.0180939\n",
      "[3200]\ttraining's rmse: 0.0112312\tvalid_1's rmse: 0.0179805\n",
      "[3300]\ttraining's rmse: 0.0110657\tvalid_1's rmse: 0.0179212\n",
      "[3400]\ttraining's rmse: 0.0108939\tvalid_1's rmse: 0.0178424\n",
      "[3500]\ttraining's rmse: 0.0107248\tvalid_1's rmse: 0.017777\n",
      "[3600]\ttraining's rmse: 0.0105651\tvalid_1's rmse: 0.0177119\n",
      "[3700]\ttraining's rmse: 0.0104265\tvalid_1's rmse: 0.0176664\n",
      "[3800]\ttraining's rmse: 0.0102679\tvalid_1's rmse: 0.0175962\n",
      "[3900]\ttraining's rmse: 0.0101246\tvalid_1's rmse: 0.0175247\n",
      "[4000]\ttraining's rmse: 0.00998158\tvalid_1's rmse: 0.0174712\n",
      "[4100]\ttraining's rmse: 0.00983412\tvalid_1's rmse: 0.0174126\n",
      "[4200]\ttraining's rmse: 0.00969447\tvalid_1's rmse: 0.0173482\n",
      "[4300]\ttraining's rmse: 0.00957067\tvalid_1's rmse: 0.0173087\n",
      "[4400]\ttraining's rmse: 0.00943649\tvalid_1's rmse: 0.017252\n",
      "[4500]\ttraining's rmse: 0.0093147\tvalid_1's rmse: 0.0171967\n",
      "[4600]\ttraining's rmse: 0.00917669\tvalid_1's rmse: 0.0171224\n",
      "[4700]\ttraining's rmse: 0.00906612\tvalid_1's rmse: 0.0170866\n",
      "[4800]\ttraining's rmse: 0.00896535\tvalid_1's rmse: 0.0170558\n",
      "[4900]\ttraining's rmse: 0.00884381\tvalid_1's rmse: 0.017008\n",
      "[5000]\ttraining's rmse: 0.00873696\tvalid_1's rmse: 0.0169703\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4983]\ttraining's rmse: 0.00875161\tvalid_1's rmse: 0.0169698\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.146799\tvalid_1's rmse: 0.148733\n",
      "[200]\ttraining's rmse: 0.0949611\tvalid_1's rmse: 0.0982377\n",
      "[300]\ttraining's rmse: 0.0662087\tvalid_1's rmse: 0.0703211\n",
      "[400]\ttraining's rmse: 0.0524774\tvalid_1's rmse: 0.0568661\n",
      "[500]\ttraining's rmse: 0.0436067\tvalid_1's rmse: 0.0482687\n",
      "[600]\ttraining's rmse: 0.0374078\tvalid_1's rmse: 0.0422677\n",
      "[700]\ttraining's rmse: 0.0325078\tvalid_1's rmse: 0.0375519\n",
      "[800]\ttraining's rmse: 0.0288517\tvalid_1's rmse: 0.0340013\n",
      "[900]\ttraining's rmse: 0.0259064\tvalid_1's rmse: 0.0311486\n",
      "[1000]\ttraining's rmse: 0.0235897\tvalid_1's rmse: 0.0288701\n",
      "[1100]\ttraining's rmse: 0.0217882\tvalid_1's rmse: 0.0271187\n",
      "[1200]\ttraining's rmse: 0.0202944\tvalid_1's rmse: 0.0256946\n",
      "[1300]\ttraining's rmse: 0.0190146\tvalid_1's rmse: 0.0245065\n",
      "[1400]\ttraining's rmse: 0.0179185\tvalid_1's rmse: 0.0234294\n",
      "[1500]\ttraining's rmse: 0.0170203\tvalid_1's rmse: 0.0225847\n",
      "[1600]\ttraining's rmse: 0.0162714\tvalid_1's rmse: 0.021906\n",
      "[1700]\ttraining's rmse: 0.0156353\tvalid_1's rmse: 0.0213377\n",
      "[1800]\ttraining's rmse: 0.0150445\tvalid_1's rmse: 0.0208444\n",
      "[1900]\ttraining's rmse: 0.014523\tvalid_1's rmse: 0.0204125\n",
      "[2000]\ttraining's rmse: 0.0140548\tvalid_1's rmse: 0.0200373\n",
      "[2100]\ttraining's rmse: 0.0136442\tvalid_1's rmse: 0.0197291\n",
      "[2200]\ttraining's rmse: 0.0132891\tvalid_1's rmse: 0.0195033\n",
      "[2300]\ttraining's rmse: 0.012952\tvalid_1's rmse: 0.0192714\n",
      "[2400]\ttraining's rmse: 0.0126447\tvalid_1's rmse: 0.0190874\n",
      "[2500]\ttraining's rmse: 0.0123565\tvalid_1's rmse: 0.0189168\n",
      "[2600]\ttraining's rmse: 0.0120427\tvalid_1's rmse: 0.0187133\n",
      "[2700]\ttraining's rmse: 0.0117943\tvalid_1's rmse: 0.0185719\n",
      "[2800]\ttraining's rmse: 0.011546\tvalid_1's rmse: 0.018438\n",
      "[2900]\ttraining's rmse: 0.0113071\tvalid_1's rmse: 0.0182816\n",
      "[3000]\ttraining's rmse: 0.0110926\tvalid_1's rmse: 0.018165\n",
      "[3100]\ttraining's rmse: 0.0109064\tvalid_1's rmse: 0.0180882\n",
      "[3200]\ttraining's rmse: 0.0107098\tvalid_1's rmse: 0.0179942\n",
      "[3300]\ttraining's rmse: 0.0105128\tvalid_1's rmse: 0.0178827\n",
      "[3400]\ttraining's rmse: 0.0103315\tvalid_1's rmse: 0.0178047\n",
      "[3500]\ttraining's rmse: 0.0101629\tvalid_1's rmse: 0.0177307\n",
      "[3600]\ttraining's rmse: 0.00998449\tvalid_1's rmse: 0.0176343\n",
      "[3700]\ttraining's rmse: 0.00984135\tvalid_1's rmse: 0.0175857\n",
      "[3800]\ttraining's rmse: 0.00969347\tvalid_1's rmse: 0.0175324\n",
      "[3900]\ttraining's rmse: 0.00955751\tvalid_1's rmse: 0.0174871\n",
      "[4000]\ttraining's rmse: 0.00942151\tvalid_1's rmse: 0.0174401\n",
      "[4100]\ttraining's rmse: 0.00927976\tvalid_1's rmse: 0.0173782\n",
      "[4200]\ttraining's rmse: 0.00914185\tvalid_1's rmse: 0.0173253\n",
      "[4300]\ttraining's rmse: 0.00902259\tvalid_1's rmse: 0.017286\n",
      "[4400]\ttraining's rmse: 0.00890516\tvalid_1's rmse: 0.0172476\n",
      "[4500]\ttraining's rmse: 0.00877546\tvalid_1's rmse: 0.0171916\n",
      "[4600]\ttraining's rmse: 0.00866484\tvalid_1's rmse: 0.0171574\n",
      "[4700]\ttraining's rmse: 0.00855358\tvalid_1's rmse: 0.0171189\n",
      "[4800]\ttraining's rmse: 0.00845212\tvalid_1's rmse: 0.0170856\n",
      "[4900]\ttraining's rmse: 0.00835517\tvalid_1's rmse: 0.0170687\n",
      "[5000]\ttraining's rmse: 0.00825143\tvalid_1's rmse: 0.0170382\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00825143\tvalid_1's rmse: 0.0170382\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.148662\tvalid_1's rmse: 0.150225\n",
      "[200]\ttraining's rmse: 0.0971519\tvalid_1's rmse: 0.100209\n",
      "[300]\ttraining's rmse: 0.0678814\tvalid_1's rmse: 0.0716147\n",
      "[400]\ttraining's rmse: 0.0512218\tvalid_1's rmse: 0.0552704\n",
      "[500]\ttraining's rmse: 0.0415521\tvalid_1's rmse: 0.0458074\n",
      "[600]\ttraining's rmse: 0.0348438\tvalid_1's rmse: 0.0391877\n",
      "[700]\ttraining's rmse: 0.0297639\tvalid_1's rmse: 0.0341178\n",
      "[800]\ttraining's rmse: 0.0262551\tvalid_1's rmse: 0.0306093\n",
      "[900]\ttraining's rmse: 0.0236763\tvalid_1's rmse: 0.0281058\n",
      "[1000]\ttraining's rmse: 0.0216767\tvalid_1's rmse: 0.0261615\n",
      "[1100]\ttraining's rmse: 0.01997\tvalid_1's rmse: 0.0244622\n",
      "[1200]\ttraining's rmse: 0.0186462\tvalid_1's rmse: 0.0231353\n",
      "[1300]\ttraining's rmse: 0.0175712\tvalid_1's rmse: 0.0220614\n",
      "[1400]\ttraining's rmse: 0.0166783\tvalid_1's rmse: 0.0211517\n",
      "[1500]\ttraining's rmse: 0.0159935\tvalid_1's rmse: 0.0205021\n",
      "[1600]\ttraining's rmse: 0.015386\tvalid_1's rmse: 0.0199362\n",
      "[1700]\ttraining's rmse: 0.0148558\tvalid_1's rmse: 0.0194875\n",
      "[1800]\ttraining's rmse: 0.0143258\tvalid_1's rmse: 0.019021\n",
      "[1900]\ttraining's rmse: 0.0139112\tvalid_1's rmse: 0.0186906\n",
      "[2000]\ttraining's rmse: 0.0135452\tvalid_1's rmse: 0.0184236\n",
      "[2100]\ttraining's rmse: 0.0131902\tvalid_1's rmse: 0.0181666\n",
      "[2200]\ttraining's rmse: 0.0128888\tvalid_1's rmse: 0.0179688\n",
      "[2300]\ttraining's rmse: 0.0125912\tvalid_1's rmse: 0.0177745\n",
      "[2400]\ttraining's rmse: 0.0123249\tvalid_1's rmse: 0.017607\n",
      "[2500]\ttraining's rmse: 0.0120661\tvalid_1's rmse: 0.0174563\n",
      "[2600]\ttraining's rmse: 0.0118309\tvalid_1's rmse: 0.0173303\n",
      "[2700]\ttraining's rmse: 0.0115882\tvalid_1's rmse: 0.0171823\n",
      "[2800]\ttraining's rmse: 0.0113696\tvalid_1's rmse: 0.0170639\n",
      "[2900]\ttraining's rmse: 0.0111873\tvalid_1's rmse: 0.0169693\n",
      "[3000]\ttraining's rmse: 0.0109914\tvalid_1's rmse: 0.0168697\n",
      "[3100]\ttraining's rmse: 0.0107928\tvalid_1's rmse: 0.0167737\n",
      "[3200]\ttraining's rmse: 0.010636\tvalid_1's rmse: 0.0167152\n",
      "[3300]\ttraining's rmse: 0.0104614\tvalid_1's rmse: 0.0166301\n",
      "[3400]\ttraining's rmse: 0.0102934\tvalid_1's rmse: 0.0165478\n",
      "[3500]\ttraining's rmse: 0.0101331\tvalid_1's rmse: 0.0164785\n",
      "[3600]\ttraining's rmse: 0.00998117\tvalid_1's rmse: 0.0164123\n",
      "[3700]\ttraining's rmse: 0.00983379\tvalid_1's rmse: 0.0163541\n",
      "[3800]\ttraining's rmse: 0.00969312\tvalid_1's rmse: 0.0162967\n",
      "[3900]\ttraining's rmse: 0.00954259\tvalid_1's rmse: 0.0162318\n",
      "[4000]\ttraining's rmse: 0.00941701\tvalid_1's rmse: 0.016189\n",
      "[4100]\ttraining's rmse: 0.00930459\tvalid_1's rmse: 0.016154\n",
      "[4200]\ttraining's rmse: 0.0091816\tvalid_1's rmse: 0.0161017\n",
      "[4300]\ttraining's rmse: 0.00906236\tvalid_1's rmse: 0.0160664\n",
      "[4400]\ttraining's rmse: 0.00894735\tvalid_1's rmse: 0.0160269\n",
      "[4500]\ttraining's rmse: 0.00882913\tvalid_1's rmse: 0.0159898\n",
      "[4600]\ttraining's rmse: 0.00870826\tvalid_1's rmse: 0.0159377\n",
      "[4700]\ttraining's rmse: 0.00859178\tvalid_1's rmse: 0.0158906\n",
      "[4800]\ttraining's rmse: 0.00847663\tvalid_1's rmse: 0.0158535\n",
      "[4900]\ttraining's rmse: 0.00837586\tvalid_1's rmse: 0.0158246\n",
      "[5000]\ttraining's rmse: 0.00827398\tvalid_1's rmse: 0.0157842\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00827398\tvalid_1's rmse: 0.0157842\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.140529\tvalid_1's rmse: 0.142349\n",
      "[200]\ttraining's rmse: 0.0862313\tvalid_1's rmse: 0.0896486\n",
      "[300]\ttraining's rmse: 0.0599156\tvalid_1's rmse: 0.0641028\n",
      "[400]\ttraining's rmse: 0.0461601\tvalid_1's rmse: 0.0505678\n",
      "[500]\ttraining's rmse: 0.0374838\tvalid_1's rmse: 0.0418532\n",
      "[600]\ttraining's rmse: 0.0319466\tvalid_1's rmse: 0.0362718\n",
      "[700]\ttraining's rmse: 0.028228\tvalid_1's rmse: 0.0325945\n",
      "[800]\ttraining's rmse: 0.025462\tvalid_1's rmse: 0.0299075\n",
      "[900]\ttraining's rmse: 0.0232863\tvalid_1's rmse: 0.0277534\n",
      "[1000]\ttraining's rmse: 0.0214866\tvalid_1's rmse: 0.0259904\n",
      "[1100]\ttraining's rmse: 0.0199771\tvalid_1's rmse: 0.0244741\n",
      "[1200]\ttraining's rmse: 0.0188264\tvalid_1's rmse: 0.0233682\n",
      "[1300]\ttraining's rmse: 0.0178442\tvalid_1's rmse: 0.0224035\n",
      "[1400]\ttraining's rmse: 0.0170253\tvalid_1's rmse: 0.0216192\n",
      "[1500]\ttraining's rmse: 0.0163641\tvalid_1's rmse: 0.0209963\n",
      "[1600]\ttraining's rmse: 0.0157718\tvalid_1's rmse: 0.0204839\n",
      "[1700]\ttraining's rmse: 0.0152532\tvalid_1's rmse: 0.0200111\n",
      "[1800]\ttraining's rmse: 0.0147789\tvalid_1's rmse: 0.0196139\n",
      "[1900]\ttraining's rmse: 0.014367\tvalid_1's rmse: 0.0192891\n",
      "[2000]\ttraining's rmse: 0.0139764\tvalid_1's rmse: 0.0189884\n",
      "[2100]\ttraining's rmse: 0.0136089\tvalid_1's rmse: 0.0187014\n",
      "[2200]\ttraining's rmse: 0.0133197\tvalid_1's rmse: 0.0185227\n",
      "[2300]\ttraining's rmse: 0.0130164\tvalid_1's rmse: 0.018314\n",
      "[2400]\ttraining's rmse: 0.0127269\tvalid_1's rmse: 0.01812\n",
      "[2500]\ttraining's rmse: 0.012458\tvalid_1's rmse: 0.0179378\n",
      "[2600]\ttraining's rmse: 0.0121942\tvalid_1's rmse: 0.0177824\n",
      "[2700]\ttraining's rmse: 0.0119319\tvalid_1's rmse: 0.0176147\n",
      "[2800]\ttraining's rmse: 0.0116899\tvalid_1's rmse: 0.017452\n",
      "[2900]\ttraining's rmse: 0.011497\tvalid_1's rmse: 0.017343\n",
      "[3000]\ttraining's rmse: 0.0113025\tvalid_1's rmse: 0.0172347\n",
      "[3100]\ttraining's rmse: 0.0110972\tvalid_1's rmse: 0.0171337\n",
      "[3200]\ttraining's rmse: 0.0109085\tvalid_1's rmse: 0.0170272\n",
      "[3300]\ttraining's rmse: 0.0107298\tvalid_1's rmse: 0.0169405\n",
      "[3400]\ttraining's rmse: 0.0105587\tvalid_1's rmse: 0.0168677\n",
      "[3500]\ttraining's rmse: 0.0103976\tvalid_1's rmse: 0.0168\n",
      "[3600]\ttraining's rmse: 0.0102064\tvalid_1's rmse: 0.0166859\n",
      "[3700]\ttraining's rmse: 0.0100519\tvalid_1's rmse: 0.0166024\n",
      "[3800]\ttraining's rmse: 0.00990615\tvalid_1's rmse: 0.0165515\n",
      "[3900]\ttraining's rmse: 0.00975469\tvalid_1's rmse: 0.0164827\n",
      "[4000]\ttraining's rmse: 0.00960464\tvalid_1's rmse: 0.0164237\n",
      "[4100]\ttraining's rmse: 0.00947811\tvalid_1's rmse: 0.0163799\n",
      "[4200]\ttraining's rmse: 0.00935958\tvalid_1's rmse: 0.0163362\n",
      "[4300]\ttraining's rmse: 0.00923774\tvalid_1's rmse: 0.0162942\n",
      "[4400]\ttraining's rmse: 0.0091147\tvalid_1's rmse: 0.0162487\n",
      "[4500]\ttraining's rmse: 0.00899354\tvalid_1's rmse: 0.0162081\n",
      "[4600]\ttraining's rmse: 0.00888487\tvalid_1's rmse: 0.0161592\n",
      "[4700]\ttraining's rmse: 0.00876505\tvalid_1's rmse: 0.0161118\n",
      "[4800]\ttraining's rmse: 0.00862823\tvalid_1's rmse: 0.0160416\n",
      "[4900]\ttraining's rmse: 0.00852247\tvalid_1's rmse: 0.0160183\n",
      "[5000]\ttraining's rmse: 0.00841718\tvalid_1's rmse: 0.0159733\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\ttraining's rmse: 0.00841776\tvalid_1's rmse: 0.0159733\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.148301\tvalid_1's rmse: 0.152705\n",
      "[200]\ttraining's rmse: 0.0965869\tvalid_1's rmse: 0.102433\n",
      "[300]\ttraining's rmse: 0.066735\tvalid_1's rmse: 0.07276\n",
      "[400]\ttraining's rmse: 0.0519995\tvalid_1's rmse: 0.0579874\n",
      "[500]\ttraining's rmse: 0.0422397\tvalid_1's rmse: 0.0481094\n",
      "[600]\ttraining's rmse: 0.0356699\tvalid_1's rmse: 0.0414425\n",
      "[700]\ttraining's rmse: 0.0306485\tvalid_1's rmse: 0.0362793\n",
      "[800]\ttraining's rmse: 0.0271057\tvalid_1's rmse: 0.0326892\n",
      "[900]\ttraining's rmse: 0.0243524\tvalid_1's rmse: 0.0299336\n",
      "[1000]\ttraining's rmse: 0.0222005\tvalid_1's rmse: 0.0277063\n",
      "[1100]\ttraining's rmse: 0.0205494\tvalid_1's rmse: 0.0260269\n",
      "[1200]\ttraining's rmse: 0.0192417\tvalid_1's rmse: 0.024699\n",
      "[1300]\ttraining's rmse: 0.0181403\tvalid_1's rmse: 0.0235559\n",
      "[1400]\ttraining's rmse: 0.0171846\tvalid_1's rmse: 0.0225843\n",
      "[1500]\ttraining's rmse: 0.0163975\tvalid_1's rmse: 0.0217817\n",
      "[1600]\ttraining's rmse: 0.015776\tvalid_1's rmse: 0.0211938\n",
      "[1700]\ttraining's rmse: 0.0152346\tvalid_1's rmse: 0.020706\n",
      "[1800]\ttraining's rmse: 0.0147242\tvalid_1's rmse: 0.0202398\n",
      "[1900]\ttraining's rmse: 0.0143053\tvalid_1's rmse: 0.0198689\n",
      "[2000]\ttraining's rmse: 0.0138902\tvalid_1's rmse: 0.019534\n",
      "[2100]\ttraining's rmse: 0.0135357\tvalid_1's rmse: 0.019257\n",
      "[2200]\ttraining's rmse: 0.0132189\tvalid_1's rmse: 0.0190492\n",
      "[2300]\ttraining's rmse: 0.0129369\tvalid_1's rmse: 0.0188656\n",
      "[2400]\ttraining's rmse: 0.0126613\tvalid_1's rmse: 0.0186818\n",
      "[2500]\ttraining's rmse: 0.0123862\tvalid_1's rmse: 0.0185107\n",
      "[2600]\ttraining's rmse: 0.0121335\tvalid_1's rmse: 0.0183486\n",
      "[2700]\ttraining's rmse: 0.011909\tvalid_1's rmse: 0.0182083\n",
      "[2800]\ttraining's rmse: 0.011703\tvalid_1's rmse: 0.0180976\n",
      "[2900]\ttraining's rmse: 0.0115076\tvalid_1's rmse: 0.0179882\n",
      "[3000]\ttraining's rmse: 0.0113103\tvalid_1's rmse: 0.0178759\n",
      "[3100]\ttraining's rmse: 0.011135\tvalid_1's rmse: 0.0177967\n",
      "[3200]\ttraining's rmse: 0.0109489\tvalid_1's rmse: 0.0176946\n",
      "[3300]\ttraining's rmse: 0.0107744\tvalid_1's rmse: 0.0176012\n",
      "[3400]\ttraining's rmse: 0.0106068\tvalid_1's rmse: 0.0175161\n",
      "[3500]\ttraining's rmse: 0.0104292\tvalid_1's rmse: 0.0174192\n",
      "[3600]\ttraining's rmse: 0.0102741\tvalid_1's rmse: 0.017348\n",
      "[3700]\ttraining's rmse: 0.0101397\tvalid_1's rmse: 0.017294\n",
      "[3800]\ttraining's rmse: 0.00997224\tvalid_1's rmse: 0.0172039\n",
      "[3900]\ttraining's rmse: 0.00984194\tvalid_1's rmse: 0.0171482\n",
      "[4000]\ttraining's rmse: 0.00969558\tvalid_1's rmse: 0.0170815\n",
      "[4100]\ttraining's rmse: 0.00954017\tvalid_1's rmse: 0.0170083\n",
      "[4200]\ttraining's rmse: 0.00939165\tvalid_1's rmse: 0.016934\n",
      "[4300]\ttraining's rmse: 0.00927292\tvalid_1's rmse: 0.0168911\n",
      "[4400]\ttraining's rmse: 0.00915286\tvalid_1's rmse: 0.0168359\n",
      "[4500]\ttraining's rmse: 0.00903856\tvalid_1's rmse: 0.0167923\n",
      "[4600]\ttraining's rmse: 0.00890172\tvalid_1's rmse: 0.0167402\n",
      "[4700]\ttraining's rmse: 0.00877804\tvalid_1's rmse: 0.0166813\n",
      "[4800]\ttraining's rmse: 0.00868291\tvalid_1's rmse: 0.0166546\n",
      "[4900]\ttraining's rmse: 0.00857438\tvalid_1's rmse: 0.0166176\n",
      "[5000]\ttraining's rmse: 0.00846264\tvalid_1's rmse: 0.0165811\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00846264\tvalid_1's rmse: 0.0165811\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.148574\tvalid_1's rmse: 0.149076\n",
      "[200]\ttraining's rmse: 0.0962811\tvalid_1's rmse: 0.0977254\n",
      "[300]\ttraining's rmse: 0.065798\tvalid_1's rmse: 0.0679798\n",
      "[400]\ttraining's rmse: 0.0507897\tvalid_1's rmse: 0.0535514\n",
      "[500]\ttraining's rmse: 0.0409411\tvalid_1's rmse: 0.0442066\n",
      "[600]\ttraining's rmse: 0.0345042\tvalid_1's rmse: 0.0381827\n",
      "[700]\ttraining's rmse: 0.0296378\tvalid_1's rmse: 0.033586\n",
      "[800]\ttraining's rmse: 0.0262453\tvalid_1's rmse: 0.0303877\n",
      "[900]\ttraining's rmse: 0.0236176\tvalid_1's rmse: 0.0279536\n",
      "[1000]\ttraining's rmse: 0.0215965\tvalid_1's rmse: 0.0260675\n",
      "[1100]\ttraining's rmse: 0.0200913\tvalid_1's rmse: 0.0246922\n",
      "[1200]\ttraining's rmse: 0.018908\tvalid_1's rmse: 0.0236093\n",
      "[1300]\ttraining's rmse: 0.0178617\tvalid_1's rmse: 0.0226288\n",
      "[1400]\ttraining's rmse: 0.0169979\tvalid_1's rmse: 0.0218334\n",
      "[1500]\ttraining's rmse: 0.0162665\tvalid_1's rmse: 0.0211753\n",
      "[1600]\ttraining's rmse: 0.0156875\tvalid_1's rmse: 0.0207121\n",
      "[1700]\ttraining's rmse: 0.0151769\tvalid_1's rmse: 0.0203104\n",
      "[1800]\ttraining's rmse: 0.0147004\tvalid_1's rmse: 0.0199354\n",
      "[1900]\ttraining's rmse: 0.0142626\tvalid_1's rmse: 0.0195974\n",
      "[2000]\ttraining's rmse: 0.013888\tvalid_1's rmse: 0.0193571\n",
      "[2100]\ttraining's rmse: 0.0135449\tvalid_1's rmse: 0.0191358\n",
      "[2200]\ttraining's rmse: 0.0132443\tvalid_1's rmse: 0.0189533\n",
      "[2300]\ttraining's rmse: 0.0129708\tvalid_1's rmse: 0.0187985\n",
      "[2400]\ttraining's rmse: 0.0127133\tvalid_1's rmse: 0.0186474\n",
      "[2500]\ttraining's rmse: 0.0124537\tvalid_1's rmse: 0.0185045\n",
      "[2600]\ttraining's rmse: 0.0121941\tvalid_1's rmse: 0.0183589\n",
      "[2700]\ttraining's rmse: 0.0119566\tvalid_1's rmse: 0.0182101\n",
      "[2800]\ttraining's rmse: 0.0117409\tvalid_1's rmse: 0.0181\n",
      "[2900]\ttraining's rmse: 0.0115445\tvalid_1's rmse: 0.0179916\n",
      "[3000]\ttraining's rmse: 0.0113459\tvalid_1's rmse: 0.0178815\n",
      "[3100]\ttraining's rmse: 0.0111634\tvalid_1's rmse: 0.0178075\n",
      "[3200]\ttraining's rmse: 0.0109894\tvalid_1's rmse: 0.0177414\n",
      "[3300]\ttraining's rmse: 0.0108029\tvalid_1's rmse: 0.0176586\n",
      "[3400]\ttraining's rmse: 0.0106237\tvalid_1's rmse: 0.01757\n",
      "[3500]\ttraining's rmse: 0.0104673\tvalid_1's rmse: 0.0175014\n",
      "[3600]\ttraining's rmse: 0.010332\tvalid_1's rmse: 0.0174513\n",
      "[3700]\ttraining's rmse: 0.0101908\tvalid_1's rmse: 0.0174086\n",
      "[3800]\ttraining's rmse: 0.0100275\tvalid_1's rmse: 0.0173232\n",
      "[3900]\ttraining's rmse: 0.00989116\tvalid_1's rmse: 0.0172735\n",
      "[4000]\ttraining's rmse: 0.00976016\tvalid_1's rmse: 0.0172292\n",
      "[4100]\ttraining's rmse: 0.0096359\tvalid_1's rmse: 0.0171845\n",
      "[4200]\ttraining's rmse: 0.00951094\tvalid_1's rmse: 0.0171413\n",
      "[4300]\ttraining's rmse: 0.00936975\tvalid_1's rmse: 0.0170841\n",
      "[4400]\ttraining's rmse: 0.00926076\tvalid_1's rmse: 0.0170532\n",
      "[4500]\ttraining's rmse: 0.00913981\tvalid_1's rmse: 0.0170065\n",
      "[4600]\ttraining's rmse: 0.0090207\tvalid_1's rmse: 0.0169558\n",
      "[4700]\ttraining's rmse: 0.00890888\tvalid_1's rmse: 0.016916\n",
      "[4800]\ttraining's rmse: 0.00880572\tvalid_1's rmse: 0.0168796\n",
      "[4900]\ttraining's rmse: 0.00871106\tvalid_1's rmse: 0.0168595\n",
      "[5000]\ttraining's rmse: 0.0086006\tvalid_1's rmse: 0.0168219\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.0086006\tvalid_1's rmse: 0.0168219\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.145044\tvalid_1's rmse: 0.147438\n",
      "[200]\ttraining's rmse: 0.0932378\tvalid_1's rmse: 0.0970066\n",
      "[300]\ttraining's rmse: 0.0641533\tvalid_1's rmse: 0.06851\n",
      "[400]\ttraining's rmse: 0.0501311\tvalid_1's rmse: 0.0546849\n",
      "[500]\ttraining's rmse: 0.0409507\tvalid_1's rmse: 0.0456221\n",
      "[600]\ttraining's rmse: 0.0347985\tvalid_1's rmse: 0.0395561\n",
      "[700]\ttraining's rmse: 0.0300732\tvalid_1's rmse: 0.0348456\n",
      "[800]\ttraining's rmse: 0.0266725\tvalid_1's rmse: 0.0315235\n",
      "[900]\ttraining's rmse: 0.0239993\tvalid_1's rmse: 0.028933\n",
      "[1000]\ttraining's rmse: 0.0219397\tvalid_1's rmse: 0.0269425\n",
      "[1100]\ttraining's rmse: 0.0203596\tvalid_1's rmse: 0.0254173\n",
      "[1200]\ttraining's rmse: 0.0190682\tvalid_1's rmse: 0.0241765\n",
      "[1300]\ttraining's rmse: 0.0179522\tvalid_1's rmse: 0.0230504\n",
      "[1400]\ttraining's rmse: 0.0170356\tvalid_1's rmse: 0.0221546\n",
      "[1500]\ttraining's rmse: 0.0162701\tvalid_1's rmse: 0.0213829\n",
      "[1600]\ttraining's rmse: 0.0156462\tvalid_1's rmse: 0.0207957\n",
      "[1700]\ttraining's rmse: 0.0150883\tvalid_1's rmse: 0.020282\n",
      "[1800]\ttraining's rmse: 0.0145882\tvalid_1's rmse: 0.0198365\n",
      "[1900]\ttraining's rmse: 0.0141557\tvalid_1's rmse: 0.0194616\n",
      "[2000]\ttraining's rmse: 0.0137669\tvalid_1's rmse: 0.0191467\n",
      "[2100]\ttraining's rmse: 0.0134021\tvalid_1's rmse: 0.0188469\n",
      "[2200]\ttraining's rmse: 0.0130946\tvalid_1's rmse: 0.0186137\n",
      "[2300]\ttraining's rmse: 0.0128093\tvalid_1's rmse: 0.0184047\n",
      "[2400]\ttraining's rmse: 0.0125123\tvalid_1's rmse: 0.0182007\n",
      "[2500]\ttraining's rmse: 0.012257\tvalid_1's rmse: 0.0180263\n",
      "[2600]\ttraining's rmse: 0.0120115\tvalid_1's rmse: 0.0178593\n",
      "[2700]\ttraining's rmse: 0.0117948\tvalid_1's rmse: 0.0177185\n",
      "[2800]\ttraining's rmse: 0.0115705\tvalid_1's rmse: 0.0175849\n",
      "[2900]\ttraining's rmse: 0.0113727\tvalid_1's rmse: 0.0174702\n",
      "[3000]\ttraining's rmse: 0.0111877\tvalid_1's rmse: 0.0173748\n",
      "[3100]\ttraining's rmse: 0.0109973\tvalid_1's rmse: 0.0172764\n",
      "[3200]\ttraining's rmse: 0.0108086\tvalid_1's rmse: 0.0171928\n",
      "[3300]\ttraining's rmse: 0.0106263\tvalid_1's rmse: 0.0170884\n",
      "[3400]\ttraining's rmse: 0.0104605\tvalid_1's rmse: 0.0169934\n",
      "[3500]\ttraining's rmse: 0.01032\tvalid_1's rmse: 0.0169302\n",
      "[3600]\ttraining's rmse: 0.0101729\tvalid_1's rmse: 0.0168511\n",
      "[3700]\ttraining's rmse: 0.0100434\tvalid_1's rmse: 0.0167979\n",
      "[3800]\ttraining's rmse: 0.00988133\tvalid_1's rmse: 0.0167123\n",
      "[3900]\ttraining's rmse: 0.00975752\tvalid_1's rmse: 0.0166539\n",
      "[4000]\ttraining's rmse: 0.00962091\tvalid_1's rmse: 0.0165928\n",
      "[4100]\ttraining's rmse: 0.00949258\tvalid_1's rmse: 0.0165427\n",
      "[4200]\ttraining's rmse: 0.00937351\tvalid_1's rmse: 0.0164909\n",
      "[4300]\ttraining's rmse: 0.00925491\tvalid_1's rmse: 0.0164527\n",
      "[4400]\ttraining's rmse: 0.00911849\tvalid_1's rmse: 0.0163949\n",
      "[4500]\ttraining's rmse: 0.00901366\tvalid_1's rmse: 0.0163661\n",
      "[4600]\ttraining's rmse: 0.0088979\tvalid_1's rmse: 0.0163087\n",
      "[4700]\ttraining's rmse: 0.0087757\tvalid_1's rmse: 0.0162679\n",
      "[4800]\ttraining's rmse: 0.00866984\tvalid_1's rmse: 0.0162271\n",
      "[4900]\ttraining's rmse: 0.00856369\tvalid_1's rmse: 0.0161796\n",
      "[5000]\ttraining's rmse: 0.00846815\tvalid_1's rmse: 0.0161499\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\ttraining's rmse: 0.0084688\tvalid_1's rmse: 0.0161497\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.135388\tvalid_1's rmse: 0.13911\n",
      "[200]\ttraining's rmse: 0.0829483\tvalid_1's rmse: 0.0882522\n",
      "[300]\ttraining's rmse: 0.0582037\tvalid_1's rmse: 0.0639104\n",
      "[400]\ttraining's rmse: 0.045248\tvalid_1's rmse: 0.0508168\n",
      "[500]\ttraining's rmse: 0.0368862\tvalid_1's rmse: 0.0422642\n",
      "[600]\ttraining's rmse: 0.0314969\tvalid_1's rmse: 0.0367044\n",
      "[700]\ttraining's rmse: 0.0278688\tvalid_1's rmse: 0.0329999\n",
      "[800]\ttraining's rmse: 0.0251012\tvalid_1's rmse: 0.0302201\n",
      "[900]\ttraining's rmse: 0.0229202\tvalid_1's rmse: 0.0280397\n",
      "[1000]\ttraining's rmse: 0.0210831\tvalid_1's rmse: 0.0261895\n",
      "[1100]\ttraining's rmse: 0.0195252\tvalid_1's rmse: 0.0245735\n",
      "[1200]\ttraining's rmse: 0.0183622\tvalid_1's rmse: 0.0234094\n",
      "[1300]\ttraining's rmse: 0.0173937\tvalid_1's rmse: 0.0224516\n",
      "[1400]\ttraining's rmse: 0.016596\tvalid_1's rmse: 0.0216736\n",
      "[1500]\ttraining's rmse: 0.0159078\tvalid_1's rmse: 0.0210242\n",
      "[1600]\ttraining's rmse: 0.0153183\tvalid_1's rmse: 0.0204775\n",
      "[1700]\ttraining's rmse: 0.0147968\tvalid_1's rmse: 0.0200326\n",
      "[1800]\ttraining's rmse: 0.014321\tvalid_1's rmse: 0.0196283\n",
      "[1900]\ttraining's rmse: 0.0139264\tvalid_1's rmse: 0.0192991\n",
      "[2000]\ttraining's rmse: 0.0135483\tvalid_1's rmse: 0.0190117\n",
      "[2100]\ttraining's rmse: 0.0131859\tvalid_1's rmse: 0.018704\n",
      "[2200]\ttraining's rmse: 0.0128716\tvalid_1's rmse: 0.0184877\n",
      "[2300]\ttraining's rmse: 0.0125776\tvalid_1's rmse: 0.0182804\n",
      "[2400]\ttraining's rmse: 0.0123201\tvalid_1's rmse: 0.0181015\n",
      "[2500]\ttraining's rmse: 0.0120506\tvalid_1's rmse: 0.0179199\n",
      "[2600]\ttraining's rmse: 0.0117962\tvalid_1's rmse: 0.0177578\n",
      "[2700]\ttraining's rmse: 0.0115475\tvalid_1's rmse: 0.0176062\n",
      "[2800]\ttraining's rmse: 0.0113465\tvalid_1's rmse: 0.0174962\n",
      "[2900]\ttraining's rmse: 0.0111431\tvalid_1's rmse: 0.0173665\n",
      "[3000]\ttraining's rmse: 0.0109486\tvalid_1's rmse: 0.0172581\n",
      "[3100]\ttraining's rmse: 0.0107721\tvalid_1's rmse: 0.0171666\n",
      "[3200]\ttraining's rmse: 0.0105943\tvalid_1's rmse: 0.0170686\n",
      "[3300]\ttraining's rmse: 0.0104394\tvalid_1's rmse: 0.0169967\n",
      "[3400]\ttraining's rmse: 0.0102882\tvalid_1's rmse: 0.0169344\n",
      "[3500]\ttraining's rmse: 0.0100914\tvalid_1's rmse: 0.016844\n",
      "[3600]\ttraining's rmse: 0.00994532\tvalid_1's rmse: 0.0167775\n",
      "[3700]\ttraining's rmse: 0.0097901\tvalid_1's rmse: 0.0166999\n",
      "[3800]\ttraining's rmse: 0.00964692\tvalid_1's rmse: 0.0166419\n",
      "[3900]\ttraining's rmse: 0.00950943\tvalid_1's rmse: 0.0165807\n",
      "[4000]\ttraining's rmse: 0.00937534\tvalid_1's rmse: 0.0165228\n",
      "[4100]\ttraining's rmse: 0.00925427\tvalid_1's rmse: 0.0164791\n",
      "[4200]\ttraining's rmse: 0.00913216\tvalid_1's rmse: 0.0164261\n",
      "[4300]\ttraining's rmse: 0.0090042\tvalid_1's rmse: 0.0163707\n",
      "[4400]\ttraining's rmse: 0.00889874\tvalid_1's rmse: 0.0163237\n",
      "[4500]\ttraining's rmse: 0.00878323\tvalid_1's rmse: 0.0162738\n",
      "[4600]\ttraining's rmse: 0.00867234\tvalid_1's rmse: 0.0162334\n",
      "[4700]\ttraining's rmse: 0.00856663\tvalid_1's rmse: 0.0161935\n",
      "[4800]\ttraining's rmse: 0.00845493\tvalid_1's rmse: 0.016152\n",
      "[4900]\ttraining's rmse: 0.00834814\tvalid_1's rmse: 0.0161181\n",
      "[5000]\ttraining's rmse: 0.00823728\tvalid_1's rmse: 0.0160751\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\ttraining's rmse: 0.0082381\tvalid_1's rmse: 0.0160748\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.152877\tvalid_1's rmse: 0.154268\n",
      "[200]\ttraining's rmse: 0.0992362\tvalid_1's rmse: 0.102259\n",
      "[300]\ttraining's rmse: 0.0692716\tvalid_1's rmse: 0.0730442\n",
      "[400]\ttraining's rmse: 0.0524766\tvalid_1's rmse: 0.0566558\n",
      "[500]\ttraining's rmse: 0.0427011\tvalid_1's rmse: 0.0471272\n",
      "[600]\ttraining's rmse: 0.0358481\tvalid_1's rmse: 0.0404015\n",
      "[700]\ttraining's rmse: 0.0306414\tvalid_1's rmse: 0.0352734\n",
      "[800]\ttraining's rmse: 0.027056\tvalid_1's rmse: 0.0318003\n",
      "[900]\ttraining's rmse: 0.0244418\tvalid_1's rmse: 0.0292621\n",
      "[1000]\ttraining's rmse: 0.0223985\tvalid_1's rmse: 0.0272669\n",
      "[1100]\ttraining's rmse: 0.0206879\tvalid_1's rmse: 0.0255843\n",
      "[1200]\ttraining's rmse: 0.0193353\tvalid_1's rmse: 0.0242662\n",
      "[1300]\ttraining's rmse: 0.0182717\tvalid_1's rmse: 0.0232308\n",
      "[1400]\ttraining's rmse: 0.0173859\tvalid_1's rmse: 0.0223468\n",
      "[1500]\ttraining's rmse: 0.0166943\tvalid_1's rmse: 0.0217122\n",
      "[1600]\ttraining's rmse: 0.0160667\tvalid_1's rmse: 0.021138\n",
      "[1700]\ttraining's rmse: 0.0154937\tvalid_1's rmse: 0.0206257\n",
      "[1800]\ttraining's rmse: 0.0149892\tvalid_1's rmse: 0.0201771\n",
      "[1900]\ttraining's rmse: 0.0145729\tvalid_1's rmse: 0.0198362\n",
      "[2000]\ttraining's rmse: 0.0141114\tvalid_1's rmse: 0.0194799\n",
      "[2100]\ttraining's rmse: 0.0137974\tvalid_1's rmse: 0.0192692\n",
      "[2200]\ttraining's rmse: 0.0134731\tvalid_1's rmse: 0.0190539\n",
      "[2300]\ttraining's rmse: 0.0131568\tvalid_1's rmse: 0.0188394\n",
      "[2400]\ttraining's rmse: 0.0128603\tvalid_1's rmse: 0.0186392\n",
      "[2500]\ttraining's rmse: 0.0126228\tvalid_1's rmse: 0.0184971\n",
      "[2600]\ttraining's rmse: 0.0123766\tvalid_1's rmse: 0.0183421\n",
      "[2700]\ttraining's rmse: 0.0121218\tvalid_1's rmse: 0.0181913\n",
      "[2800]\ttraining's rmse: 0.0119219\tvalid_1's rmse: 0.018093\n",
      "[2900]\ttraining's rmse: 0.0117436\tvalid_1's rmse: 0.0179979\n",
      "[3000]\ttraining's rmse: 0.0115685\tvalid_1's rmse: 0.0179256\n",
      "[3100]\ttraining's rmse: 0.01139\tvalid_1's rmse: 0.0178371\n",
      "[3200]\ttraining's rmse: 0.0111928\tvalid_1's rmse: 0.0177409\n",
      "[3300]\ttraining's rmse: 0.0110234\tvalid_1's rmse: 0.0176701\n",
      "[3400]\ttraining's rmse: 0.0108557\tvalid_1's rmse: 0.0175844\n",
      "[3500]\ttraining's rmse: 0.0106949\tvalid_1's rmse: 0.0175124\n",
      "[3600]\ttraining's rmse: 0.0105588\tvalid_1's rmse: 0.0174577\n",
      "[3700]\ttraining's rmse: 0.010405\tvalid_1's rmse: 0.0173942\n",
      "[3800]\ttraining's rmse: 0.010258\tvalid_1's rmse: 0.0173263\n",
      "[3900]\ttraining's rmse: 0.0101048\tvalid_1's rmse: 0.0172591\n",
      "[4000]\ttraining's rmse: 0.00996958\tvalid_1's rmse: 0.0172069\n",
      "[4100]\ttraining's rmse: 0.00983937\tvalid_1's rmse: 0.0171534\n",
      "[4200]\ttraining's rmse: 0.00970027\tvalid_1's rmse: 0.0170921\n",
      "[4300]\ttraining's rmse: 0.00956826\tvalid_1's rmse: 0.0170422\n",
      "[4400]\ttraining's rmse: 0.0094296\tvalid_1's rmse: 0.0169898\n",
      "[4500]\ttraining's rmse: 0.00931345\tvalid_1's rmse: 0.0169418\n",
      "[4600]\ttraining's rmse: 0.00919521\tvalid_1's rmse: 0.0168974\n",
      "[4700]\ttraining's rmse: 0.0090693\tvalid_1's rmse: 0.016842\n",
      "[4800]\ttraining's rmse: 0.00893721\tvalid_1's rmse: 0.0167864\n",
      "[4900]\ttraining's rmse: 0.00883681\tvalid_1's rmse: 0.0167642\n",
      "[5000]\ttraining's rmse: 0.00873029\tvalid_1's rmse: 0.0167225\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00873029\tvalid_1's rmse: 0.0167225\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.146979\tvalid_1's rmse: 0.151765\n",
      "[200]\ttraining's rmse: 0.0948327\tvalid_1's rmse: 0.101214\n",
      "[300]\ttraining's rmse: 0.0663738\tvalid_1's rmse: 0.0728604\n",
      "[400]\ttraining's rmse: 0.0504114\tvalid_1's rmse: 0.056655\n",
      "[500]\ttraining's rmse: 0.041366\tvalid_1's rmse: 0.0474074\n",
      "[600]\ttraining's rmse: 0.0349885\tvalid_1's rmse: 0.0408048\n",
      "[700]\ttraining's rmse: 0.0302198\tvalid_1's rmse: 0.0358241\n",
      "[800]\ttraining's rmse: 0.0269726\tvalid_1's rmse: 0.0324284\n",
      "[900]\ttraining's rmse: 0.0245888\tvalid_1's rmse: 0.0299682\n",
      "[1000]\ttraining's rmse: 0.0227131\tvalid_1's rmse: 0.0279843\n",
      "[1100]\ttraining's rmse: 0.0211057\tvalid_1's rmse: 0.0263\n",
      "[1200]\ttraining's rmse: 0.0198275\tvalid_1's rmse: 0.024954\n",
      "[1300]\ttraining's rmse: 0.0187991\tvalid_1's rmse: 0.0238766\n",
      "[1400]\ttraining's rmse: 0.0178917\tvalid_1's rmse: 0.0229268\n",
      "[1500]\ttraining's rmse: 0.0171813\tvalid_1's rmse: 0.0222333\n",
      "[1600]\ttraining's rmse: 0.0165635\tvalid_1's rmse: 0.0216615\n",
      "[1700]\ttraining's rmse: 0.0159648\tvalid_1's rmse: 0.0211136\n",
      "[1800]\ttraining's rmse: 0.0154336\tvalid_1's rmse: 0.0206127\n",
      "[1900]\ttraining's rmse: 0.0149933\tvalid_1's rmse: 0.0202603\n",
      "[2000]\ttraining's rmse: 0.0145459\tvalid_1's rmse: 0.019868\n",
      "[2100]\ttraining's rmse: 0.0141779\tvalid_1's rmse: 0.0195999\n",
      "[2200]\ttraining's rmse: 0.013849\tvalid_1's rmse: 0.0193681\n",
      "[2300]\ttraining's rmse: 0.013536\tvalid_1's rmse: 0.019156\n",
      "[2400]\ttraining's rmse: 0.0132193\tvalid_1's rmse: 0.0189361\n",
      "[2500]\ttraining's rmse: 0.0129593\tvalid_1's rmse: 0.0187711\n",
      "[2600]\ttraining's rmse: 0.0126991\tvalid_1's rmse: 0.0185873\n",
      "[2700]\ttraining's rmse: 0.0124536\tvalid_1's rmse: 0.018424\n",
      "[2800]\ttraining's rmse: 0.0122217\tvalid_1's rmse: 0.0182795\n",
      "[2900]\ttraining's rmse: 0.011989\tvalid_1's rmse: 0.0181263\n",
      "[3000]\ttraining's rmse: 0.0117771\tvalid_1's rmse: 0.0179989\n",
      "[3100]\ttraining's rmse: 0.0115656\tvalid_1's rmse: 0.0178877\n",
      "[3200]\ttraining's rmse: 0.0113721\tvalid_1's rmse: 0.0177839\n",
      "[3300]\ttraining's rmse: 0.011185\tvalid_1's rmse: 0.0176852\n",
      "[3400]\ttraining's rmse: 0.0110043\tvalid_1's rmse: 0.0175964\n",
      "[3500]\ttraining's rmse: 0.0108253\tvalid_1's rmse: 0.0174991\n",
      "[3600]\ttraining's rmse: 0.0106796\tvalid_1's rmse: 0.0174484\n",
      "[3700]\ttraining's rmse: 0.0105148\tvalid_1's rmse: 0.0173581\n",
      "[3800]\ttraining's rmse: 0.0103724\tvalid_1's rmse: 0.0172952\n",
      "[3900]\ttraining's rmse: 0.0102144\tvalid_1's rmse: 0.0172267\n",
      "[4000]\ttraining's rmse: 0.0100699\tvalid_1's rmse: 0.0171772\n",
      "[4100]\ttraining's rmse: 0.00993709\tvalid_1's rmse: 0.0171207\n",
      "[4200]\ttraining's rmse: 0.00980153\tvalid_1's rmse: 0.0170626\n",
      "[4300]\ttraining's rmse: 0.0096577\tvalid_1's rmse: 0.0169918\n",
      "[4400]\ttraining's rmse: 0.0095355\tvalid_1's rmse: 0.0169448\n",
      "[4500]\ttraining's rmse: 0.00940798\tvalid_1's rmse: 0.0168929\n",
      "[4600]\ttraining's rmse: 0.0092848\tvalid_1's rmse: 0.0168429\n",
      "[4700]\ttraining's rmse: 0.00917775\tvalid_1's rmse: 0.0168044\n",
      "[4800]\ttraining's rmse: 0.00906702\tvalid_1's rmse: 0.0167661\n",
      "[4900]\ttraining's rmse: 0.00895879\tvalid_1's rmse: 0.0167308\n",
      "[5000]\ttraining's rmse: 0.00884265\tvalid_1's rmse: 0.0166849\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00884265\tvalid_1's rmse: 0.0166849\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.144628\tvalid_1's rmse: 0.146748\n",
      "[200]\ttraining's rmse: 0.0932806\tvalid_1's rmse: 0.0975284\n",
      "[300]\ttraining's rmse: 0.0649967\tvalid_1's rmse: 0.0702433\n",
      "[400]\ttraining's rmse: 0.0511798\tvalid_1's rmse: 0.0568602\n",
      "[500]\ttraining's rmse: 0.042164\tvalid_1's rmse: 0.048062\n",
      "[600]\ttraining's rmse: 0.0359105\tvalid_1's rmse: 0.0418829\n",
      "[700]\ttraining's rmse: 0.0310732\tvalid_1's rmse: 0.0370835\n",
      "[800]\ttraining's rmse: 0.0275426\tvalid_1's rmse: 0.0335191\n",
      "[900]\ttraining's rmse: 0.0247249\tvalid_1's rmse: 0.0306589\n",
      "[1000]\ttraining's rmse: 0.0225135\tvalid_1's rmse: 0.0283569\n",
      "[1100]\ttraining's rmse: 0.0207873\tvalid_1's rmse: 0.0265673\n",
      "[1200]\ttraining's rmse: 0.0193823\tvalid_1's rmse: 0.025092\n",
      "[1300]\ttraining's rmse: 0.0181651\tvalid_1's rmse: 0.0237968\n",
      "[1400]\ttraining's rmse: 0.0171515\tvalid_1's rmse: 0.022683\n",
      "[1500]\ttraining's rmse: 0.0163046\tvalid_1's rmse: 0.0217641\n",
      "[1600]\ttraining's rmse: 0.0156064\tvalid_1's rmse: 0.0210458\n",
      "[1700]\ttraining's rmse: 0.0150081\tvalid_1's rmse: 0.0204497\n",
      "[1800]\ttraining's rmse: 0.0144822\tvalid_1's rmse: 0.0199371\n",
      "[1900]\ttraining's rmse: 0.0139998\tvalid_1's rmse: 0.0194664\n",
      "[2000]\ttraining's rmse: 0.0135684\tvalid_1's rmse: 0.0190575\n",
      "[2100]\ttraining's rmse: 0.0131892\tvalid_1's rmse: 0.0187206\n",
      "[2200]\ttraining's rmse: 0.012854\tvalid_1's rmse: 0.0184355\n",
      "[2300]\ttraining's rmse: 0.0125522\tvalid_1's rmse: 0.0182019\n",
      "[2400]\ttraining's rmse: 0.0122648\tvalid_1's rmse: 0.0179811\n",
      "[2500]\ttraining's rmse: 0.0119987\tvalid_1's rmse: 0.0177869\n",
      "[2600]\ttraining's rmse: 0.0117416\tvalid_1's rmse: 0.0175904\n",
      "[2700]\ttraining's rmse: 0.0115031\tvalid_1's rmse: 0.0174211\n",
      "[2800]\ttraining's rmse: 0.0112848\tvalid_1's rmse: 0.0172748\n",
      "[2900]\ttraining's rmse: 0.0110646\tvalid_1's rmse: 0.0171194\n",
      "[3000]\ttraining's rmse: 0.0108781\tvalid_1's rmse: 0.0169959\n",
      "[3100]\ttraining's rmse: 0.0106821\tvalid_1's rmse: 0.0168803\n",
      "[3200]\ttraining's rmse: 0.0105145\tvalid_1's rmse: 0.0167966\n",
      "[3300]\ttraining's rmse: 0.0103262\tvalid_1's rmse: 0.0166901\n",
      "[3400]\ttraining's rmse: 0.0101477\tvalid_1's rmse: 0.0165909\n",
      "[3500]\ttraining's rmse: 0.00998718\tvalid_1's rmse: 0.0165098\n",
      "[3600]\ttraining's rmse: 0.00982739\tvalid_1's rmse: 0.0164147\n",
      "[3700]\ttraining's rmse: 0.00968551\tvalid_1's rmse: 0.0163473\n",
      "[3800]\ttraining's rmse: 0.00954651\tvalid_1's rmse: 0.0162793\n",
      "[3900]\ttraining's rmse: 0.00940519\tvalid_1's rmse: 0.0162146\n",
      "[4000]\ttraining's rmse: 0.00927551\tvalid_1's rmse: 0.0161604\n",
      "[4100]\ttraining's rmse: 0.00914506\tvalid_1's rmse: 0.0160979\n",
      "[4200]\ttraining's rmse: 0.00899691\tvalid_1's rmse: 0.0160158\n",
      "[4300]\ttraining's rmse: 0.00887865\tvalid_1's rmse: 0.0159646\n",
      "[4400]\ttraining's rmse: 0.00876693\tvalid_1's rmse: 0.0159171\n",
      "[4500]\ttraining's rmse: 0.008661\tvalid_1's rmse: 0.0158803\n",
      "[4600]\ttraining's rmse: 0.00855363\tvalid_1's rmse: 0.0158296\n",
      "[4700]\ttraining's rmse: 0.00844493\tvalid_1's rmse: 0.0157852\n",
      "[4800]\ttraining's rmse: 0.00833637\tvalid_1's rmse: 0.0157359\n",
      "[4900]\ttraining's rmse: 0.00824358\tvalid_1's rmse: 0.0157101\n",
      "[5000]\ttraining's rmse: 0.00813625\tvalid_1's rmse: 0.0156618\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00813625\tvalid_1's rmse: 0.0156618\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.149616\tvalid_1's rmse: 0.151615\n",
      "[200]\ttraining's rmse: 0.0957743\tvalid_1's rmse: 0.099448\n",
      "[300]\ttraining's rmse: 0.0669996\tvalid_1's rmse: 0.0714802\n",
      "[400]\ttraining's rmse: 0.0500918\tvalid_1's rmse: 0.0548554\n",
      "[500]\ttraining's rmse: 0.0399362\tvalid_1's rmse: 0.0448416\n",
      "[600]\ttraining's rmse: 0.0324194\tvalid_1's rmse: 0.0371936\n",
      "[700]\ttraining's rmse: 0.027967\tvalid_1's rmse: 0.0326991\n",
      "[800]\ttraining's rmse: 0.0248082\tvalid_1's rmse: 0.0294587\n",
      "[900]\ttraining's rmse: 0.0223462\tvalid_1's rmse: 0.026937\n",
      "[1000]\ttraining's rmse: 0.020425\tvalid_1's rmse: 0.0249588\n",
      "[1100]\ttraining's rmse: 0.0189604\tvalid_1's rmse: 0.0234454\n",
      "[1200]\ttraining's rmse: 0.0178546\tvalid_1's rmse: 0.0223151\n",
      "[1300]\ttraining's rmse: 0.0170185\tvalid_1's rmse: 0.021494\n",
      "[1400]\ttraining's rmse: 0.0162849\tvalid_1's rmse: 0.0207528\n",
      "[1500]\ttraining's rmse: 0.0156216\tvalid_1's rmse: 0.0200685\n",
      "[1600]\ttraining's rmse: 0.0151249\tvalid_1's rmse: 0.0196137\n",
      "[1700]\ttraining's rmse: 0.0146709\tvalid_1's rmse: 0.0192388\n",
      "[1800]\ttraining's rmse: 0.0142757\tvalid_1's rmse: 0.0189164\n",
      "[1900]\ttraining's rmse: 0.0139277\tvalid_1's rmse: 0.0186644\n",
      "[2000]\ttraining's rmse: 0.0136038\tvalid_1's rmse: 0.0184234\n",
      "[2100]\ttraining's rmse: 0.013304\tvalid_1's rmse: 0.0182212\n",
      "[2200]\ttraining's rmse: 0.0130005\tvalid_1's rmse: 0.0180113\n",
      "[2300]\ttraining's rmse: 0.0127454\tvalid_1's rmse: 0.0178593\n",
      "[2400]\ttraining's rmse: 0.012507\tvalid_1's rmse: 0.0177307\n",
      "[2500]\ttraining's rmse: 0.0122647\tvalid_1's rmse: 0.0175787\n",
      "[2600]\ttraining's rmse: 0.0120476\tvalid_1's rmse: 0.0174578\n",
      "[2700]\ttraining's rmse: 0.0118414\tvalid_1's rmse: 0.0173555\n",
      "[2800]\ttraining's rmse: 0.011646\tvalid_1's rmse: 0.0172542\n",
      "[2900]\ttraining's rmse: 0.0114727\tvalid_1's rmse: 0.0171634\n",
      "[3000]\ttraining's rmse: 0.011305\tvalid_1's rmse: 0.0170876\n",
      "[3100]\ttraining's rmse: 0.0111237\tvalid_1's rmse: 0.0169937\n",
      "[3200]\ttraining's rmse: 0.0109718\tvalid_1's rmse: 0.0169373\n",
      "[3300]\ttraining's rmse: 0.0108276\tvalid_1's rmse: 0.0168789\n",
      "[3400]\ttraining's rmse: 0.010662\tvalid_1's rmse: 0.0167989\n",
      "[3500]\ttraining's rmse: 0.0105127\tvalid_1's rmse: 0.0167265\n",
      "[3600]\ttraining's rmse: 0.010365\tvalid_1's rmse: 0.016656\n",
      "[3700]\ttraining's rmse: 0.0102266\tvalid_1's rmse: 0.0166025\n",
      "[3800]\ttraining's rmse: 0.0100811\tvalid_1's rmse: 0.016538\n",
      "[3900]\ttraining's rmse: 0.00994927\tvalid_1's rmse: 0.0164766\n",
      "[4000]\ttraining's rmse: 0.00982976\tvalid_1's rmse: 0.0164333\n",
      "[4100]\ttraining's rmse: 0.00969877\tvalid_1's rmse: 0.0163671\n",
      "[4200]\ttraining's rmse: 0.00958123\tvalid_1's rmse: 0.0163223\n",
      "[4300]\ttraining's rmse: 0.00947667\tvalid_1's rmse: 0.016292\n",
      "[4400]\ttraining's rmse: 0.0093445\tvalid_1's rmse: 0.0162388\n",
      "[4500]\ttraining's rmse: 0.00922903\tvalid_1's rmse: 0.0162066\n",
      "[4600]\ttraining's rmse: 0.00911981\tvalid_1's rmse: 0.0161707\n",
      "[4700]\ttraining's rmse: 0.00901736\tvalid_1's rmse: 0.0161324\n",
      "[4800]\ttraining's rmse: 0.00891317\tvalid_1's rmse: 0.0160962\n",
      "[4900]\ttraining's rmse: 0.00881631\tvalid_1's rmse: 0.0160745\n",
      "[5000]\ttraining's rmse: 0.00872027\tvalid_1's rmse: 0.0160403\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4997]\ttraining's rmse: 0.00872315\tvalid_1's rmse: 0.0160396\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.142366\tvalid_1's rmse: 0.144184\n",
      "[200]\ttraining's rmse: 0.0923908\tvalid_1's rmse: 0.0958307\n",
      "[300]\ttraining's rmse: 0.0658909\tvalid_1's rmse: 0.0698722\n",
      "[400]\ttraining's rmse: 0.0509964\tvalid_1's rmse: 0.0552607\n",
      "[500]\ttraining's rmse: 0.0423116\tvalid_1's rmse: 0.0469052\n",
      "[600]\ttraining's rmse: 0.0360695\tvalid_1's rmse: 0.0408316\n",
      "[700]\ttraining's rmse: 0.03117\tvalid_1's rmse: 0.0361063\n",
      "[800]\ttraining's rmse: 0.0276325\tvalid_1's rmse: 0.0326835\n",
      "[900]\ttraining's rmse: 0.0249535\tvalid_1's rmse: 0.0301476\n",
      "[1000]\ttraining's rmse: 0.0227727\tvalid_1's rmse: 0.0280153\n",
      "[1100]\ttraining's rmse: 0.0208752\tvalid_1's rmse: 0.0261513\n",
      "[1200]\ttraining's rmse: 0.0193363\tvalid_1's rmse: 0.0246356\n",
      "[1300]\ttraining's rmse: 0.0180772\tvalid_1's rmse: 0.0234336\n",
      "[1400]\ttraining's rmse: 0.0170045\tvalid_1's rmse: 0.0223754\n",
      "[1500]\ttraining's rmse: 0.0161684\tvalid_1's rmse: 0.0215848\n",
      "[1600]\ttraining's rmse: 0.0154287\tvalid_1's rmse: 0.0209055\n",
      "[1700]\ttraining's rmse: 0.0147748\tvalid_1's rmse: 0.0203173\n",
      "[1800]\ttraining's rmse: 0.0141445\tvalid_1's rmse: 0.0197328\n",
      "[1900]\ttraining's rmse: 0.013643\tvalid_1's rmse: 0.0193116\n",
      "[2000]\ttraining's rmse: 0.01317\tvalid_1's rmse: 0.0189055\n",
      "[2100]\ttraining's rmse: 0.012788\tvalid_1's rmse: 0.0186147\n",
      "[2200]\ttraining's rmse: 0.0124252\tvalid_1's rmse: 0.0183237\n",
      "[2300]\ttraining's rmse: 0.0120915\tvalid_1's rmse: 0.0180749\n",
      "[2400]\ttraining's rmse: 0.011758\tvalid_1's rmse: 0.0178226\n",
      "[2500]\ttraining's rmse: 0.0114815\tvalid_1's rmse: 0.0176276\n",
      "[2600]\ttraining's rmse: 0.0111968\tvalid_1's rmse: 0.0174455\n",
      "[2700]\ttraining's rmse: 0.01093\tvalid_1's rmse: 0.0172791\n",
      "[2800]\ttraining's rmse: 0.0107029\tvalid_1's rmse: 0.0171418\n",
      "[2900]\ttraining's rmse: 0.0104816\tvalid_1's rmse: 0.0170058\n",
      "[3000]\ttraining's rmse: 0.0102652\tvalid_1's rmse: 0.0168871\n",
      "[3100]\ttraining's rmse: 0.0100677\tvalid_1's rmse: 0.0167856\n",
      "[3200]\ttraining's rmse: 0.00988091\tvalid_1's rmse: 0.0166892\n",
      "[3300]\ttraining's rmse: 0.00970337\tvalid_1's rmse: 0.0165951\n",
      "[3400]\ttraining's rmse: 0.00953666\tvalid_1's rmse: 0.0165234\n",
      "[3500]\ttraining's rmse: 0.00936031\tvalid_1's rmse: 0.0164419\n",
      "[3600]\ttraining's rmse: 0.00920941\tvalid_1's rmse: 0.0163765\n",
      "[3700]\ttraining's rmse: 0.00906772\tvalid_1's rmse: 0.0163055\n",
      "[3800]\ttraining's rmse: 0.00891483\tvalid_1's rmse: 0.0162355\n",
      "[3900]\ttraining's rmse: 0.00877701\tvalid_1's rmse: 0.0161799\n",
      "[4000]\ttraining's rmse: 0.00864011\tvalid_1's rmse: 0.016124\n",
      "[4100]\ttraining's rmse: 0.00851871\tvalid_1's rmse: 0.0160761\n",
      "[4200]\ttraining's rmse: 0.00839497\tvalid_1's rmse: 0.0160277\n",
      "[4300]\ttraining's rmse: 0.00828075\tvalid_1's rmse: 0.0159964\n",
      "[4400]\ttraining's rmse: 0.0081696\tvalid_1's rmse: 0.0159553\n",
      "[4500]\ttraining's rmse: 0.00806598\tvalid_1's rmse: 0.015924\n",
      "[4600]\ttraining's rmse: 0.00795398\tvalid_1's rmse: 0.0158826\n",
      "[4700]\ttraining's rmse: 0.00785207\tvalid_1's rmse: 0.0158587\n",
      "[4800]\ttraining's rmse: 0.00775418\tvalid_1's rmse: 0.0158215\n",
      "[4900]\ttraining's rmse: 0.0076645\tvalid_1's rmse: 0.0158011\n",
      "[5000]\ttraining's rmse: 0.00755564\tvalid_1's rmse: 0.0157563\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00755564\tvalid_1's rmse: 0.0157563\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.147179\tvalid_1's rmse: 0.149376\n",
      "[200]\ttraining's rmse: 0.0950474\tvalid_1's rmse: 0.0988122\n",
      "[300]\ttraining's rmse: 0.0670743\tvalid_1's rmse: 0.0714584\n",
      "[400]\ttraining's rmse: 0.051697\tvalid_1's rmse: 0.0564151\n",
      "[500]\ttraining's rmse: 0.0428422\tvalid_1's rmse: 0.0477448\n",
      "[600]\ttraining's rmse: 0.0365278\tvalid_1's rmse: 0.0416219\n",
      "[700]\ttraining's rmse: 0.0315654\tvalid_1's rmse: 0.0367806\n",
      "[800]\ttraining's rmse: 0.028043\tvalid_1's rmse: 0.0333797\n",
      "[900]\ttraining's rmse: 0.0253807\tvalid_1's rmse: 0.0308486\n",
      "[1000]\ttraining's rmse: 0.0232175\tvalid_1's rmse: 0.0287327\n",
      "[1100]\ttraining's rmse: 0.0213494\tvalid_1's rmse: 0.0268981\n",
      "[1200]\ttraining's rmse: 0.019858\tvalid_1's rmse: 0.0254217\n",
      "[1300]\ttraining's rmse: 0.0186321\tvalid_1's rmse: 0.0242479\n",
      "[1400]\ttraining's rmse: 0.017583\tvalid_1's rmse: 0.0232051\n",
      "[1500]\ttraining's rmse: 0.0167615\tvalid_1's rmse: 0.0224374\n",
      "[1600]\ttraining's rmse: 0.0160515\tvalid_1's rmse: 0.0217675\n",
      "[1700]\ttraining's rmse: 0.0154209\tvalid_1's rmse: 0.0211847\n",
      "[1800]\ttraining's rmse: 0.0148157\tvalid_1's rmse: 0.0206119\n",
      "[1900]\ttraining's rmse: 0.0143364\tvalid_1's rmse: 0.0202076\n",
      "[2000]\ttraining's rmse: 0.0138668\tvalid_1's rmse: 0.0197916\n",
      "[2100]\ttraining's rmse: 0.0134862\tvalid_1's rmse: 0.019497\n",
      "[2200]\ttraining's rmse: 0.0131359\tvalid_1's rmse: 0.0192336\n",
      "[2300]\ttraining's rmse: 0.012808\tvalid_1's rmse: 0.018987\n",
      "[2400]\ttraining's rmse: 0.0125059\tvalid_1's rmse: 0.0187671\n",
      "[2500]\ttraining's rmse: 0.0122398\tvalid_1's rmse: 0.0185879\n",
      "[2600]\ttraining's rmse: 0.0119819\tvalid_1's rmse: 0.0184107\n",
      "[2700]\ttraining's rmse: 0.0117298\tvalid_1's rmse: 0.018265\n",
      "[2800]\ttraining's rmse: 0.0115083\tvalid_1's rmse: 0.0181506\n",
      "[2900]\ttraining's rmse: 0.0112952\tvalid_1's rmse: 0.0180328\n",
      "[3000]\ttraining's rmse: 0.0110906\tvalid_1's rmse: 0.0179167\n",
      "[3100]\ttraining's rmse: 0.0109026\tvalid_1's rmse: 0.0178316\n",
      "[3200]\ttraining's rmse: 0.0107194\tvalid_1's rmse: 0.0177414\n",
      "[3300]\ttraining's rmse: 0.0105428\tvalid_1's rmse: 0.0176439\n",
      "[3400]\ttraining's rmse: 0.010376\tvalid_1's rmse: 0.017574\n",
      "[3500]\ttraining's rmse: 0.0102296\tvalid_1's rmse: 0.0175241\n",
      "[3600]\ttraining's rmse: 0.0100791\tvalid_1's rmse: 0.0174633\n",
      "[3700]\ttraining's rmse: 0.00993065\tvalid_1's rmse: 0.017407\n",
      "[3800]\ttraining's rmse: 0.0097883\tvalid_1's rmse: 0.017342\n",
      "[3900]\ttraining's rmse: 0.00964973\tvalid_1's rmse: 0.0172997\n",
      "[4000]\ttraining's rmse: 0.0095243\tvalid_1's rmse: 0.0172616\n",
      "[4100]\ttraining's rmse: 0.00939267\tvalid_1's rmse: 0.0172054\n",
      "[4200]\ttraining's rmse: 0.00925437\tvalid_1's rmse: 0.0171392\n",
      "[4300]\ttraining's rmse: 0.00914335\tvalid_1's rmse: 0.0171149\n",
      "[4400]\ttraining's rmse: 0.00903241\tvalid_1's rmse: 0.01708\n",
      "[4500]\ttraining's rmse: 0.00890738\tvalid_1's rmse: 0.0170304\n",
      "[4600]\ttraining's rmse: 0.00879834\tvalid_1's rmse: 0.0169884\n",
      "[4700]\ttraining's rmse: 0.00868465\tvalid_1's rmse: 0.0169466\n",
      "[4800]\ttraining's rmse: 0.00858032\tvalid_1's rmse: 0.0169119\n",
      "[4900]\ttraining's rmse: 0.00847834\tvalid_1's rmse: 0.0168758\n",
      "[5000]\ttraining's rmse: 0.0083854\tvalid_1's rmse: 0.0168458\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.0083854\tvalid_1's rmse: 0.0168458\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.145494\tvalid_1's rmse: 0.147148\n",
      "[200]\ttraining's rmse: 0.0934\tvalid_1's rmse: 0.0967246\n",
      "[300]\ttraining's rmse: 0.0651107\tvalid_1's rmse: 0.0693295\n",
      "[400]\ttraining's rmse: 0.0516194\tvalid_1's rmse: 0.0563253\n",
      "[500]\ttraining's rmse: 0.0427971\tvalid_1's rmse: 0.0477775\n",
      "[600]\ttraining's rmse: 0.0366086\tvalid_1's rmse: 0.0417357\n",
      "[700]\ttraining's rmse: 0.0316722\tvalid_1's rmse: 0.0369579\n",
      "[800]\ttraining's rmse: 0.028054\tvalid_1's rmse: 0.033454\n",
      "[900]\ttraining's rmse: 0.0251543\tvalid_1's rmse: 0.0306308\n",
      "[1000]\ttraining's rmse: 0.0229026\tvalid_1's rmse: 0.0284175\n",
      "[1100]\ttraining's rmse: 0.02118\tvalid_1's rmse: 0.0267294\n",
      "[1200]\ttraining's rmse: 0.0197672\tvalid_1's rmse: 0.0253196\n",
      "[1300]\ttraining's rmse: 0.0185662\tvalid_1's rmse: 0.0241065\n",
      "[1400]\ttraining's rmse: 0.0175346\tvalid_1's rmse: 0.0230608\n",
      "[1500]\ttraining's rmse: 0.0166975\tvalid_1's rmse: 0.0222167\n",
      "[1600]\ttraining's rmse: 0.0159949\tvalid_1's rmse: 0.0215701\n",
      "[1700]\ttraining's rmse: 0.0154021\tvalid_1's rmse: 0.0210456\n",
      "[1800]\ttraining's rmse: 0.0148626\tvalid_1's rmse: 0.0205599\n",
      "[1900]\ttraining's rmse: 0.0143787\tvalid_1's rmse: 0.0201533\n",
      "[2000]\ttraining's rmse: 0.0139354\tvalid_1's rmse: 0.0198016\n",
      "[2100]\ttraining's rmse: 0.0135492\tvalid_1's rmse: 0.019487\n",
      "[2200]\ttraining's rmse: 0.0132054\tvalid_1's rmse: 0.0192401\n",
      "[2300]\ttraining's rmse: 0.0128828\tvalid_1's rmse: 0.0190017\n",
      "[2400]\ttraining's rmse: 0.0125683\tvalid_1's rmse: 0.018785\n",
      "[2500]\ttraining's rmse: 0.0122818\tvalid_1's rmse: 0.0186003\n",
      "[2600]\ttraining's rmse: 0.0120345\tvalid_1's rmse: 0.0184173\n",
      "[2700]\ttraining's rmse: 0.0117946\tvalid_1's rmse: 0.0182644\n",
      "[2800]\ttraining's rmse: 0.0115754\tvalid_1's rmse: 0.0181345\n",
      "[2900]\ttraining's rmse: 0.0113706\tvalid_1's rmse: 0.0179968\n",
      "[3000]\ttraining's rmse: 0.0111529\tvalid_1's rmse: 0.0178686\n",
      "[3100]\ttraining's rmse: 0.0109667\tvalid_1's rmse: 0.0177667\n",
      "[3200]\ttraining's rmse: 0.010769\tvalid_1's rmse: 0.0176445\n",
      "[3300]\ttraining's rmse: 0.0105905\tvalid_1's rmse: 0.0175575\n",
      "[3400]\ttraining's rmse: 0.0104027\tvalid_1's rmse: 0.0174568\n",
      "[3500]\ttraining's rmse: 0.0102413\tvalid_1's rmse: 0.0173718\n",
      "[3600]\ttraining's rmse: 0.0100741\tvalid_1's rmse: 0.0172723\n",
      "[3700]\ttraining's rmse: 0.00992753\tvalid_1's rmse: 0.0172108\n",
      "[3800]\ttraining's rmse: 0.00977263\tvalid_1's rmse: 0.0171309\n",
      "[3900]\ttraining's rmse: 0.00964027\tvalid_1's rmse: 0.0170665\n",
      "[4000]\ttraining's rmse: 0.00950124\tvalid_1's rmse: 0.0170002\n",
      "[4100]\ttraining's rmse: 0.0093621\tvalid_1's rmse: 0.0169262\n",
      "[4200]\ttraining's rmse: 0.0092234\tvalid_1's rmse: 0.0168627\n",
      "[4300]\ttraining's rmse: 0.00910404\tvalid_1's rmse: 0.0168137\n",
      "[4400]\ttraining's rmse: 0.0089852\tvalid_1's rmse: 0.0167665\n",
      "[4500]\ttraining's rmse: 0.00886963\tvalid_1's rmse: 0.0167269\n",
      "[4600]\ttraining's rmse: 0.00875484\tvalid_1's rmse: 0.0166858\n",
      "[4700]\ttraining's rmse: 0.00864833\tvalid_1's rmse: 0.0166511\n",
      "[4800]\ttraining's rmse: 0.00853173\tvalid_1's rmse: 0.0166061\n",
      "[4900]\ttraining's rmse: 0.00842187\tvalid_1's rmse: 0.0165597\n",
      "[5000]\ttraining's rmse: 0.00830799\tvalid_1's rmse: 0.0165125\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\ttraining's rmse: 0.00830887\tvalid_1's rmse: 0.0165124\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.144806\tvalid_1's rmse: 0.148709\n",
      "[200]\ttraining's rmse: 0.0934485\tvalid_1's rmse: 0.098818\n",
      "[300]\ttraining's rmse: 0.0646525\tvalid_1's rmse: 0.0704318\n",
      "[400]\ttraining's rmse: 0.0506147\tvalid_1's rmse: 0.0565118\n",
      "[500]\ttraining's rmse: 0.0414825\tvalid_1's rmse: 0.0474133\n",
      "[600]\ttraining's rmse: 0.0352992\tvalid_1's rmse: 0.0412146\n",
      "[700]\ttraining's rmse: 0.0305605\tvalid_1's rmse: 0.0363863\n",
      "[800]\ttraining's rmse: 0.0271505\tvalid_1's rmse: 0.0329\n",
      "[900]\ttraining's rmse: 0.0244583\tvalid_1's rmse: 0.0301645\n",
      "[1000]\ttraining's rmse: 0.0223462\tvalid_1's rmse: 0.0279436\n",
      "[1100]\ttraining's rmse: 0.0207524\tvalid_1's rmse: 0.0262863\n",
      "[1200]\ttraining's rmse: 0.0194408\tvalid_1's rmse: 0.0249346\n",
      "[1300]\ttraining's rmse: 0.0183151\tvalid_1's rmse: 0.0237208\n",
      "[1400]\ttraining's rmse: 0.0174056\tvalid_1's rmse: 0.0227359\n",
      "[1500]\ttraining's rmse: 0.01663\tvalid_1's rmse: 0.0219339\n",
      "[1600]\ttraining's rmse: 0.0159982\tvalid_1's rmse: 0.021298\n",
      "[1700]\ttraining's rmse: 0.015444\tvalid_1's rmse: 0.0207733\n",
      "[1800]\ttraining's rmse: 0.0149413\tvalid_1's rmse: 0.0203288\n",
      "[1900]\ttraining's rmse: 0.0144849\tvalid_1's rmse: 0.0199148\n",
      "[2000]\ttraining's rmse: 0.0140793\tvalid_1's rmse: 0.0195691\n",
      "[2100]\ttraining's rmse: 0.0137139\tvalid_1's rmse: 0.0192742\n",
      "[2200]\ttraining's rmse: 0.01339\tvalid_1's rmse: 0.0190185\n",
      "[2300]\ttraining's rmse: 0.0131057\tvalid_1's rmse: 0.0188149\n",
      "[2400]\ttraining's rmse: 0.0128253\tvalid_1's rmse: 0.0186374\n",
      "[2500]\ttraining's rmse: 0.0125686\tvalid_1's rmse: 0.0184737\n",
      "[2600]\ttraining's rmse: 0.0122911\tvalid_1's rmse: 0.0182761\n",
      "[2700]\ttraining's rmse: 0.0120543\tvalid_1's rmse: 0.0181367\n",
      "[2800]\ttraining's rmse: 0.0118389\tvalid_1's rmse: 0.0180113\n",
      "[2900]\ttraining's rmse: 0.0116351\tvalid_1's rmse: 0.017879\n",
      "[3000]\ttraining's rmse: 0.0114112\tvalid_1's rmse: 0.0177456\n",
      "[3100]\ttraining's rmse: 0.0112274\tvalid_1's rmse: 0.0176441\n",
      "[3200]\ttraining's rmse: 0.0110423\tvalid_1's rmse: 0.0175505\n",
      "[3300]\ttraining's rmse: 0.0108812\tvalid_1's rmse: 0.0174693\n",
      "[3400]\ttraining's rmse: 0.0107152\tvalid_1's rmse: 0.0173894\n",
      "[3500]\ttraining's rmse: 0.0105602\tvalid_1's rmse: 0.0173218\n",
      "[3600]\ttraining's rmse: 0.0103978\tvalid_1's rmse: 0.0172411\n",
      "[3700]\ttraining's rmse: 0.0102617\tvalid_1's rmse: 0.0171904\n",
      "[3800]\ttraining's rmse: 0.0101036\tvalid_1's rmse: 0.0171292\n",
      "[3900]\ttraining's rmse: 0.00997248\tvalid_1's rmse: 0.0170821\n",
      "[4000]\ttraining's rmse: 0.00983745\tvalid_1's rmse: 0.0170243\n",
      "[4100]\ttraining's rmse: 0.00970477\tvalid_1's rmse: 0.0169655\n",
      "[4200]\ttraining's rmse: 0.00958198\tvalid_1's rmse: 0.016923\n",
      "[4300]\ttraining's rmse: 0.00945949\tvalid_1's rmse: 0.0168782\n",
      "[4400]\ttraining's rmse: 0.00934741\tvalid_1's rmse: 0.0168445\n",
      "[4500]\ttraining's rmse: 0.00922519\tvalid_1's rmse: 0.0167971\n",
      "[4600]\ttraining's rmse: 0.0091123\tvalid_1's rmse: 0.0167496\n",
      "[4700]\ttraining's rmse: 0.00898968\tvalid_1's rmse: 0.0166852\n",
      "[4800]\ttraining's rmse: 0.00887907\tvalid_1's rmse: 0.0166451\n",
      "[4900]\ttraining's rmse: 0.00877183\tvalid_1's rmse: 0.0166025\n",
      "[5000]\ttraining's rmse: 0.00866763\tvalid_1's rmse: 0.0165743\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00866763\tvalid_1's rmse: 0.0165743\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.143131\tvalid_1's rmse: 0.14542\n",
      "[200]\ttraining's rmse: 0.0923144\tvalid_1's rmse: 0.096355\n",
      "[300]\ttraining's rmse: 0.0660812\tvalid_1's rmse: 0.0709002\n",
      "[400]\ttraining's rmse: 0.0515462\tvalid_1's rmse: 0.0566834\n",
      "[500]\ttraining's rmse: 0.0431253\tvalid_1's rmse: 0.0486058\n",
      "[600]\ttraining's rmse: 0.0369883\tvalid_1's rmse: 0.042668\n",
      "[700]\ttraining's rmse: 0.0320901\tvalid_1's rmse: 0.0379049\n",
      "[800]\ttraining's rmse: 0.0284692\tvalid_1's rmse: 0.0344238\n",
      "[900]\ttraining's rmse: 0.0257232\tvalid_1's rmse: 0.0318129\n",
      "[1000]\ttraining's rmse: 0.0234645\tvalid_1's rmse: 0.0296594\n",
      "[1100]\ttraining's rmse: 0.0214883\tvalid_1's rmse: 0.0277359\n",
      "[1200]\ttraining's rmse: 0.0198824\tvalid_1's rmse: 0.0262101\n",
      "[1300]\ttraining's rmse: 0.018544\tvalid_1's rmse: 0.0249124\n",
      "[1400]\ttraining's rmse: 0.0174048\tvalid_1's rmse: 0.0237996\n",
      "[1500]\ttraining's rmse: 0.0165404\tvalid_1's rmse: 0.0230094\n",
      "[1600]\ttraining's rmse: 0.0157583\tvalid_1's rmse: 0.0222749\n",
      "[1700]\ttraining's rmse: 0.0150653\tvalid_1's rmse: 0.0216189\n",
      "[1800]\ttraining's rmse: 0.0144344\tvalid_1's rmse: 0.0210624\n",
      "[1900]\ttraining's rmse: 0.0139295\tvalid_1's rmse: 0.0206187\n",
      "[2000]\ttraining's rmse: 0.0134509\tvalid_1's rmse: 0.0202185\n",
      "[2100]\ttraining's rmse: 0.0130517\tvalid_1's rmse: 0.0198976\n",
      "[2200]\ttraining's rmse: 0.0126856\tvalid_1's rmse: 0.0196031\n",
      "[2300]\ttraining's rmse: 0.0123431\tvalid_1's rmse: 0.0193509\n",
      "[2400]\ttraining's rmse: 0.0120067\tvalid_1's rmse: 0.0190938\n",
      "[2500]\ttraining's rmse: 0.011712\tvalid_1's rmse: 0.0188838\n",
      "[2600]\ttraining's rmse: 0.0114413\tvalid_1's rmse: 0.0186944\n",
      "[2700]\ttraining's rmse: 0.0111826\tvalid_1's rmse: 0.0185162\n",
      "[2800]\ttraining's rmse: 0.0109463\tvalid_1's rmse: 0.0183764\n",
      "[2900]\ttraining's rmse: 0.0107298\tvalid_1's rmse: 0.0182466\n",
      "[3000]\ttraining's rmse: 0.0105067\tvalid_1's rmse: 0.018117\n",
      "[3100]\ttraining's rmse: 0.0103125\tvalid_1's rmse: 0.0180242\n",
      "[3200]\ttraining's rmse: 0.010142\tvalid_1's rmse: 0.0179353\n",
      "[3300]\ttraining's rmse: 0.00995198\tvalid_1's rmse: 0.0178321\n",
      "[3400]\ttraining's rmse: 0.00978808\tvalid_1's rmse: 0.0177519\n",
      "[3500]\ttraining's rmse: 0.00960849\tvalid_1's rmse: 0.0176615\n",
      "[3600]\ttraining's rmse: 0.00944858\tvalid_1's rmse: 0.0175845\n",
      "[3700]\ttraining's rmse: 0.00930092\tvalid_1's rmse: 0.0175305\n",
      "[3800]\ttraining's rmse: 0.00916727\tvalid_1's rmse: 0.0174701\n",
      "[3900]\ttraining's rmse: 0.00902562\tvalid_1's rmse: 0.017414\n",
      "[4000]\ttraining's rmse: 0.00890336\tvalid_1's rmse: 0.0173708\n",
      "[4100]\ttraining's rmse: 0.00878479\tvalid_1's rmse: 0.0173274\n",
      "[4200]\ttraining's rmse: 0.0086516\tvalid_1's rmse: 0.0172649\n",
      "[4300]\ttraining's rmse: 0.00852851\tvalid_1's rmse: 0.0172106\n",
      "[4400]\ttraining's rmse: 0.0084079\tvalid_1's rmse: 0.0171582\n",
      "[4500]\ttraining's rmse: 0.00827655\tvalid_1's rmse: 0.0171025\n",
      "[4600]\ttraining's rmse: 0.00815981\tvalid_1's rmse: 0.0170419\n",
      "[4700]\ttraining's rmse: 0.00805957\tvalid_1's rmse: 0.0170146\n",
      "[4800]\ttraining's rmse: 0.00795654\tvalid_1's rmse: 0.0169757\n",
      "[4900]\ttraining's rmse: 0.00785996\tvalid_1's rmse: 0.0169489\n",
      "[5000]\ttraining's rmse: 0.00776524\tvalid_1's rmse: 0.01692\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4994]\ttraining's rmse: 0.00776872\tvalid_1's rmse: 0.0169198\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.152618\tvalid_1's rmse: 0.153409\n",
      "[200]\ttraining's rmse: 0.0984262\tvalid_1's rmse: 0.1008\n",
      "[300]\ttraining's rmse: 0.0678995\tvalid_1's rmse: 0.0710735\n",
      "[400]\ttraining's rmse: 0.0509139\tvalid_1's rmse: 0.0545345\n",
      "[500]\ttraining's rmse: 0.0413144\tvalid_1's rmse: 0.0452397\n",
      "[600]\ttraining's rmse: 0.034747\tvalid_1's rmse: 0.0388275\n",
      "[700]\ttraining's rmse: 0.0298194\tvalid_1's rmse: 0.0339406\n",
      "[800]\ttraining's rmse: 0.0264593\tvalid_1's rmse: 0.0306299\n",
      "[900]\ttraining's rmse: 0.024\tvalid_1's rmse: 0.0282155\n",
      "[1000]\ttraining's rmse: 0.022069\tvalid_1's rmse: 0.0263253\n",
      "[1100]\ttraining's rmse: 0.0204399\tvalid_1's rmse: 0.0247131\n",
      "[1200]\ttraining's rmse: 0.0191547\tvalid_1's rmse: 0.0234176\n",
      "[1300]\ttraining's rmse: 0.0181256\tvalid_1's rmse: 0.0224128\n",
      "[1400]\ttraining's rmse: 0.0172358\tvalid_1's rmse: 0.0215035\n",
      "[1500]\ttraining's rmse: 0.0165333\tvalid_1's rmse: 0.0208674\n",
      "[1600]\ttraining's rmse: 0.0159426\tvalid_1's rmse: 0.020298\n",
      "[1700]\ttraining's rmse: 0.0153956\tvalid_1's rmse: 0.0198326\n",
      "[1800]\ttraining's rmse: 0.0148588\tvalid_1's rmse: 0.0193653\n",
      "[1900]\ttraining's rmse: 0.0144263\tvalid_1's rmse: 0.01903\n",
      "[2000]\ttraining's rmse: 0.0140209\tvalid_1's rmse: 0.0187043\n",
      "[2100]\ttraining's rmse: 0.0136731\tvalid_1's rmse: 0.0184603\n",
      "[2200]\ttraining's rmse: 0.0133424\tvalid_1's rmse: 0.0182343\n",
      "[2300]\ttraining's rmse: 0.0130389\tvalid_1's rmse: 0.0180335\n",
      "[2400]\ttraining's rmse: 0.0127218\tvalid_1's rmse: 0.0178006\n",
      "[2500]\ttraining's rmse: 0.0124569\tvalid_1's rmse: 0.0176424\n",
      "[2600]\ttraining's rmse: 0.0122207\tvalid_1's rmse: 0.017512\n",
      "[2700]\ttraining's rmse: 0.0119751\tvalid_1's rmse: 0.0173618\n",
      "[2800]\ttraining's rmse: 0.0117525\tvalid_1's rmse: 0.0172414\n",
      "[2900]\ttraining's rmse: 0.0115377\tvalid_1's rmse: 0.0171227\n",
      "[3000]\ttraining's rmse: 0.0113442\tvalid_1's rmse: 0.0170225\n",
      "[3100]\ttraining's rmse: 0.0111305\tvalid_1's rmse: 0.0169101\n",
      "[3200]\ttraining's rmse: 0.0109613\tvalid_1's rmse: 0.0168439\n",
      "[3300]\ttraining's rmse: 0.010803\tvalid_1's rmse: 0.0167708\n",
      "[3400]\ttraining's rmse: 0.0106492\tvalid_1's rmse: 0.0167174\n",
      "[3500]\ttraining's rmse: 0.0104747\tvalid_1's rmse: 0.0166304\n",
      "[3600]\ttraining's rmse: 0.0103063\tvalid_1's rmse: 0.0165564\n",
      "[3700]\ttraining's rmse: 0.0101322\tvalid_1's rmse: 0.0164698\n",
      "[3800]\ttraining's rmse: 0.0100039\tvalid_1's rmse: 0.0164385\n",
      "[3900]\ttraining's rmse: 0.00986385\tvalid_1's rmse: 0.0163859\n",
      "[4000]\ttraining's rmse: 0.00972639\tvalid_1's rmse: 0.016329\n",
      "[4100]\ttraining's rmse: 0.00960589\tvalid_1's rmse: 0.0162846\n",
      "[4200]\ttraining's rmse: 0.00947555\tvalid_1's rmse: 0.0162277\n",
      "[4300]\ttraining's rmse: 0.00932113\tvalid_1's rmse: 0.0161371\n",
      "[4400]\ttraining's rmse: 0.00920399\tvalid_1's rmse: 0.0160956\n",
      "[4500]\ttraining's rmse: 0.00907287\tvalid_1's rmse: 0.0160386\n",
      "[4600]\ttraining's rmse: 0.00896139\tvalid_1's rmse: 0.0160009\n",
      "[4700]\ttraining's rmse: 0.00884248\tvalid_1's rmse: 0.0159516\n",
      "[4800]\ttraining's rmse: 0.00873012\tvalid_1's rmse: 0.0159048\n",
      "[4900]\ttraining's rmse: 0.00862867\tvalid_1's rmse: 0.0158605\n",
      "[5000]\ttraining's rmse: 0.00851057\tvalid_1's rmse: 0.0158178\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00851057\tvalid_1's rmse: 0.0158178\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.142076\tvalid_1's rmse: 0.144876\n",
      "[200]\ttraining's rmse: 0.0920471\tvalid_1's rmse: 0.0963085\n",
      "[300]\ttraining's rmse: 0.0654508\tvalid_1's rmse: 0.0703185\n",
      "[400]\ttraining's rmse: 0.0504987\tvalid_1's rmse: 0.0555958\n",
      "[500]\ttraining's rmse: 0.0417818\tvalid_1's rmse: 0.047\n",
      "[600]\ttraining's rmse: 0.0355921\tvalid_1's rmse: 0.0409483\n",
      "[700]\ttraining's rmse: 0.0307724\tvalid_1's rmse: 0.0362218\n",
      "[800]\ttraining's rmse: 0.0273411\tvalid_1's rmse: 0.0329742\n",
      "[900]\ttraining's rmse: 0.0247872\tvalid_1's rmse: 0.0305807\n",
      "[1000]\ttraining's rmse: 0.0227465\tvalid_1's rmse: 0.0286165\n",
      "[1100]\ttraining's rmse: 0.0209436\tvalid_1's rmse: 0.0268875\n",
      "[1200]\ttraining's rmse: 0.0194913\tvalid_1's rmse: 0.0255125\n",
      "[1300]\ttraining's rmse: 0.0182965\tvalid_1's rmse: 0.0243734\n",
      "[1400]\ttraining's rmse: 0.0172755\tvalid_1's rmse: 0.0233928\n",
      "[1500]\ttraining's rmse: 0.0164743\tvalid_1's rmse: 0.0226487\n",
      "[1600]\ttraining's rmse: 0.0157748\tvalid_1's rmse: 0.0220081\n",
      "[1700]\ttraining's rmse: 0.0151562\tvalid_1's rmse: 0.0214363\n",
      "[1800]\ttraining's rmse: 0.0145674\tvalid_1's rmse: 0.0208833\n",
      "[1900]\ttraining's rmse: 0.0140822\tvalid_1's rmse: 0.0204814\n",
      "[2000]\ttraining's rmse: 0.0136362\tvalid_1's rmse: 0.020105\n",
      "[2100]\ttraining's rmse: 0.0132507\tvalid_1's rmse: 0.0197966\n",
      "[2200]\ttraining's rmse: 0.0129119\tvalid_1's rmse: 0.0195303\n",
      "[2300]\ttraining's rmse: 0.0125972\tvalid_1's rmse: 0.0193021\n",
      "[2400]\ttraining's rmse: 0.0122805\tvalid_1's rmse: 0.0190629\n",
      "[2500]\ttraining's rmse: 0.011989\tvalid_1's rmse: 0.018851\n",
      "[2600]\ttraining's rmse: 0.0117126\tvalid_1's rmse: 0.0186604\n",
      "[2700]\ttraining's rmse: 0.0114456\tvalid_1's rmse: 0.0184716\n",
      "[2800]\ttraining's rmse: 0.0111891\tvalid_1's rmse: 0.0183044\n",
      "[2900]\ttraining's rmse: 0.0109601\tvalid_1's rmse: 0.0181517\n",
      "[3000]\ttraining's rmse: 0.0107567\tvalid_1's rmse: 0.0180396\n",
      "[3100]\ttraining's rmse: 0.0105573\tvalid_1's rmse: 0.0179368\n",
      "[3200]\ttraining's rmse: 0.0103711\tvalid_1's rmse: 0.0178503\n",
      "[3300]\ttraining's rmse: 0.0101906\tvalid_1's rmse: 0.0177449\n",
      "[3400]\ttraining's rmse: 0.0100192\tvalid_1's rmse: 0.0176498\n",
      "[3500]\ttraining's rmse: 0.00984555\tvalid_1's rmse: 0.0175614\n",
      "[3600]\ttraining's rmse: 0.0096882\tvalid_1's rmse: 0.0174857\n",
      "[3700]\ttraining's rmse: 0.00952329\tvalid_1's rmse: 0.0173952\n",
      "[3800]\ttraining's rmse: 0.00937317\tvalid_1's rmse: 0.0173282\n",
      "[3900]\ttraining's rmse: 0.0092421\tvalid_1's rmse: 0.0172676\n",
      "[4000]\ttraining's rmse: 0.00911051\tvalid_1's rmse: 0.0172035\n",
      "[4100]\ttraining's rmse: 0.0089764\tvalid_1's rmse: 0.0171519\n",
      "[4200]\ttraining's rmse: 0.00884842\tvalid_1's rmse: 0.0171008\n",
      "[4300]\ttraining's rmse: 0.00873109\tvalid_1's rmse: 0.0170586\n",
      "[4400]\ttraining's rmse: 0.00860518\tvalid_1's rmse: 0.0170055\n",
      "[4500]\ttraining's rmse: 0.00846552\tvalid_1's rmse: 0.016943\n",
      "[4600]\ttraining's rmse: 0.00834837\tvalid_1's rmse: 0.0168879\n",
      "[4700]\ttraining's rmse: 0.00822782\tvalid_1's rmse: 0.0168362\n",
      "[4800]\ttraining's rmse: 0.00811053\tvalid_1's rmse: 0.016787\n",
      "[4900]\ttraining's rmse: 0.00800077\tvalid_1's rmse: 0.0167411\n",
      "[5000]\ttraining's rmse: 0.0078933\tvalid_1's rmse: 0.0166919\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.0078933\tvalid_1's rmse: 0.0166919\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.149447\tvalid_1's rmse: 0.15134\n",
      "[200]\ttraining's rmse: 0.0968474\tvalid_1's rmse: 0.10038\n",
      "[300]\ttraining's rmse: 0.0668393\tvalid_1's rmse: 0.0710435\n",
      "[400]\ttraining's rmse: 0.0522541\tvalid_1's rmse: 0.0566701\n",
      "[500]\ttraining's rmse: 0.0427053\tvalid_1's rmse: 0.0471275\n",
      "[600]\ttraining's rmse: 0.0362381\tvalid_1's rmse: 0.0406745\n",
      "[700]\ttraining's rmse: 0.0311554\tvalid_1's rmse: 0.0355965\n",
      "[800]\ttraining's rmse: 0.0275127\tvalid_1's rmse: 0.0319662\n",
      "[900]\ttraining's rmse: 0.0246038\tvalid_1's rmse: 0.0290455\n",
      "[1000]\ttraining's rmse: 0.0223383\tvalid_1's rmse: 0.02674\n",
      "[1100]\ttraining's rmse: 0.0206191\tvalid_1's rmse: 0.0250521\n",
      "[1200]\ttraining's rmse: 0.0192244\tvalid_1's rmse: 0.0236481\n",
      "[1300]\ttraining's rmse: 0.0180369\tvalid_1's rmse: 0.0224444\n",
      "[1400]\ttraining's rmse: 0.0170604\tvalid_1's rmse: 0.0214772\n",
      "[1500]\ttraining's rmse: 0.0162618\tvalid_1's rmse: 0.0206834\n",
      "[1600]\ttraining's rmse: 0.0155886\tvalid_1's rmse: 0.0200468\n",
      "[1700]\ttraining's rmse: 0.0149893\tvalid_1's rmse: 0.0194903\n",
      "[1800]\ttraining's rmse: 0.0144859\tvalid_1's rmse: 0.0190671\n",
      "[1900]\ttraining's rmse: 0.0140276\tvalid_1's rmse: 0.0186609\n",
      "[2000]\ttraining's rmse: 0.0136178\tvalid_1's rmse: 0.0183521\n",
      "[2100]\ttraining's rmse: 0.0132465\tvalid_1's rmse: 0.0180566\n",
      "[2200]\ttraining's rmse: 0.0129202\tvalid_1's rmse: 0.0178324\n",
      "[2300]\ttraining's rmse: 0.0126169\tvalid_1's rmse: 0.0176298\n",
      "[2400]\ttraining's rmse: 0.012311\tvalid_1's rmse: 0.0174239\n",
      "[2500]\ttraining's rmse: 0.0120489\tvalid_1's rmse: 0.0172543\n",
      "[2600]\ttraining's rmse: 0.0117827\tvalid_1's rmse: 0.0170824\n",
      "[2700]\ttraining's rmse: 0.0115201\tvalid_1's rmse: 0.0169155\n",
      "[2800]\ttraining's rmse: 0.0113034\tvalid_1's rmse: 0.0168057\n",
      "[2900]\ttraining's rmse: 0.0110927\tvalid_1's rmse: 0.016675\n",
      "[3000]\ttraining's rmse: 0.0109041\tvalid_1's rmse: 0.01657\n",
      "[3100]\ttraining's rmse: 0.0107171\tvalid_1's rmse: 0.0164776\n",
      "[3200]\ttraining's rmse: 0.0105237\tvalid_1's rmse: 0.0163815\n",
      "[3300]\ttraining's rmse: 0.0103438\tvalid_1's rmse: 0.0162939\n",
      "[3400]\ttraining's rmse: 0.0101701\tvalid_1's rmse: 0.0162119\n",
      "[3500]\ttraining's rmse: 0.010021\tvalid_1's rmse: 0.0161546\n",
      "[3600]\ttraining's rmse: 0.00986048\tvalid_1's rmse: 0.0160768\n",
      "[3700]\ttraining's rmse: 0.00971716\tvalid_1's rmse: 0.0160157\n",
      "[3800]\ttraining's rmse: 0.00954887\tvalid_1's rmse: 0.0159305\n",
      "[3900]\ttraining's rmse: 0.00941675\tvalid_1's rmse: 0.0158751\n",
      "[4000]\ttraining's rmse: 0.00928687\tvalid_1's rmse: 0.0158205\n",
      "[4100]\ttraining's rmse: 0.00915695\tvalid_1's rmse: 0.0157631\n",
      "[4200]\ttraining's rmse: 0.00902554\tvalid_1's rmse: 0.0157056\n",
      "[4300]\ttraining's rmse: 0.00891099\tvalid_1's rmse: 0.0156658\n",
      "[4400]\ttraining's rmse: 0.0087827\tvalid_1's rmse: 0.0156082\n",
      "[4500]\ttraining's rmse: 0.00866186\tvalid_1's rmse: 0.0155612\n",
      "[4600]\ttraining's rmse: 0.00855115\tvalid_1's rmse: 0.0155178\n",
      "[4700]\ttraining's rmse: 0.00844347\tvalid_1's rmse: 0.0154813\n",
      "[4800]\ttraining's rmse: 0.00833961\tvalid_1's rmse: 0.0154443\n",
      "[4900]\ttraining's rmse: 0.00823785\tvalid_1's rmse: 0.0154039\n",
      "[5000]\ttraining's rmse: 0.00813493\tvalid_1's rmse: 0.015363\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00813493\tvalid_1's rmse: 0.015363\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.136865\tvalid_1's rmse: 0.140164\n",
      "[200]\ttraining's rmse: 0.0836205\tvalid_1's rmse: 0.0881826\n",
      "[300]\ttraining's rmse: 0.0582963\tvalid_1's rmse: 0.0632773\n",
      "[400]\ttraining's rmse: 0.0451552\tvalid_1's rmse: 0.050147\n",
      "[500]\ttraining's rmse: 0.036721\tvalid_1's rmse: 0.0416761\n",
      "[600]\ttraining's rmse: 0.0312702\tvalid_1's rmse: 0.0361283\n",
      "[700]\ttraining's rmse: 0.0276284\tvalid_1's rmse: 0.0325475\n",
      "[800]\ttraining's rmse: 0.0248613\tvalid_1's rmse: 0.0298833\n",
      "[900]\ttraining's rmse: 0.022702\tvalid_1's rmse: 0.027718\n",
      "[1000]\ttraining's rmse: 0.0208893\tvalid_1's rmse: 0.0259303\n",
      "[1100]\ttraining's rmse: 0.0193642\tvalid_1's rmse: 0.0243885\n",
      "[1200]\ttraining's rmse: 0.0182052\tvalid_1's rmse: 0.0232469\n",
      "[1300]\ttraining's rmse: 0.0172692\tvalid_1's rmse: 0.0223293\n",
      "[1400]\ttraining's rmse: 0.0164613\tvalid_1's rmse: 0.0215885\n",
      "[1500]\ttraining's rmse: 0.015768\tvalid_1's rmse: 0.0209371\n",
      "[1600]\ttraining's rmse: 0.015173\tvalid_1's rmse: 0.0204179\n",
      "[1700]\ttraining's rmse: 0.0146817\tvalid_1's rmse: 0.0200188\n",
      "[1800]\ttraining's rmse: 0.0141931\tvalid_1's rmse: 0.0196319\n",
      "[1900]\ttraining's rmse: 0.0137931\tvalid_1's rmse: 0.0192954\n",
      "[2000]\ttraining's rmse: 0.0133967\tvalid_1's rmse: 0.0189889\n",
      "[2100]\ttraining's rmse: 0.0130521\tvalid_1's rmse: 0.0187472\n",
      "[2200]\ttraining's rmse: 0.0127345\tvalid_1's rmse: 0.0185476\n",
      "[2300]\ttraining's rmse: 0.0124595\tvalid_1's rmse: 0.0183908\n",
      "[2400]\ttraining's rmse: 0.0121884\tvalid_1's rmse: 0.0181991\n",
      "[2500]\ttraining's rmse: 0.0119411\tvalid_1's rmse: 0.0180382\n",
      "[2600]\ttraining's rmse: 0.0116831\tvalid_1's rmse: 0.0178577\n",
      "[2700]\ttraining's rmse: 0.0114233\tvalid_1's rmse: 0.017713\n",
      "[2800]\ttraining's rmse: 0.0112163\tvalid_1's rmse: 0.0176202\n",
      "[2900]\ttraining's rmse: 0.0109844\tvalid_1's rmse: 0.0174512\n",
      "[3000]\ttraining's rmse: 0.0107965\tvalid_1's rmse: 0.0173285\n",
      "[3100]\ttraining's rmse: 0.0106086\tvalid_1's rmse: 0.0172142\n",
      "[3200]\ttraining's rmse: 0.0104214\tvalid_1's rmse: 0.0171202\n",
      "[3300]\ttraining's rmse: 0.0102302\tvalid_1's rmse: 0.0170125\n",
      "[3400]\ttraining's rmse: 0.0100726\tvalid_1's rmse: 0.0169431\n",
      "[3500]\ttraining's rmse: 0.00991442\tvalid_1's rmse: 0.0168735\n",
      "[3600]\ttraining's rmse: 0.00977442\tvalid_1's rmse: 0.0168226\n",
      "[3700]\ttraining's rmse: 0.00961401\tvalid_1's rmse: 0.0167498\n",
      "[3800]\ttraining's rmse: 0.00946737\tvalid_1's rmse: 0.01669\n",
      "[3900]\ttraining's rmse: 0.00932044\tvalid_1's rmse: 0.0166198\n",
      "[4000]\ttraining's rmse: 0.00917454\tvalid_1's rmse: 0.0165397\n",
      "[4100]\ttraining's rmse: 0.00905301\tvalid_1's rmse: 0.0164847\n",
      "[4200]\ttraining's rmse: 0.00893372\tvalid_1's rmse: 0.0164311\n",
      "[4300]\ttraining's rmse: 0.00882284\tvalid_1's rmse: 0.0163911\n",
      "[4400]\ttraining's rmse: 0.00869314\tvalid_1's rmse: 0.0163336\n",
      "[4500]\ttraining's rmse: 0.00858334\tvalid_1's rmse: 0.0162957\n",
      "[4600]\ttraining's rmse: 0.00845008\tvalid_1's rmse: 0.0162277\n",
      "[4700]\ttraining's rmse: 0.00834155\tvalid_1's rmse: 0.016194\n",
      "[4800]\ttraining's rmse: 0.00823843\tvalid_1's rmse: 0.0161514\n",
      "[4900]\ttraining's rmse: 0.00813538\tvalid_1's rmse: 0.0161099\n",
      "[5000]\ttraining's rmse: 0.0080318\tvalid_1's rmse: 0.0160754\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\ttraining's rmse: 0.00803234\tvalid_1's rmse: 0.0160753\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.145416\tvalid_1's rmse: 0.148119\n",
      "[200]\ttraining's rmse: 0.0946951\tvalid_1's rmse: 0.0992262\n",
      "[300]\ttraining's rmse: 0.0667392\tvalid_1's rmse: 0.0715073\n",
      "[400]\ttraining's rmse: 0.0509166\tvalid_1's rmse: 0.0557292\n",
      "[500]\ttraining's rmse: 0.0417366\tvalid_1's rmse: 0.046597\n",
      "[600]\ttraining's rmse: 0.0352611\tvalid_1's rmse: 0.0400414\n",
      "[700]\ttraining's rmse: 0.0302967\tvalid_1's rmse: 0.0349501\n",
      "[800]\ttraining's rmse: 0.0268157\tvalid_1's rmse: 0.0314004\n",
      "[900]\ttraining's rmse: 0.0242527\tvalid_1's rmse: 0.0288308\n",
      "[1000]\ttraining's rmse: 0.0221777\tvalid_1's rmse: 0.0267459\n",
      "[1100]\ttraining's rmse: 0.0203938\tvalid_1's rmse: 0.024967\n",
      "[1200]\ttraining's rmse: 0.0189601\tvalid_1's rmse: 0.0235197\n",
      "[1300]\ttraining's rmse: 0.0178195\tvalid_1's rmse: 0.0223917\n",
      "[1400]\ttraining's rmse: 0.0168303\tvalid_1's rmse: 0.0213849\n",
      "[1500]\ttraining's rmse: 0.01608\tvalid_1's rmse: 0.0206522\n",
      "[1600]\ttraining's rmse: 0.0154208\tvalid_1's rmse: 0.0199974\n",
      "[1700]\ttraining's rmse: 0.014819\tvalid_1's rmse: 0.0194552\n",
      "[1800]\ttraining's rmse: 0.0142472\tvalid_1's rmse: 0.0189168\n",
      "[1900]\ttraining's rmse: 0.0138096\tvalid_1's rmse: 0.0185278\n",
      "[2000]\ttraining's rmse: 0.0133817\tvalid_1's rmse: 0.0181484\n",
      "[2100]\ttraining's rmse: 0.0130112\tvalid_1's rmse: 0.017856\n",
      "[2200]\ttraining's rmse: 0.0126852\tvalid_1's rmse: 0.0175981\n",
      "[2300]\ttraining's rmse: 0.01239\tvalid_1's rmse: 0.017381\n",
      "[2400]\ttraining's rmse: 0.0120636\tvalid_1's rmse: 0.0171124\n",
      "[2500]\ttraining's rmse: 0.0118019\tvalid_1's rmse: 0.0169292\n",
      "[2600]\ttraining's rmse: 0.0115206\tvalid_1's rmse: 0.0167341\n",
      "[2700]\ttraining's rmse: 0.0112692\tvalid_1's rmse: 0.0165643\n",
      "[2800]\ttraining's rmse: 0.0110621\tvalid_1's rmse: 0.016439\n",
      "[2900]\ttraining's rmse: 0.0108449\tvalid_1's rmse: 0.0163034\n",
      "[3000]\ttraining's rmse: 0.0106543\tvalid_1's rmse: 0.0161983\n",
      "[3100]\ttraining's rmse: 0.010475\tvalid_1's rmse: 0.0160857\n",
      "[3200]\ttraining's rmse: 0.0102986\tvalid_1's rmse: 0.0159879\n",
      "[3300]\ttraining's rmse: 0.0101192\tvalid_1's rmse: 0.0158841\n",
      "[3400]\ttraining's rmse: 0.00996321\tvalid_1's rmse: 0.0158115\n",
      "[3500]\ttraining's rmse: 0.00980216\tvalid_1's rmse: 0.0157203\n",
      "[3600]\ttraining's rmse: 0.00965207\tvalid_1's rmse: 0.0156443\n",
      "[3700]\ttraining's rmse: 0.00951651\tvalid_1's rmse: 0.0155875\n",
      "[3800]\ttraining's rmse: 0.0093689\tvalid_1's rmse: 0.015508\n",
      "[3900]\ttraining's rmse: 0.00923304\tvalid_1's rmse: 0.0154484\n",
      "[4000]\ttraining's rmse: 0.0090956\tvalid_1's rmse: 0.0153798\n",
      "[4100]\ttraining's rmse: 0.00899125\tvalid_1's rmse: 0.0153502\n",
      "[4200]\ttraining's rmse: 0.00886463\tvalid_1's rmse: 0.0152933\n",
      "[4300]\ttraining's rmse: 0.00874557\tvalid_1's rmse: 0.0152394\n",
      "[4400]\ttraining's rmse: 0.00863317\tvalid_1's rmse: 0.0152033\n",
      "[4500]\ttraining's rmse: 0.00852421\tvalid_1's rmse: 0.0151614\n",
      "[4600]\ttraining's rmse: 0.00841218\tvalid_1's rmse: 0.0151088\n",
      "[4700]\ttraining's rmse: 0.00829808\tvalid_1's rmse: 0.0150608\n",
      "[4800]\ttraining's rmse: 0.00818578\tvalid_1's rmse: 0.0150091\n",
      "[4900]\ttraining's rmse: 0.0080787\tvalid_1's rmse: 0.0149564\n",
      "[5000]\ttraining's rmse: 0.00797505\tvalid_1's rmse: 0.0149198\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\ttraining's rmse: 0.0079761\tvalid_1's rmse: 0.0149196\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.147095\tvalid_1's rmse: 0.150342\n",
      "[200]\ttraining's rmse: 0.0990442\tvalid_1's rmse: 0.103045\n",
      "[300]\ttraining's rmse: 0.073182\tvalid_1's rmse: 0.0777641\n",
      "[400]\ttraining's rmse: 0.0552038\tvalid_1's rmse: 0.0600662\n",
      "[500]\ttraining's rmse: 0.0436193\tvalid_1's rmse: 0.0485818\n",
      "[600]\ttraining's rmse: 0.0358734\tvalid_1's rmse: 0.0407837\n",
      "[700]\ttraining's rmse: 0.0306758\tvalid_1's rmse: 0.0355112\n",
      "[800]\ttraining's rmse: 0.0269985\tvalid_1's rmse: 0.0317712\n",
      "[900]\ttraining's rmse: 0.0239781\tvalid_1's rmse: 0.0286164\n",
      "[1000]\ttraining's rmse: 0.0219366\tvalid_1's rmse: 0.0265024\n",
      "[1100]\ttraining's rmse: 0.0203507\tvalid_1's rmse: 0.024859\n",
      "[1200]\ttraining's rmse: 0.0190376\tvalid_1's rmse: 0.0235277\n",
      "[1300]\ttraining's rmse: 0.0180012\tvalid_1's rmse: 0.0225016\n",
      "[1400]\ttraining's rmse: 0.0171075\tvalid_1's rmse: 0.0215977\n",
      "[1500]\ttraining's rmse: 0.0163392\tvalid_1's rmse: 0.0208111\n",
      "[1600]\ttraining's rmse: 0.0156514\tvalid_1's rmse: 0.02013\n",
      "[1700]\ttraining's rmse: 0.0150639\tvalid_1's rmse: 0.0195792\n",
      "[1800]\ttraining's rmse: 0.0145495\tvalid_1's rmse: 0.0191114\n",
      "[1900]\ttraining's rmse: 0.0141107\tvalid_1's rmse: 0.0187463\n",
      "[2000]\ttraining's rmse: 0.0137311\tvalid_1's rmse: 0.0184496\n",
      "[2100]\ttraining's rmse: 0.0133509\tvalid_1's rmse: 0.0181679\n",
      "[2200]\ttraining's rmse: 0.013008\tvalid_1's rmse: 0.0179054\n",
      "[2300]\ttraining's rmse: 0.0126814\tvalid_1's rmse: 0.0176542\n",
      "[2400]\ttraining's rmse: 0.0124169\tvalid_1's rmse: 0.0174788\n",
      "[2500]\ttraining's rmse: 0.0121424\tvalid_1's rmse: 0.017288\n",
      "[2600]\ttraining's rmse: 0.0119013\tvalid_1's rmse: 0.0171372\n",
      "[2700]\ttraining's rmse: 0.0116491\tvalid_1's rmse: 0.0169683\n",
      "[2800]\ttraining's rmse: 0.0114394\tvalid_1's rmse: 0.0168466\n",
      "[2900]\ttraining's rmse: 0.0112324\tvalid_1's rmse: 0.0167177\n",
      "[3000]\ttraining's rmse: 0.0110541\tvalid_1's rmse: 0.0166247\n",
      "[3100]\ttraining's rmse: 0.0108701\tvalid_1's rmse: 0.016537\n",
      "[3200]\ttraining's rmse: 0.0106958\tvalid_1's rmse: 0.0164565\n",
      "[3300]\ttraining's rmse: 0.0105182\tvalid_1's rmse: 0.0163489\n",
      "[3400]\ttraining's rmse: 0.0103481\tvalid_1's rmse: 0.0162581\n",
      "[3500]\ttraining's rmse: 0.0101763\tvalid_1's rmse: 0.0161695\n",
      "[3600]\ttraining's rmse: 0.0100199\tvalid_1's rmse: 0.0160796\n",
      "[3700]\ttraining's rmse: 0.00987804\tvalid_1's rmse: 0.0160095\n",
      "[3800]\ttraining's rmse: 0.00972389\tvalid_1's rmse: 0.0159193\n",
      "[3900]\ttraining's rmse: 0.00958777\tvalid_1's rmse: 0.0158418\n",
      "[4000]\ttraining's rmse: 0.00945173\tvalid_1's rmse: 0.0157679\n",
      "[4100]\ttraining's rmse: 0.00932441\tvalid_1's rmse: 0.015704\n",
      "[4200]\ttraining's rmse: 0.00920459\tvalid_1's rmse: 0.0156577\n",
      "[4300]\ttraining's rmse: 0.0090798\tvalid_1's rmse: 0.0156056\n",
      "[4400]\ttraining's rmse: 0.00896673\tvalid_1's rmse: 0.0155638\n",
      "[4500]\ttraining's rmse: 0.00885059\tvalid_1's rmse: 0.0155102\n",
      "[4600]\ttraining's rmse: 0.00874023\tvalid_1's rmse: 0.0154543\n",
      "[4700]\ttraining's rmse: 0.00863098\tvalid_1's rmse: 0.0153994\n",
      "[4800]\ttraining's rmse: 0.00852548\tvalid_1's rmse: 0.0153478\n",
      "[4900]\ttraining's rmse: 0.00843089\tvalid_1's rmse: 0.0153026\n",
      "[5000]\ttraining's rmse: 0.00834349\tvalid_1's rmse: 0.0152743\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\ttraining's rmse: 0.00834448\tvalid_1's rmse: 0.0152741\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.14736\tvalid_1's rmse: 0.149517\n",
      "[200]\ttraining's rmse: 0.0955943\tvalid_1's rmse: 0.0988358\n",
      "[300]\ttraining's rmse: 0.0663534\tvalid_1's rmse: 0.0703139\n",
      "[400]\ttraining's rmse: 0.0519286\tvalid_1's rmse: 0.056272\n",
      "[500]\ttraining's rmse: 0.0423288\tvalid_1's rmse: 0.0468696\n",
      "[600]\ttraining's rmse: 0.0358553\tvalid_1's rmse: 0.0404944\n",
      "[700]\ttraining's rmse: 0.0308888\tvalid_1's rmse: 0.0355529\n",
      "[800]\ttraining's rmse: 0.0272604\tvalid_1's rmse: 0.0319397\n",
      "[900]\ttraining's rmse: 0.0244321\tvalid_1's rmse: 0.02915\n",
      "[1000]\ttraining's rmse: 0.0222014\tvalid_1's rmse: 0.0269322\n",
      "[1100]\ttraining's rmse: 0.0205132\tvalid_1's rmse: 0.0252364\n",
      "[1200]\ttraining's rmse: 0.0191564\tvalid_1's rmse: 0.023858\n",
      "[1300]\ttraining's rmse: 0.0179757\tvalid_1's rmse: 0.0226588\n",
      "[1400]\ttraining's rmse: 0.0169775\tvalid_1's rmse: 0.0216449\n",
      "[1500]\ttraining's rmse: 0.0161465\tvalid_1's rmse: 0.0207906\n",
      "[1600]\ttraining's rmse: 0.0154616\tvalid_1's rmse: 0.020118\n",
      "[1700]\ttraining's rmse: 0.0148894\tvalid_1's rmse: 0.0195583\n",
      "[1800]\ttraining's rmse: 0.0144026\tvalid_1's rmse: 0.0190915\n",
      "[1900]\ttraining's rmse: 0.013936\tvalid_1's rmse: 0.0186604\n",
      "[2000]\ttraining's rmse: 0.0135179\tvalid_1's rmse: 0.0182985\n",
      "[2100]\ttraining's rmse: 0.0131678\tvalid_1's rmse: 0.0180099\n",
      "[2200]\ttraining's rmse: 0.0128519\tvalid_1's rmse: 0.0177855\n",
      "[2300]\ttraining's rmse: 0.0125792\tvalid_1's rmse: 0.0176038\n",
      "[2400]\ttraining's rmse: 0.0123135\tvalid_1's rmse: 0.0174231\n",
      "[2500]\ttraining's rmse: 0.0120397\tvalid_1's rmse: 0.0172505\n",
      "[2600]\ttraining's rmse: 0.0117862\tvalid_1's rmse: 0.0170746\n",
      "[2700]\ttraining's rmse: 0.0115705\tvalid_1's rmse: 0.0169495\n",
      "[2800]\ttraining's rmse: 0.0113705\tvalid_1's rmse: 0.0168376\n",
      "[2900]\ttraining's rmse: 0.011148\tvalid_1's rmse: 0.0166999\n",
      "[3000]\ttraining's rmse: 0.010946\tvalid_1's rmse: 0.0165864\n",
      "[3100]\ttraining's rmse: 0.0107763\tvalid_1's rmse: 0.0165\n",
      "[3200]\ttraining's rmse: 0.0105926\tvalid_1's rmse: 0.0163897\n",
      "[3300]\ttraining's rmse: 0.0104193\tvalid_1's rmse: 0.0163008\n",
      "[3400]\ttraining's rmse: 0.0102425\tvalid_1's rmse: 0.0162092\n",
      "[3500]\ttraining's rmse: 0.0100979\tvalid_1's rmse: 0.0161435\n",
      "[3600]\ttraining's rmse: 0.00995732\tvalid_1's rmse: 0.0160689\n",
      "[3700]\ttraining's rmse: 0.00982357\tvalid_1's rmse: 0.0160248\n",
      "[3800]\ttraining's rmse: 0.00968113\tvalid_1's rmse: 0.0159567\n",
      "[3900]\ttraining's rmse: 0.0095452\tvalid_1's rmse: 0.0158996\n",
      "[4000]\ttraining's rmse: 0.00942442\tvalid_1's rmse: 0.0158487\n",
      "[4100]\ttraining's rmse: 0.00930609\tvalid_1's rmse: 0.0158025\n",
      "[4200]\ttraining's rmse: 0.00917686\tvalid_1's rmse: 0.0157445\n",
      "[4300]\ttraining's rmse: 0.00906191\tvalid_1's rmse: 0.015706\n",
      "[4400]\ttraining's rmse: 0.00894721\tvalid_1's rmse: 0.0156591\n",
      "[4500]\ttraining's rmse: 0.00880946\tvalid_1's rmse: 0.0155967\n",
      "[4600]\ttraining's rmse: 0.00869717\tvalid_1's rmse: 0.0155549\n",
      "[4700]\ttraining's rmse: 0.00859878\tvalid_1's rmse: 0.0155184\n",
      "[4800]\ttraining's rmse: 0.0084872\tvalid_1's rmse: 0.0154702\n",
      "[4900]\ttraining's rmse: 0.00838059\tvalid_1's rmse: 0.0154329\n",
      "[5000]\ttraining's rmse: 0.00827265\tvalid_1's rmse: 0.0153947\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00827265\tvalid_1's rmse: 0.0153947\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.140358\tvalid_1's rmse: 0.142127\n",
      "[200]\ttraining's rmse: 0.0894139\tvalid_1's rmse: 0.0927289\n",
      "[300]\ttraining's rmse: 0.0622654\tvalid_1's rmse: 0.0663297\n",
      "[400]\ttraining's rmse: 0.0493629\tvalid_1's rmse: 0.0538094\n",
      "[500]\ttraining's rmse: 0.0409486\tvalid_1's rmse: 0.0455414\n",
      "[600]\ttraining's rmse: 0.0351414\tvalid_1's rmse: 0.0398662\n",
      "[700]\ttraining's rmse: 0.030577\tvalid_1's rmse: 0.035332\n",
      "[800]\ttraining's rmse: 0.0272432\tvalid_1's rmse: 0.0321058\n",
      "[900]\ttraining's rmse: 0.0245972\tvalid_1's rmse: 0.0295269\n",
      "[1000]\ttraining's rmse: 0.0225042\tvalid_1's rmse: 0.0275052\n",
      "[1100]\ttraining's rmse: 0.020867\tvalid_1's rmse: 0.0258938\n",
      "[1200]\ttraining's rmse: 0.019514\tvalid_1's rmse: 0.0245649\n",
      "[1300]\ttraining's rmse: 0.0183298\tvalid_1's rmse: 0.023425\n",
      "[1400]\ttraining's rmse: 0.0173126\tvalid_1's rmse: 0.0224266\n",
      "[1500]\ttraining's rmse: 0.0164848\tvalid_1's rmse: 0.0216269\n",
      "[1600]\ttraining's rmse: 0.015779\tvalid_1's rmse: 0.0209601\n",
      "[1700]\ttraining's rmse: 0.0151666\tvalid_1's rmse: 0.0204052\n",
      "[1800]\ttraining's rmse: 0.0146313\tvalid_1's rmse: 0.0199341\n",
      "[1900]\ttraining's rmse: 0.0141569\tvalid_1's rmse: 0.0195175\n",
      "[2000]\ttraining's rmse: 0.0137262\tvalid_1's rmse: 0.0191482\n",
      "[2100]\ttraining's rmse: 0.0133419\tvalid_1's rmse: 0.0188374\n",
      "[2200]\ttraining's rmse: 0.0130027\tvalid_1's rmse: 0.0185956\n",
      "[2300]\ttraining's rmse: 0.0126967\tvalid_1's rmse: 0.0183804\n",
      "[2400]\ttraining's rmse: 0.0124002\tvalid_1's rmse: 0.0181847\n",
      "[2500]\ttraining's rmse: 0.0121189\tvalid_1's rmse: 0.017978\n",
      "[2600]\ttraining's rmse: 0.011847\tvalid_1's rmse: 0.017796\n",
      "[2700]\ttraining's rmse: 0.0115964\tvalid_1's rmse: 0.017622\n",
      "[2800]\ttraining's rmse: 0.0113612\tvalid_1's rmse: 0.0174993\n",
      "[2900]\ttraining's rmse: 0.0111611\tvalid_1's rmse: 0.0173708\n",
      "[3000]\ttraining's rmse: 0.0109534\tvalid_1's rmse: 0.0172505\n",
      "[3100]\ttraining's rmse: 0.0107608\tvalid_1's rmse: 0.0171706\n",
      "[3200]\ttraining's rmse: 0.0105645\tvalid_1's rmse: 0.0170562\n",
      "[3300]\ttraining's rmse: 0.0103793\tvalid_1's rmse: 0.0169581\n",
      "[3400]\ttraining's rmse: 0.0102066\tvalid_1's rmse: 0.0168693\n",
      "[3500]\ttraining's rmse: 0.0100396\tvalid_1's rmse: 0.0168016\n",
      "[3600]\ttraining's rmse: 0.00986391\tvalid_1's rmse: 0.0167068\n",
      "[3700]\ttraining's rmse: 0.00971436\tvalid_1's rmse: 0.0166381\n",
      "[3800]\ttraining's rmse: 0.00957787\tvalid_1's rmse: 0.0165768\n",
      "[3900]\ttraining's rmse: 0.00944183\tvalid_1's rmse: 0.0165272\n",
      "[4000]\ttraining's rmse: 0.00930915\tvalid_1's rmse: 0.0164813\n",
      "[4100]\ttraining's rmse: 0.00917729\tvalid_1's rmse: 0.0164306\n",
      "[4200]\ttraining's rmse: 0.0090422\tvalid_1's rmse: 0.0163611\n",
      "[4300]\ttraining's rmse: 0.00890526\tvalid_1's rmse: 0.0163107\n",
      "[4400]\ttraining's rmse: 0.0087853\tvalid_1's rmse: 0.0162727\n",
      "[4500]\ttraining's rmse: 0.00867108\tvalid_1's rmse: 0.0162276\n",
      "[4600]\ttraining's rmse: 0.00855635\tvalid_1's rmse: 0.0161865\n",
      "[4700]\ttraining's rmse: 0.00844409\tvalid_1's rmse: 0.0161531\n",
      "[4800]\ttraining's rmse: 0.00834176\tvalid_1's rmse: 0.0161207\n",
      "[4900]\ttraining's rmse: 0.0082387\tvalid_1's rmse: 0.0160883\n",
      "[5000]\ttraining's rmse: 0.0081303\tvalid_1's rmse: 0.0160673\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.0081303\tvalid_1's rmse: 0.0160673\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.152131\tvalid_1's rmse: 0.155729\n",
      "[200]\ttraining's rmse: 0.099987\tvalid_1's rmse: 0.105043\n",
      "[300]\ttraining's rmse: 0.0697884\tvalid_1's rmse: 0.0753105\n",
      "[400]\ttraining's rmse: 0.0522904\tvalid_1's rmse: 0.0578724\n",
      "[500]\ttraining's rmse: 0.0421258\tvalid_1's rmse: 0.0476562\n",
      "[600]\ttraining's rmse: 0.0351603\tvalid_1's rmse: 0.0406568\n",
      "[700]\ttraining's rmse: 0.029954\tvalid_1's rmse: 0.0352957\n",
      "[800]\ttraining's rmse: 0.0264184\tvalid_1's rmse: 0.0316892\n",
      "[900]\ttraining's rmse: 0.0238384\tvalid_1's rmse: 0.0290695\n",
      "[1000]\ttraining's rmse: 0.0218333\tvalid_1's rmse: 0.0270323\n",
      "[1100]\ttraining's rmse: 0.0201376\tvalid_1's rmse: 0.0252986\n",
      "[1200]\ttraining's rmse: 0.0188216\tvalid_1's rmse: 0.0239346\n",
      "[1300]\ttraining's rmse: 0.0177644\tvalid_1's rmse: 0.0228643\n",
      "[1400]\ttraining's rmse: 0.0169081\tvalid_1's rmse: 0.021956\n",
      "[1500]\ttraining's rmse: 0.0162496\tvalid_1's rmse: 0.0213209\n",
      "[1600]\ttraining's rmse: 0.0156298\tvalid_1's rmse: 0.0207285\n",
      "[1700]\ttraining's rmse: 0.0151187\tvalid_1's rmse: 0.0202522\n",
      "[1800]\ttraining's rmse: 0.0146462\tvalid_1's rmse: 0.0198075\n",
      "[1900]\ttraining's rmse: 0.0142575\tvalid_1's rmse: 0.0194921\n",
      "[2000]\ttraining's rmse: 0.013887\tvalid_1's rmse: 0.019198\n",
      "[2100]\ttraining's rmse: 0.013561\tvalid_1's rmse: 0.0189841\n",
      "[2200]\ttraining's rmse: 0.0132606\tvalid_1's rmse: 0.0187686\n",
      "[2300]\ttraining's rmse: 0.0129844\tvalid_1's rmse: 0.0185837\n",
      "[2400]\ttraining's rmse: 0.0126987\tvalid_1's rmse: 0.0183746\n",
      "[2500]\ttraining's rmse: 0.0124284\tvalid_1's rmse: 0.0181897\n",
      "[2600]\ttraining's rmse: 0.0121826\tvalid_1's rmse: 0.0180224\n",
      "[2700]\ttraining's rmse: 0.0119359\tvalid_1's rmse: 0.0178687\n",
      "[2800]\ttraining's rmse: 0.0117351\tvalid_1's rmse: 0.0177504\n",
      "[2900]\ttraining's rmse: 0.0115257\tvalid_1's rmse: 0.0176323\n",
      "[3000]\ttraining's rmse: 0.0113655\tvalid_1's rmse: 0.0175618\n",
      "[3100]\ttraining's rmse: 0.0111773\tvalid_1's rmse: 0.0174562\n",
      "[3200]\ttraining's rmse: 0.0110091\tvalid_1's rmse: 0.0173821\n",
      "[3300]\ttraining's rmse: 0.0108075\tvalid_1's rmse: 0.0172692\n",
      "[3400]\ttraining's rmse: 0.010645\tvalid_1's rmse: 0.017192\n",
      "[3500]\ttraining's rmse: 0.0104799\tvalid_1's rmse: 0.0171081\n",
      "[3600]\ttraining's rmse: 0.0103235\tvalid_1's rmse: 0.0170202\n",
      "[3700]\ttraining's rmse: 0.010187\tvalid_1's rmse: 0.0169628\n",
      "[3800]\ttraining's rmse: 0.0100326\tvalid_1's rmse: 0.0168883\n",
      "[3900]\ttraining's rmse: 0.00989412\tvalid_1's rmse: 0.0168354\n",
      "[4000]\ttraining's rmse: 0.00974318\tvalid_1's rmse: 0.0167622\n",
      "[4100]\ttraining's rmse: 0.00961733\tvalid_1's rmse: 0.0167051\n",
      "[4200]\ttraining's rmse: 0.0094854\tvalid_1's rmse: 0.0166443\n",
      "[4300]\ttraining's rmse: 0.00936911\tvalid_1's rmse: 0.0165991\n",
      "[4400]\ttraining's rmse: 0.00925036\tvalid_1's rmse: 0.0165522\n",
      "[4500]\ttraining's rmse: 0.00913915\tvalid_1's rmse: 0.0165058\n",
      "[4600]\ttraining's rmse: 0.00901839\tvalid_1's rmse: 0.0164456\n",
      "[4700]\ttraining's rmse: 0.00891455\tvalid_1's rmse: 0.0164131\n",
      "[4800]\ttraining's rmse: 0.00881279\tvalid_1's rmse: 0.0163807\n",
      "[4900]\ttraining's rmse: 0.00869734\tvalid_1's rmse: 0.0163403\n",
      "[5000]\ttraining's rmse: 0.00859939\tvalid_1's rmse: 0.0163044\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00859939\tvalid_1's rmse: 0.0163044\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.145855\tvalid_1's rmse: 0.146117\n",
      "[200]\ttraining's rmse: 0.0942286\tvalid_1's rmse: 0.0964334\n",
      "[300]\ttraining's rmse: 0.0668795\tvalid_1's rmse: 0.0700953\n",
      "[400]\ttraining's rmse: 0.0516967\tvalid_1's rmse: 0.0554586\n",
      "[500]\ttraining's rmse: 0.0429489\tvalid_1's rmse: 0.0470197\n",
      "[600]\ttraining's rmse: 0.0366633\tvalid_1's rmse: 0.040984\n",
      "[700]\ttraining's rmse: 0.0317159\tvalid_1's rmse: 0.0362366\n",
      "[800]\ttraining's rmse: 0.0282024\tvalid_1's rmse: 0.0329346\n",
      "[900]\ttraining's rmse: 0.0255122\tvalid_1's rmse: 0.0304126\n",
      "[1000]\ttraining's rmse: 0.0233036\tvalid_1's rmse: 0.0283251\n",
      "[1100]\ttraining's rmse: 0.0214005\tvalid_1's rmse: 0.0265165\n",
      "[1200]\ttraining's rmse: 0.0198422\tvalid_1's rmse: 0.0250648\n",
      "[1300]\ttraining's rmse: 0.0185802\tvalid_1's rmse: 0.0238741\n",
      "[1400]\ttraining's rmse: 0.0174891\tvalid_1's rmse: 0.0228215\n",
      "[1500]\ttraining's rmse: 0.0166447\tvalid_1's rmse: 0.0220385\n",
      "[1600]\ttraining's rmse: 0.0159033\tvalid_1's rmse: 0.0213853\n",
      "[1700]\ttraining's rmse: 0.0152582\tvalid_1's rmse: 0.0208146\n",
      "[1800]\ttraining's rmse: 0.0146552\tvalid_1's rmse: 0.0202702\n",
      "[1900]\ttraining's rmse: 0.0141653\tvalid_1's rmse: 0.0198581\n",
      "[2000]\ttraining's rmse: 0.0137159\tvalid_1's rmse: 0.0194841\n",
      "[2100]\ttraining's rmse: 0.0133357\tvalid_1's rmse: 0.0191904\n",
      "[2200]\ttraining's rmse: 0.0129845\tvalid_1's rmse: 0.0189422\n",
      "[2300]\ttraining's rmse: 0.0126394\tvalid_1's rmse: 0.0186808\n",
      "[2400]\ttraining's rmse: 0.0123272\tvalid_1's rmse: 0.0184521\n",
      "[2500]\ttraining's rmse: 0.0120488\tvalid_1's rmse: 0.018261\n",
      "[2600]\ttraining's rmse: 0.0117765\tvalid_1's rmse: 0.0180871\n",
      "[2700]\ttraining's rmse: 0.0115151\tvalid_1's rmse: 0.0179312\n",
      "[2800]\ttraining's rmse: 0.0112639\tvalid_1's rmse: 0.0177819\n",
      "[2900]\ttraining's rmse: 0.0110585\tvalid_1's rmse: 0.0176654\n",
      "[3000]\ttraining's rmse: 0.0108582\tvalid_1's rmse: 0.0175503\n",
      "[3100]\ttraining's rmse: 0.0106676\tvalid_1's rmse: 0.0174584\n",
      "[3200]\ttraining's rmse: 0.010491\tvalid_1's rmse: 0.017378\n",
      "[3300]\ttraining's rmse: 0.0103124\tvalid_1's rmse: 0.0172997\n",
      "[3400]\ttraining's rmse: 0.0101358\tvalid_1's rmse: 0.0172158\n",
      "[3500]\ttraining's rmse: 0.00996877\tvalid_1's rmse: 0.0171511\n",
      "[3600]\ttraining's rmse: 0.00982628\tvalid_1's rmse: 0.0170821\n",
      "[3700]\ttraining's rmse: 0.00966957\tvalid_1's rmse: 0.0170073\n",
      "[3800]\ttraining's rmse: 0.00953097\tvalid_1's rmse: 0.0169634\n",
      "[3900]\ttraining's rmse: 0.00939584\tvalid_1's rmse: 0.0169097\n",
      "[4000]\ttraining's rmse: 0.00926368\tvalid_1's rmse: 0.0168502\n",
      "[4100]\ttraining's rmse: 0.00914886\tvalid_1's rmse: 0.0168048\n",
      "[4200]\ttraining's rmse: 0.00902401\tvalid_1's rmse: 0.0167541\n",
      "[4300]\ttraining's rmse: 0.00889546\tvalid_1's rmse: 0.0167044\n",
      "[4400]\ttraining's rmse: 0.00878301\tvalid_1's rmse: 0.0166602\n",
      "[4500]\ttraining's rmse: 0.00867447\tvalid_1's rmse: 0.0166229\n",
      "[4600]\ttraining's rmse: 0.00856451\tvalid_1's rmse: 0.0165735\n",
      "[4700]\ttraining's rmse: 0.0084608\tvalid_1's rmse: 0.0165329\n",
      "[4800]\ttraining's rmse: 0.00835707\tvalid_1's rmse: 0.0164986\n",
      "[4900]\ttraining's rmse: 0.00825646\tvalid_1's rmse: 0.0164609\n",
      "[5000]\ttraining's rmse: 0.00815601\tvalid_1's rmse: 0.0164283\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4975]\ttraining's rmse: 0.00817913\tvalid_1's rmse: 0.0164283\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.149533\tvalid_1's rmse: 0.151703\n",
      "[200]\ttraining's rmse: 0.0975092\tvalid_1's rmse: 0.101225\n",
      "[300]\ttraining's rmse: 0.0681389\tvalid_1's rmse: 0.0726053\n",
      "[400]\ttraining's rmse: 0.051418\tvalid_1's rmse: 0.0562151\n",
      "[500]\ttraining's rmse: 0.0417228\tvalid_1's rmse: 0.0467186\n",
      "[600]\ttraining's rmse: 0.0349029\tvalid_1's rmse: 0.0399682\n",
      "[700]\ttraining's rmse: 0.0297951\tvalid_1's rmse: 0.0347796\n",
      "[800]\ttraining's rmse: 0.0262189\tvalid_1's rmse: 0.0311489\n",
      "[900]\ttraining's rmse: 0.0236588\tvalid_1's rmse: 0.0285757\n",
      "[1000]\ttraining's rmse: 0.0216495\tvalid_1's rmse: 0.0265203\n",
      "[1100]\ttraining's rmse: 0.019958\tvalid_1's rmse: 0.0247569\n",
      "[1200]\ttraining's rmse: 0.0186586\tvalid_1's rmse: 0.0234072\n",
      "[1300]\ttraining's rmse: 0.0176034\tvalid_1's rmse: 0.0223088\n",
      "[1400]\ttraining's rmse: 0.0167343\tvalid_1's rmse: 0.0213855\n",
      "[1500]\ttraining's rmse: 0.0160584\tvalid_1's rmse: 0.0207411\n",
      "[1600]\ttraining's rmse: 0.015471\tvalid_1's rmse: 0.0202024\n",
      "[1700]\ttraining's rmse: 0.0149623\tvalid_1's rmse: 0.0197383\n",
      "[1800]\ttraining's rmse: 0.0144811\tvalid_1's rmse: 0.0192963\n",
      "[1900]\ttraining's rmse: 0.0140891\tvalid_1's rmse: 0.018955\n",
      "[2000]\ttraining's rmse: 0.0137204\tvalid_1's rmse: 0.0186622\n",
      "[2100]\ttraining's rmse: 0.0133849\tvalid_1's rmse: 0.0184222\n",
      "[2200]\ttraining's rmse: 0.013097\tvalid_1's rmse: 0.018226\n",
      "[2300]\ttraining's rmse: 0.012812\tvalid_1's rmse: 0.0180441\n",
      "[2400]\ttraining's rmse: 0.012536\tvalid_1's rmse: 0.0178497\n",
      "[2500]\ttraining's rmse: 0.01229\tvalid_1's rmse: 0.0176922\n",
      "[2600]\ttraining's rmse: 0.0120474\tvalid_1's rmse: 0.0175257\n",
      "[2700]\ttraining's rmse: 0.0118263\tvalid_1's rmse: 0.0173899\n",
      "[2800]\ttraining's rmse: 0.0116245\tvalid_1's rmse: 0.0172885\n",
      "[2900]\ttraining's rmse: 0.0114316\tvalid_1's rmse: 0.0171836\n",
      "[3000]\ttraining's rmse: 0.0112643\tvalid_1's rmse: 0.0170948\n",
      "[3100]\ttraining's rmse: 0.0110715\tvalid_1's rmse: 0.0169891\n",
      "[3200]\ttraining's rmse: 0.0109249\tvalid_1's rmse: 0.0169344\n",
      "[3300]\ttraining's rmse: 0.0107731\tvalid_1's rmse: 0.0168744\n",
      "[3400]\ttraining's rmse: 0.0106172\tvalid_1's rmse: 0.0168055\n",
      "[3500]\ttraining's rmse: 0.0104676\tvalid_1's rmse: 0.0167448\n",
      "[3600]\ttraining's rmse: 0.0103174\tvalid_1's rmse: 0.0166825\n",
      "[3700]\ttraining's rmse: 0.0101909\tvalid_1's rmse: 0.0166357\n",
      "[3800]\ttraining's rmse: 0.0100361\tvalid_1's rmse: 0.0165685\n",
      "[3900]\ttraining's rmse: 0.00989932\tvalid_1's rmse: 0.0165198\n",
      "[4000]\ttraining's rmse: 0.00975926\tvalid_1's rmse: 0.0164706\n",
      "[4100]\ttraining's rmse: 0.00964025\tvalid_1's rmse: 0.0164377\n",
      "[4200]\ttraining's rmse: 0.00951875\tvalid_1's rmse: 0.0163903\n",
      "[4300]\ttraining's rmse: 0.00938592\tvalid_1's rmse: 0.0163355\n",
      "[4400]\ttraining's rmse: 0.0092675\tvalid_1's rmse: 0.0162958\n",
      "[4500]\ttraining's rmse: 0.00915738\tvalid_1's rmse: 0.0162647\n",
      "[4600]\ttraining's rmse: 0.00904016\tvalid_1's rmse: 0.0162154\n",
      "[4700]\ttraining's rmse: 0.00892699\tvalid_1's rmse: 0.0161694\n",
      "[4800]\ttraining's rmse: 0.00882762\tvalid_1's rmse: 0.0161454\n",
      "[4900]\ttraining's rmse: 0.00872424\tvalid_1's rmse: 0.016112\n",
      "[5000]\ttraining's rmse: 0.00862103\tvalid_1's rmse: 0.0160794\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00862103\tvalid_1's rmse: 0.0160794\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.149164\tvalid_1's rmse: 0.150682\n",
      "[200]\ttraining's rmse: 0.0969061\tvalid_1's rmse: 0.100651\n",
      "[300]\ttraining's rmse: 0.0669001\tvalid_1's rmse: 0.0713955\n",
      "[400]\ttraining's rmse: 0.0521302\tvalid_1's rmse: 0.0569709\n",
      "[500]\ttraining's rmse: 0.0426141\tvalid_1's rmse: 0.0475302\n",
      "[600]\ttraining's rmse: 0.0362247\tvalid_1's rmse: 0.041179\n",
      "[700]\ttraining's rmse: 0.0312524\tvalid_1's rmse: 0.0362373\n",
      "[800]\ttraining's rmse: 0.0276251\tvalid_1's rmse: 0.0326402\n",
      "[900]\ttraining's rmse: 0.0247542\tvalid_1's rmse: 0.0297583\n",
      "[1000]\ttraining's rmse: 0.0224919\tvalid_1's rmse: 0.0274851\n",
      "[1100]\ttraining's rmse: 0.0208102\tvalid_1's rmse: 0.0258086\n",
      "[1200]\ttraining's rmse: 0.0194287\tvalid_1's rmse: 0.0244286\n",
      "[1300]\ttraining's rmse: 0.018234\tvalid_1's rmse: 0.0232274\n",
      "[1400]\ttraining's rmse: 0.0172261\tvalid_1's rmse: 0.0221686\n",
      "[1500]\ttraining's rmse: 0.0164237\tvalid_1's rmse: 0.0213471\n",
      "[1600]\ttraining's rmse: 0.0157397\tvalid_1's rmse: 0.0207076\n",
      "[1700]\ttraining's rmse: 0.0151562\tvalid_1's rmse: 0.0201657\n",
      "[1800]\ttraining's rmse: 0.0146464\tvalid_1's rmse: 0.0197085\n",
      "[1900]\ttraining's rmse: 0.0141995\tvalid_1's rmse: 0.0192913\n",
      "[2000]\ttraining's rmse: 0.0137833\tvalid_1's rmse: 0.0189396\n",
      "[2100]\ttraining's rmse: 0.0134227\tvalid_1's rmse: 0.018644\n",
      "[2200]\ttraining's rmse: 0.0130757\tvalid_1's rmse: 0.0183826\n",
      "[2300]\ttraining's rmse: 0.0127871\tvalid_1's rmse: 0.0181695\n",
      "[2400]\ttraining's rmse: 0.0125009\tvalid_1's rmse: 0.0179763\n",
      "[2500]\ttraining's rmse: 0.0122313\tvalid_1's rmse: 0.0177922\n",
      "[2600]\ttraining's rmse: 0.0119666\tvalid_1's rmse: 0.0176092\n",
      "[2700]\ttraining's rmse: 0.0117262\tvalid_1's rmse: 0.0174589\n",
      "[2800]\ttraining's rmse: 0.0115188\tvalid_1's rmse: 0.0173388\n",
      "[2900]\ttraining's rmse: 0.0113025\tvalid_1's rmse: 0.0172098\n",
      "[3000]\ttraining's rmse: 0.0111032\tvalid_1's rmse: 0.0170798\n",
      "[3100]\ttraining's rmse: 0.0109168\tvalid_1's rmse: 0.0169795\n",
      "[3200]\ttraining's rmse: 0.0107262\tvalid_1's rmse: 0.0168823\n",
      "[3300]\ttraining's rmse: 0.0105454\tvalid_1's rmse: 0.0167806\n",
      "[3400]\ttraining's rmse: 0.010374\tvalid_1's rmse: 0.0166864\n",
      "[3500]\ttraining's rmse: 0.010211\tvalid_1's rmse: 0.0166083\n",
      "[3600]\ttraining's rmse: 0.0100594\tvalid_1's rmse: 0.0165343\n",
      "[3700]\ttraining's rmse: 0.00992622\tvalid_1's rmse: 0.0164762\n",
      "[3800]\ttraining's rmse: 0.00975396\tvalid_1's rmse: 0.0163852\n",
      "[3900]\ttraining's rmse: 0.00962554\tvalid_1's rmse: 0.0163348\n",
      "[4000]\ttraining's rmse: 0.00949192\tvalid_1's rmse: 0.01627\n",
      "[4100]\ttraining's rmse: 0.00935769\tvalid_1's rmse: 0.0162171\n",
      "[4200]\ttraining's rmse: 0.00923059\tvalid_1's rmse: 0.016155\n",
      "[4300]\ttraining's rmse: 0.00912455\tvalid_1's rmse: 0.0161142\n",
      "[4400]\ttraining's rmse: 0.00902481\tvalid_1's rmse: 0.0160813\n",
      "[4500]\ttraining's rmse: 0.0088958\tvalid_1's rmse: 0.0160237\n",
      "[4600]\ttraining's rmse: 0.00876355\tvalid_1's rmse: 0.0159529\n",
      "[4700]\ttraining's rmse: 0.00866657\tvalid_1's rmse: 0.0159202\n",
      "[4800]\ttraining's rmse: 0.00855599\tvalid_1's rmse: 0.0158704\n",
      "[4900]\ttraining's rmse: 0.0084545\tvalid_1's rmse: 0.0158423\n",
      "[5000]\ttraining's rmse: 0.00834495\tvalid_1's rmse: 0.0157995\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00834495\tvalid_1's rmse: 0.0157995\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.150092\tvalid_1's rmse: 0.150282\n",
      "[200]\ttraining's rmse: 0.0991189\tvalid_1's rmse: 0.101473\n",
      "[300]\ttraining's rmse: 0.0698129\tvalid_1's rmse: 0.0733123\n",
      "[400]\ttraining's rmse: 0.0528969\tvalid_1's rmse: 0.0570529\n",
      "[500]\ttraining's rmse: 0.0429888\tvalid_1's rmse: 0.0473895\n",
      "[600]\ttraining's rmse: 0.0360733\tvalid_1's rmse: 0.0406198\n",
      "[700]\ttraining's rmse: 0.0307047\tvalid_1's rmse: 0.0352939\n",
      "[800]\ttraining's rmse: 0.0269039\tvalid_1's rmse: 0.0315094\n",
      "[900]\ttraining's rmse: 0.0240898\tvalid_1's rmse: 0.0286963\n",
      "[1000]\ttraining's rmse: 0.0218366\tvalid_1's rmse: 0.0264095\n",
      "[1100]\ttraining's rmse: 0.0199234\tvalid_1's rmse: 0.0244367\n",
      "[1200]\ttraining's rmse: 0.0184287\tvalid_1's rmse: 0.0229238\n",
      "[1300]\ttraining's rmse: 0.0172218\tvalid_1's rmse: 0.0216897\n",
      "[1400]\ttraining's rmse: 0.0162188\tvalid_1's rmse: 0.0206414\n",
      "[1500]\ttraining's rmse: 0.0154855\tvalid_1's rmse: 0.0199072\n",
      "[1600]\ttraining's rmse: 0.0148337\tvalid_1's rmse: 0.0192638\n",
      "[1700]\ttraining's rmse: 0.014263\tvalid_1's rmse: 0.0187128\n",
      "[1800]\ttraining's rmse: 0.013753\tvalid_1's rmse: 0.0182149\n",
      "[1900]\ttraining's rmse: 0.013355\tvalid_1's rmse: 0.0178626\n",
      "[2000]\ttraining's rmse: 0.0129715\tvalid_1's rmse: 0.0175345\n",
      "[2100]\ttraining's rmse: 0.0126512\tvalid_1's rmse: 0.0172851\n",
      "[2200]\ttraining's rmse: 0.0123453\tvalid_1's rmse: 0.0170772\n",
      "[2300]\ttraining's rmse: 0.0120723\tvalid_1's rmse: 0.0168878\n",
      "[2400]\ttraining's rmse: 0.0117989\tvalid_1's rmse: 0.0166833\n",
      "[2500]\ttraining's rmse: 0.0115723\tvalid_1's rmse: 0.0165477\n",
      "[2600]\ttraining's rmse: 0.0113487\tvalid_1's rmse: 0.0164303\n",
      "[2700]\ttraining's rmse: 0.011133\tvalid_1's rmse: 0.0163096\n",
      "[2800]\ttraining's rmse: 0.0109449\tvalid_1's rmse: 0.016215\n",
      "[2900]\ttraining's rmse: 0.0107677\tvalid_1's rmse: 0.0161365\n",
      "[3000]\ttraining's rmse: 0.0105879\tvalid_1's rmse: 0.0160524\n",
      "[3100]\ttraining's rmse: 0.0104258\tvalid_1's rmse: 0.0159812\n",
      "[3200]\ttraining's rmse: 0.010257\tvalid_1's rmse: 0.0159052\n",
      "[3300]\ttraining's rmse: 0.0101082\tvalid_1's rmse: 0.0158263\n",
      "[3400]\ttraining's rmse: 0.00995781\tvalid_1's rmse: 0.015767\n",
      "[3500]\ttraining's rmse: 0.00979842\tvalid_1's rmse: 0.0156916\n",
      "[3600]\ttraining's rmse: 0.00966649\tvalid_1's rmse: 0.0156419\n",
      "[3700]\ttraining's rmse: 0.00954238\tvalid_1's rmse: 0.0156029\n",
      "[3800]\ttraining's rmse: 0.00940751\tvalid_1's rmse: 0.0155414\n",
      "[3900]\ttraining's rmse: 0.00929152\tvalid_1's rmse: 0.0155152\n",
      "[4000]\ttraining's rmse: 0.00915965\tvalid_1's rmse: 0.0154637\n",
      "[4100]\ttraining's rmse: 0.00904062\tvalid_1's rmse: 0.0154134\n",
      "[4200]\ttraining's rmse: 0.00891491\tvalid_1's rmse: 0.01536\n",
      "[4300]\ttraining's rmse: 0.00879436\tvalid_1's rmse: 0.0153135\n",
      "[4400]\ttraining's rmse: 0.00868312\tvalid_1's rmse: 0.0152772\n",
      "[4500]\ttraining's rmse: 0.00858774\tvalid_1's rmse: 0.0152577\n",
      "[4600]\ttraining's rmse: 0.00848856\tvalid_1's rmse: 0.0152274\n",
      "[4700]\ttraining's rmse: 0.00837792\tvalid_1's rmse: 0.0151848\n",
      "[4800]\ttraining's rmse: 0.00829016\tvalid_1's rmse: 0.0151585\n",
      "[4900]\ttraining's rmse: 0.00820323\tvalid_1's rmse: 0.0151362\n",
      "[5000]\ttraining's rmse: 0.00811834\tvalid_1's rmse: 0.0151132\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\ttraining's rmse: 0.00811913\tvalid_1's rmse: 0.015113\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.147901\tvalid_1's rmse: 0.151528\n",
      "[200]\ttraining's rmse: 0.0951565\tvalid_1's rmse: 0.100936\n",
      "[300]\ttraining's rmse: 0.0656212\tvalid_1's rmse: 0.072065\n",
      "[400]\ttraining's rmse: 0.0513167\tvalid_1's rmse: 0.0579398\n",
      "[500]\ttraining's rmse: 0.041968\tvalid_1's rmse: 0.0485276\n",
      "[600]\ttraining's rmse: 0.0356149\tvalid_1's rmse: 0.0421017\n",
      "[700]\ttraining's rmse: 0.030682\tvalid_1's rmse: 0.0371163\n",
      "[800]\ttraining's rmse: 0.0270883\tvalid_1's rmse: 0.0334695\n",
      "[900]\ttraining's rmse: 0.0242506\tvalid_1's rmse: 0.0305892\n",
      "[1000]\ttraining's rmse: 0.0220322\tvalid_1's rmse: 0.0282819\n",
      "[1100]\ttraining's rmse: 0.0203717\tvalid_1's rmse: 0.0265693\n",
      "[1200]\ttraining's rmse: 0.0190256\tvalid_1's rmse: 0.0251763\n",
      "[1300]\ttraining's rmse: 0.0178984\tvalid_1's rmse: 0.0239902\n",
      "[1400]\ttraining's rmse: 0.0169366\tvalid_1's rmse: 0.0229529\n",
      "[1500]\ttraining's rmse: 0.0161705\tvalid_1's rmse: 0.0221356\n",
      "[1600]\ttraining's rmse: 0.0155229\tvalid_1's rmse: 0.0214743\n",
      "[1700]\ttraining's rmse: 0.0149703\tvalid_1's rmse: 0.0209501\n",
      "[1800]\ttraining's rmse: 0.0144579\tvalid_1's rmse: 0.0204719\n",
      "[1900]\ttraining's rmse: 0.0140191\tvalid_1's rmse: 0.020072\n",
      "[2000]\ttraining's rmse: 0.0136096\tvalid_1's rmse: 0.0197166\n",
      "[2100]\ttraining's rmse: 0.0132703\tvalid_1's rmse: 0.0194521\n",
      "[2200]\ttraining's rmse: 0.0129562\tvalid_1's rmse: 0.0192073\n",
      "[2300]\ttraining's rmse: 0.0126703\tvalid_1's rmse: 0.0189878\n",
      "[2400]\ttraining's rmse: 0.0123854\tvalid_1's rmse: 0.0187981\n",
      "[2500]\ttraining's rmse: 0.0121343\tvalid_1's rmse: 0.0186347\n",
      "[2600]\ttraining's rmse: 0.0118746\tvalid_1's rmse: 0.0184437\n",
      "[2700]\ttraining's rmse: 0.011625\tvalid_1's rmse: 0.0182651\n",
      "[2800]\ttraining's rmse: 0.0114192\tvalid_1's rmse: 0.0181448\n",
      "[2900]\ttraining's rmse: 0.0112151\tvalid_1's rmse: 0.0180422\n",
      "[3000]\ttraining's rmse: 0.0110033\tvalid_1's rmse: 0.0179199\n",
      "[3100]\ttraining's rmse: 0.0108177\tvalid_1's rmse: 0.0178226\n",
      "[3200]\ttraining's rmse: 0.010627\tvalid_1's rmse: 0.0177214\n",
      "[3300]\ttraining's rmse: 0.0104599\tvalid_1's rmse: 0.0176417\n",
      "[3400]\ttraining's rmse: 0.0103006\tvalid_1's rmse: 0.0175785\n",
      "[3500]\ttraining's rmse: 0.0101284\tvalid_1's rmse: 0.0174919\n",
      "[3600]\ttraining's rmse: 0.00998304\tvalid_1's rmse: 0.0174225\n",
      "[3700]\ttraining's rmse: 0.00985569\tvalid_1's rmse: 0.0173742\n",
      "[3800]\ttraining's rmse: 0.00968939\tvalid_1's rmse: 0.0172904\n",
      "[3900]\ttraining's rmse: 0.00954962\tvalid_1's rmse: 0.0172314\n",
      "[4000]\ttraining's rmse: 0.00941812\tvalid_1's rmse: 0.0171825\n",
      "[4100]\ttraining's rmse: 0.0092903\tvalid_1's rmse: 0.0171245\n",
      "[4200]\ttraining's rmse: 0.00916699\tvalid_1's rmse: 0.0170671\n",
      "[4300]\ttraining's rmse: 0.00904439\tvalid_1's rmse: 0.0170164\n",
      "[4400]\ttraining's rmse: 0.00891897\tvalid_1's rmse: 0.0169691\n",
      "[4500]\ttraining's rmse: 0.00879847\tvalid_1's rmse: 0.0169084\n",
      "[4600]\ttraining's rmse: 0.0086867\tvalid_1's rmse: 0.0168829\n",
      "[4700]\ttraining's rmse: 0.00856901\tvalid_1's rmse: 0.0168371\n",
      "[4800]\ttraining's rmse: 0.00845495\tvalid_1's rmse: 0.0167884\n",
      "[4900]\ttraining's rmse: 0.00836075\tvalid_1's rmse: 0.016767\n",
      "[5000]\ttraining's rmse: 0.00825096\tvalid_1's rmse: 0.0167236\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4995]\ttraining's rmse: 0.0082552\tvalid_1's rmse: 0.0167229\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.146658\tvalid_1's rmse: 0.14944\n",
      "[200]\ttraining's rmse: 0.0949563\tvalid_1's rmse: 0.100106\n",
      "[300]\ttraining's rmse: 0.0665151\tvalid_1's rmse: 0.072696\n",
      "[400]\ttraining's rmse: 0.0528153\tvalid_1's rmse: 0.0593842\n",
      "[500]\ttraining's rmse: 0.0437721\tvalid_1's rmse: 0.0505256\n",
      "[600]\ttraining's rmse: 0.0374541\tvalid_1's rmse: 0.0442858\n",
      "[700]\ttraining's rmse: 0.0324479\tvalid_1's rmse: 0.0392662\n",
      "[800]\ttraining's rmse: 0.0287626\tvalid_1's rmse: 0.0354922\n",
      "[900]\ttraining's rmse: 0.025808\tvalid_1's rmse: 0.0324559\n",
      "[1000]\ttraining's rmse: 0.0235019\tvalid_1's rmse: 0.0300496\n",
      "[1100]\ttraining's rmse: 0.0217129\tvalid_1's rmse: 0.0281629\n",
      "[1200]\ttraining's rmse: 0.0202533\tvalid_1's rmse: 0.0266372\n",
      "[1300]\ttraining's rmse: 0.0189882\tvalid_1's rmse: 0.025313\n",
      "[1400]\ttraining's rmse: 0.017922\tvalid_1's rmse: 0.0241817\n",
      "[1500]\ttraining's rmse: 0.0170558\tvalid_1's rmse: 0.0232535\n",
      "[1600]\ttraining's rmse: 0.0163256\tvalid_1's rmse: 0.0225065\n",
      "[1700]\ttraining's rmse: 0.0156893\tvalid_1's rmse: 0.021893\n",
      "[1800]\ttraining's rmse: 0.0151377\tvalid_1's rmse: 0.0213768\n",
      "[1900]\ttraining's rmse: 0.0146431\tvalid_1's rmse: 0.0209042\n",
      "[2000]\ttraining's rmse: 0.0141976\tvalid_1's rmse: 0.0205081\n",
      "[2100]\ttraining's rmse: 0.013805\tvalid_1's rmse: 0.020164\n",
      "[2200]\ttraining's rmse: 0.0134403\tvalid_1's rmse: 0.0198714\n",
      "[2300]\ttraining's rmse: 0.0131115\tvalid_1's rmse: 0.0196082\n",
      "[2400]\ttraining's rmse: 0.0127813\tvalid_1's rmse: 0.0193729\n",
      "[2500]\ttraining's rmse: 0.0124825\tvalid_1's rmse: 0.0191432\n",
      "[2600]\ttraining's rmse: 0.0122116\tvalid_1's rmse: 0.0189306\n",
      "[2700]\ttraining's rmse: 0.011944\tvalid_1's rmse: 0.0187257\n",
      "[2800]\ttraining's rmse: 0.0117072\tvalid_1's rmse: 0.0185715\n",
      "[2900]\ttraining's rmse: 0.011492\tvalid_1's rmse: 0.0184274\n",
      "[3000]\ttraining's rmse: 0.0112806\tvalid_1's rmse: 0.0182871\n",
      "[3100]\ttraining's rmse: 0.0110892\tvalid_1's rmse: 0.0181751\n",
      "[3200]\ttraining's rmse: 0.0109011\tvalid_1's rmse: 0.0180735\n",
      "[3300]\ttraining's rmse: 0.010705\tvalid_1's rmse: 0.0179614\n",
      "[3400]\ttraining's rmse: 0.0105349\tvalid_1's rmse: 0.0178768\n",
      "[3500]\ttraining's rmse: 0.0103508\tvalid_1's rmse: 0.0177636\n",
      "[3600]\ttraining's rmse: 0.01019\tvalid_1's rmse: 0.0176687\n",
      "[3700]\ttraining's rmse: 0.0100302\tvalid_1's rmse: 0.0175942\n",
      "[3800]\ttraining's rmse: 0.00986451\tvalid_1's rmse: 0.0174953\n",
      "[3900]\ttraining's rmse: 0.00971745\tvalid_1's rmse: 0.0174307\n",
      "[4000]\ttraining's rmse: 0.00958627\tvalid_1's rmse: 0.0173721\n",
      "[4100]\ttraining's rmse: 0.00945183\tvalid_1's rmse: 0.0173028\n",
      "[4200]\ttraining's rmse: 0.00930358\tvalid_1's rmse: 0.0172274\n",
      "[4300]\ttraining's rmse: 0.009185\tvalid_1's rmse: 0.0171855\n",
      "[4400]\ttraining's rmse: 0.00906337\tvalid_1's rmse: 0.0171314\n",
      "[4500]\ttraining's rmse: 0.00895049\tvalid_1's rmse: 0.0170945\n",
      "[4600]\ttraining's rmse: 0.00883278\tvalid_1's rmse: 0.0170425\n",
      "[4700]\ttraining's rmse: 0.00870897\tvalid_1's rmse: 0.016989\n",
      "[4800]\ttraining's rmse: 0.00859345\tvalid_1's rmse: 0.0169372\n",
      "[4900]\ttraining's rmse: 0.0084895\tvalid_1's rmse: 0.0169043\n",
      "[5000]\ttraining's rmse: 0.00838122\tvalid_1's rmse: 0.0168549\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00838122\tvalid_1's rmse: 0.0168549\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.141757\tvalid_1's rmse: 0.140923\n",
      "[200]\ttraining's rmse: 0.0876624\tvalid_1's rmse: 0.0891086\n",
      "[300]\ttraining's rmse: 0.0610293\tvalid_1's rmse: 0.0636474\n",
      "[400]\ttraining's rmse: 0.0468233\tvalid_1's rmse: 0.0500774\n",
      "[500]\ttraining's rmse: 0.0376292\tvalid_1's rmse: 0.0412002\n",
      "[600]\ttraining's rmse: 0.0317356\tvalid_1's rmse: 0.0354796\n",
      "[700]\ttraining's rmse: 0.0278583\tvalid_1's rmse: 0.0317222\n",
      "[800]\ttraining's rmse: 0.0250537\tvalid_1's rmse: 0.0290085\n",
      "[900]\ttraining's rmse: 0.0229215\tvalid_1's rmse: 0.0269539\n",
      "[1000]\ttraining's rmse: 0.0211539\tvalid_1's rmse: 0.0252365\n",
      "[1100]\ttraining's rmse: 0.0196782\tvalid_1's rmse: 0.0237895\n",
      "[1200]\ttraining's rmse: 0.0185632\tvalid_1's rmse: 0.022734\n",
      "[1300]\ttraining's rmse: 0.0176531\tvalid_1's rmse: 0.02188\n",
      "[1400]\ttraining's rmse: 0.0169008\tvalid_1's rmse: 0.0211894\n",
      "[1500]\ttraining's rmse: 0.0162647\tvalid_1's rmse: 0.0206156\n",
      "[1600]\ttraining's rmse: 0.0157318\tvalid_1's rmse: 0.0201724\n",
      "[1700]\ttraining's rmse: 0.0152198\tvalid_1's rmse: 0.0197468\n",
      "[1800]\ttraining's rmse: 0.0147999\tvalid_1's rmse: 0.0194356\n",
      "[1900]\ttraining's rmse: 0.0144354\tvalid_1's rmse: 0.0191539\n",
      "[2000]\ttraining's rmse: 0.0140656\tvalid_1's rmse: 0.0188813\n",
      "[2100]\ttraining's rmse: 0.0137525\tvalid_1's rmse: 0.0186705\n",
      "[2200]\ttraining's rmse: 0.0134387\tvalid_1's rmse: 0.0184627\n",
      "[2300]\ttraining's rmse: 0.0131815\tvalid_1's rmse: 0.0182959\n",
      "[2400]\ttraining's rmse: 0.0129285\tvalid_1's rmse: 0.0181425\n",
      "[2500]\ttraining's rmse: 0.0126792\tvalid_1's rmse: 0.0179716\n",
      "[2600]\ttraining's rmse: 0.0124088\tvalid_1's rmse: 0.0177969\n",
      "[2700]\ttraining's rmse: 0.0121922\tvalid_1's rmse: 0.0176635\n",
      "[2800]\ttraining's rmse: 0.0119923\tvalid_1's rmse: 0.0175526\n",
      "[2900]\ttraining's rmse: 0.0117925\tvalid_1's rmse: 0.0174405\n",
      "[3000]\ttraining's rmse: 0.0115891\tvalid_1's rmse: 0.0173173\n",
      "[3100]\ttraining's rmse: 0.0114049\tvalid_1's rmse: 0.0172267\n",
      "[3200]\ttraining's rmse: 0.0112247\tvalid_1's rmse: 0.0171309\n",
      "[3300]\ttraining's rmse: 0.0110501\tvalid_1's rmse: 0.0170495\n",
      "[3400]\ttraining's rmse: 0.0108887\tvalid_1's rmse: 0.0169825\n",
      "[3500]\ttraining's rmse: 0.0107231\tvalid_1's rmse: 0.0169225\n",
      "[3600]\ttraining's rmse: 0.0105607\tvalid_1's rmse: 0.0168361\n",
      "[3700]\ttraining's rmse: 0.0103886\tvalid_1's rmse: 0.0167598\n",
      "[3800]\ttraining's rmse: 0.0102058\tvalid_1's rmse: 0.0166534\n",
      "[3900]\ttraining's rmse: 0.0100424\tvalid_1's rmse: 0.0165604\n",
      "[4000]\ttraining's rmse: 0.00991588\tvalid_1's rmse: 0.0165197\n",
      "[4100]\ttraining's rmse: 0.00978646\tvalid_1's rmse: 0.0164674\n",
      "[4200]\ttraining's rmse: 0.00965224\tvalid_1's rmse: 0.0163972\n",
      "[4300]\ttraining's rmse: 0.00953076\tvalid_1's rmse: 0.0163472\n",
      "[4400]\ttraining's rmse: 0.00941233\tvalid_1's rmse: 0.0162912\n",
      "[4500]\ttraining's rmse: 0.00928643\tvalid_1's rmse: 0.0162418\n",
      "[4600]\ttraining's rmse: 0.00917253\tvalid_1's rmse: 0.0161839\n",
      "[4700]\ttraining's rmse: 0.00906048\tvalid_1's rmse: 0.0161431\n",
      "[4800]\ttraining's rmse: 0.00895434\tvalid_1's rmse: 0.0160951\n",
      "[4900]\ttraining's rmse: 0.00884907\tvalid_1's rmse: 0.0160478\n",
      "[5000]\ttraining's rmse: 0.00874744\tvalid_1's rmse: 0.0160046\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00874744\tvalid_1's rmse: 0.0160046\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.150978\tvalid_1's rmse: 0.152101\n",
      "[200]\ttraining's rmse: 0.0982553\tvalid_1's rmse: 0.10134\n",
      "[300]\ttraining's rmse: 0.0677288\tvalid_1's rmse: 0.0717691\n",
      "[400]\ttraining's rmse: 0.0526915\tvalid_1's rmse: 0.0571841\n",
      "[500]\ttraining's rmse: 0.042692\tvalid_1's rmse: 0.0473766\n",
      "[600]\ttraining's rmse: 0.0359872\tvalid_1's rmse: 0.0408063\n",
      "[700]\ttraining's rmse: 0.030829\tvalid_1's rmse: 0.0356558\n",
      "[800]\ttraining's rmse: 0.0271739\tvalid_1's rmse: 0.0320399\n",
      "[900]\ttraining's rmse: 0.0243284\tvalid_1's rmse: 0.0292526\n",
      "[1000]\ttraining's rmse: 0.0221622\tvalid_1's rmse: 0.0271149\n",
      "[1100]\ttraining's rmse: 0.0205068\tvalid_1's rmse: 0.0255043\n",
      "[1200]\ttraining's rmse: 0.0191726\tvalid_1's rmse: 0.0241759\n",
      "[1300]\ttraining's rmse: 0.0180246\tvalid_1's rmse: 0.023083\n",
      "[1400]\ttraining's rmse: 0.0171043\tvalid_1's rmse: 0.0221493\n",
      "[1500]\ttraining's rmse: 0.0163697\tvalid_1's rmse: 0.0214169\n",
      "[1600]\ttraining's rmse: 0.0157121\tvalid_1's rmse: 0.0208272\n",
      "[1700]\ttraining's rmse: 0.0151748\tvalid_1's rmse: 0.020317\n",
      "[1800]\ttraining's rmse: 0.0146769\tvalid_1's rmse: 0.0198904\n",
      "[1900]\ttraining's rmse: 0.0142601\tvalid_1's rmse: 0.0195531\n",
      "[2000]\ttraining's rmse: 0.013868\tvalid_1's rmse: 0.0192482\n",
      "[2100]\ttraining's rmse: 0.013538\tvalid_1's rmse: 0.0190163\n",
      "[2200]\ttraining's rmse: 0.0132327\tvalid_1's rmse: 0.0188194\n",
      "[2300]\ttraining's rmse: 0.0129727\tvalid_1's rmse: 0.0186496\n",
      "[2400]\ttraining's rmse: 0.0126979\tvalid_1's rmse: 0.0184717\n",
      "[2500]\ttraining's rmse: 0.0124373\tvalid_1's rmse: 0.0183112\n",
      "[2600]\ttraining's rmse: 0.0121855\tvalid_1's rmse: 0.0181656\n",
      "[2700]\ttraining's rmse: 0.0119612\tvalid_1's rmse: 0.0180275\n",
      "[2800]\ttraining's rmse: 0.011774\tvalid_1's rmse: 0.017934\n",
      "[2900]\ttraining's rmse: 0.0115794\tvalid_1's rmse: 0.0178311\n",
      "[3000]\ttraining's rmse: 0.0113767\tvalid_1's rmse: 0.0177215\n",
      "[3100]\ttraining's rmse: 0.0111955\tvalid_1's rmse: 0.0176407\n",
      "[3200]\ttraining's rmse: 0.0110035\tvalid_1's rmse: 0.0175438\n",
      "[3300]\ttraining's rmse: 0.0108361\tvalid_1's rmse: 0.0174622\n",
      "[3400]\ttraining's rmse: 0.0106782\tvalid_1's rmse: 0.0173969\n",
      "[3500]\ttraining's rmse: 0.01051\tvalid_1's rmse: 0.0173236\n",
      "[3600]\ttraining's rmse: 0.0103618\tvalid_1's rmse: 0.0172626\n",
      "[3700]\ttraining's rmse: 0.0102288\tvalid_1's rmse: 0.0172182\n",
      "[3800]\ttraining's rmse: 0.0100903\tvalid_1's rmse: 0.0171637\n",
      "[3900]\ttraining's rmse: 0.00995699\tvalid_1's rmse: 0.0171406\n",
      "[4000]\ttraining's rmse: 0.00982529\tvalid_1's rmse: 0.0171103\n",
      "[4100]\ttraining's rmse: 0.00968257\tvalid_1's rmse: 0.0170452\n",
      "[4200]\ttraining's rmse: 0.00953187\tvalid_1's rmse: 0.0169753\n",
      "[4300]\ttraining's rmse: 0.00939766\tvalid_1's rmse: 0.0169194\n",
      "[4400]\ttraining's rmse: 0.00926913\tvalid_1's rmse: 0.0168795\n",
      "[4500]\ttraining's rmse: 0.00915217\tvalid_1's rmse: 0.0168357\n",
      "[4600]\ttraining's rmse: 0.00903412\tvalid_1's rmse: 0.0167883\n",
      "[4700]\ttraining's rmse: 0.00892619\tvalid_1's rmse: 0.0167542\n",
      "[4800]\ttraining's rmse: 0.00881774\tvalid_1's rmse: 0.0167223\n",
      "[4900]\ttraining's rmse: 0.00871735\tvalid_1's rmse: 0.0167028\n",
      "[5000]\ttraining's rmse: 0.00862637\tvalid_1's rmse: 0.0166774\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\ttraining's rmse: 0.00862719\tvalid_1's rmse: 0.0166772\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.148748\tvalid_1's rmse: 0.149114\n",
      "[200]\ttraining's rmse: 0.0974122\tvalid_1's rmse: 0.100151\n",
      "[300]\ttraining's rmse: 0.068547\tvalid_1's rmse: 0.0723779\n",
      "[400]\ttraining's rmse: 0.0521836\tvalid_1's rmse: 0.0565197\n",
      "[500]\ttraining's rmse: 0.0425899\tvalid_1's rmse: 0.0471381\n",
      "[600]\ttraining's rmse: 0.0358787\tvalid_1's rmse: 0.0404908\n",
      "[700]\ttraining's rmse: 0.0307679\tvalid_1's rmse: 0.035373\n",
      "[800]\ttraining's rmse: 0.0271594\tvalid_1's rmse: 0.0317813\n",
      "[900]\ttraining's rmse: 0.0244807\tvalid_1's rmse: 0.0291302\n",
      "[1000]\ttraining's rmse: 0.0223661\tvalid_1's rmse: 0.0270117\n",
      "[1100]\ttraining's rmse: 0.0205335\tvalid_1's rmse: 0.0251996\n",
      "[1200]\ttraining's rmse: 0.0190906\tvalid_1's rmse: 0.0237455\n",
      "[1300]\ttraining's rmse: 0.0179234\tvalid_1's rmse: 0.0225863\n",
      "[1400]\ttraining's rmse: 0.0169424\tvalid_1's rmse: 0.0215628\n",
      "[1500]\ttraining's rmse: 0.0162069\tvalid_1's rmse: 0.0208323\n",
      "[1600]\ttraining's rmse: 0.0155434\tvalid_1's rmse: 0.0202195\n",
      "[1700]\ttraining's rmse: 0.0149521\tvalid_1's rmse: 0.0196482\n",
      "[1800]\ttraining's rmse: 0.0143986\tvalid_1's rmse: 0.0191234\n",
      "[1900]\ttraining's rmse: 0.0139586\tvalid_1's rmse: 0.0187441\n",
      "[2000]\ttraining's rmse: 0.0135536\tvalid_1's rmse: 0.0183839\n",
      "[2100]\ttraining's rmse: 0.0132218\tvalid_1's rmse: 0.0181239\n",
      "[2200]\ttraining's rmse: 0.0129063\tvalid_1's rmse: 0.0178888\n",
      "[2300]\ttraining's rmse: 0.0126192\tvalid_1's rmse: 0.0176836\n",
      "[2400]\ttraining's rmse: 0.0123441\tvalid_1's rmse: 0.0174769\n",
      "[2500]\ttraining's rmse: 0.0120832\tvalid_1's rmse: 0.0172915\n",
      "[2600]\ttraining's rmse: 0.0118531\tvalid_1's rmse: 0.0171388\n",
      "[2700]\ttraining's rmse: 0.0116014\tvalid_1's rmse: 0.0169709\n",
      "[2800]\ttraining's rmse: 0.0113968\tvalid_1's rmse: 0.016843\n",
      "[2900]\ttraining's rmse: 0.0111929\tvalid_1's rmse: 0.016722\n",
      "[3000]\ttraining's rmse: 0.0109836\tvalid_1's rmse: 0.0166035\n",
      "[3100]\ttraining's rmse: 0.010779\tvalid_1's rmse: 0.016486\n",
      "[3200]\ttraining's rmse: 0.0106247\tvalid_1's rmse: 0.016415\n",
      "[3300]\ttraining's rmse: 0.0104602\tvalid_1's rmse: 0.0163278\n",
      "[3400]\ttraining's rmse: 0.0103004\tvalid_1's rmse: 0.0162479\n",
      "[3500]\ttraining's rmse: 0.0101453\tvalid_1's rmse: 0.0161744\n",
      "[3600]\ttraining's rmse: 0.00999836\tvalid_1's rmse: 0.0161095\n",
      "[3700]\ttraining's rmse: 0.00985192\tvalid_1's rmse: 0.0160354\n",
      "[3800]\ttraining's rmse: 0.00972797\tvalid_1's rmse: 0.0159882\n",
      "[3900]\ttraining's rmse: 0.00959903\tvalid_1's rmse: 0.015928\n",
      "[4000]\ttraining's rmse: 0.00945268\tvalid_1's rmse: 0.0158583\n",
      "[4100]\ttraining's rmse: 0.00933365\tvalid_1's rmse: 0.0158084\n",
      "[4200]\ttraining's rmse: 0.00919255\tvalid_1's rmse: 0.0157388\n",
      "[4300]\ttraining's rmse: 0.0090804\tvalid_1's rmse: 0.0156973\n",
      "[4400]\ttraining's rmse: 0.00894786\tvalid_1's rmse: 0.0156349\n",
      "[4500]\ttraining's rmse: 0.00883444\tvalid_1's rmse: 0.0155865\n",
      "[4600]\ttraining's rmse: 0.00872544\tvalid_1's rmse: 0.0155447\n",
      "[4700]\ttraining's rmse: 0.0086007\tvalid_1's rmse: 0.0154833\n",
      "[4800]\ttraining's rmse: 0.00850043\tvalid_1's rmse: 0.0154452\n",
      "[4900]\ttraining's rmse: 0.00840656\tvalid_1's rmse: 0.0154116\n",
      "[5000]\ttraining's rmse: 0.00829613\tvalid_1's rmse: 0.0153618\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\ttraining's rmse: 0.00829719\tvalid_1's rmse: 0.0153617\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.149465\tvalid_1's rmse: 0.151716\n",
      "[200]\ttraining's rmse: 0.0978107\tvalid_1's rmse: 0.101909\n",
      "[300]\ttraining's rmse: 0.0673297\tvalid_1's rmse: 0.072349\n",
      "[400]\ttraining's rmse: 0.0521034\tvalid_1's rmse: 0.0576011\n",
      "[500]\ttraining's rmse: 0.0421518\tvalid_1's rmse: 0.0478381\n",
      "[600]\ttraining's rmse: 0.0354659\tvalid_1's rmse: 0.0412509\n",
      "[700]\ttraining's rmse: 0.0303141\tvalid_1's rmse: 0.0360824\n",
      "[800]\ttraining's rmse: 0.0266718\tvalid_1's rmse: 0.0324515\n",
      "[900]\ttraining's rmse: 0.0238022\tvalid_1's rmse: 0.0295477\n",
      "[1000]\ttraining's rmse: 0.0215846\tvalid_1's rmse: 0.0272753\n",
      "[1100]\ttraining's rmse: 0.0199148\tvalid_1's rmse: 0.0255574\n",
      "[1200]\ttraining's rmse: 0.0185808\tvalid_1's rmse: 0.0241996\n",
      "[1300]\ttraining's rmse: 0.0174392\tvalid_1's rmse: 0.0229728\n",
      "[1400]\ttraining's rmse: 0.0165146\tvalid_1's rmse: 0.0220002\n",
      "[1500]\ttraining's rmse: 0.0157502\tvalid_1's rmse: 0.0211645\n",
      "[1600]\ttraining's rmse: 0.01513\tvalid_1's rmse: 0.0205326\n",
      "[1700]\ttraining's rmse: 0.0145894\tvalid_1's rmse: 0.0199798\n",
      "[1800]\ttraining's rmse: 0.0141372\tvalid_1's rmse: 0.0195399\n",
      "[1900]\ttraining's rmse: 0.013744\tvalid_1's rmse: 0.0191688\n",
      "[2000]\ttraining's rmse: 0.0133652\tvalid_1's rmse: 0.0188512\n",
      "[2100]\ttraining's rmse: 0.0130444\tvalid_1's rmse: 0.0185817\n",
      "[2200]\ttraining's rmse: 0.0127656\tvalid_1's rmse: 0.0183717\n",
      "[2300]\ttraining's rmse: 0.0125017\tvalid_1's rmse: 0.0181798\n",
      "[2400]\ttraining's rmse: 0.0122674\tvalid_1's rmse: 0.0180122\n",
      "[2500]\ttraining's rmse: 0.0120244\tvalid_1's rmse: 0.0178422\n",
      "[2600]\ttraining's rmse: 0.0117811\tvalid_1's rmse: 0.0176764\n",
      "[2700]\ttraining's rmse: 0.0115665\tvalid_1's rmse: 0.0175283\n",
      "[2800]\ttraining's rmse: 0.0113688\tvalid_1's rmse: 0.0174168\n",
      "[2900]\ttraining's rmse: 0.0111858\tvalid_1's rmse: 0.0173136\n",
      "[3000]\ttraining's rmse: 0.0109927\tvalid_1's rmse: 0.0171911\n",
      "[3100]\ttraining's rmse: 0.0108235\tvalid_1's rmse: 0.0170862\n",
      "[3200]\ttraining's rmse: 0.0106589\tvalid_1's rmse: 0.0170037\n",
      "[3300]\ttraining's rmse: 0.0105113\tvalid_1's rmse: 0.0169276\n",
      "[3400]\ttraining's rmse: 0.0103621\tvalid_1's rmse: 0.0168602\n",
      "[3500]\ttraining's rmse: 0.0102236\tvalid_1's rmse: 0.016817\n",
      "[3600]\ttraining's rmse: 0.010082\tvalid_1's rmse: 0.0167511\n",
      "[3700]\ttraining's rmse: 0.00996121\tvalid_1's rmse: 0.0167078\n",
      "[3800]\ttraining's rmse: 0.00982732\tvalid_1's rmse: 0.0166435\n",
      "[3900]\ttraining's rmse: 0.00969145\tvalid_1's rmse: 0.0165865\n",
      "[4000]\ttraining's rmse: 0.00957835\tvalid_1's rmse: 0.0165501\n",
      "[4100]\ttraining's rmse: 0.00946405\tvalid_1's rmse: 0.0165024\n",
      "[4200]\ttraining's rmse: 0.00935562\tvalid_1's rmse: 0.0164592\n",
      "[4300]\ttraining's rmse: 0.00924124\tvalid_1's rmse: 0.0164307\n",
      "[4400]\ttraining's rmse: 0.00913501\tvalid_1's rmse: 0.0163848\n",
      "[4500]\ttraining's rmse: 0.00902323\tvalid_1's rmse: 0.0163422\n",
      "[4600]\ttraining's rmse: 0.00891488\tvalid_1's rmse: 0.0162994\n",
      "[4700]\ttraining's rmse: 0.00881279\tvalid_1's rmse: 0.0162777\n",
      "[4800]\ttraining's rmse: 0.00871472\tvalid_1's rmse: 0.0162446\n",
      "[4900]\ttraining's rmse: 0.0086138\tvalid_1's rmse: 0.0162122\n",
      "[5000]\ttraining's rmse: 0.00851824\tvalid_1's rmse: 0.0161903\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00851824\tvalid_1's rmse: 0.0161903\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.15269\tvalid_1's rmse: 0.152028\n",
      "[200]\ttraining's rmse: 0.100022\tvalid_1's rmse: 0.101062\n",
      "[300]\ttraining's rmse: 0.0699465\tvalid_1's rmse: 0.0717997\n",
      "[400]\ttraining's rmse: 0.0527408\tvalid_1's rmse: 0.055105\n",
      "[500]\ttraining's rmse: 0.0427713\tvalid_1's rmse: 0.0455025\n",
      "[600]\ttraining's rmse: 0.0358701\tvalid_1's rmse: 0.0388129\n",
      "[700]\ttraining's rmse: 0.0305976\tvalid_1's rmse: 0.0337647\n",
      "[800]\ttraining's rmse: 0.0268871\tvalid_1's rmse: 0.0301928\n",
      "[900]\ttraining's rmse: 0.0241721\tvalid_1's rmse: 0.0276247\n",
      "[1000]\ttraining's rmse: 0.0220443\tvalid_1's rmse: 0.0256127\n",
      "[1100]\ttraining's rmse: 0.0202392\tvalid_1's rmse: 0.0238821\n",
      "[1200]\ttraining's rmse: 0.0188473\tvalid_1's rmse: 0.022563\n",
      "[1300]\ttraining's rmse: 0.017695\tvalid_1's rmse: 0.0214936\n",
      "[1400]\ttraining's rmse: 0.016727\tvalid_1's rmse: 0.0205707\n",
      "[1500]\ttraining's rmse: 0.016001\tvalid_1's rmse: 0.019916\n",
      "[1600]\ttraining's rmse: 0.0153534\tvalid_1's rmse: 0.0193445\n",
      "[1700]\ttraining's rmse: 0.0147929\tvalid_1's rmse: 0.0188685\n",
      "[1800]\ttraining's rmse: 0.0142632\tvalid_1's rmse: 0.0184046\n",
      "[1900]\ttraining's rmse: 0.0138606\tvalid_1's rmse: 0.0181034\n",
      "[2000]\ttraining's rmse: 0.0134404\tvalid_1's rmse: 0.0177717\n",
      "[2100]\ttraining's rmse: 0.0131002\tvalid_1's rmse: 0.0175446\n",
      "[2200]\ttraining's rmse: 0.0127937\tvalid_1's rmse: 0.0173396\n",
      "[2300]\ttraining's rmse: 0.0125128\tvalid_1's rmse: 0.0171815\n",
      "[2400]\ttraining's rmse: 0.0122251\tvalid_1's rmse: 0.0169892\n",
      "[2500]\ttraining's rmse: 0.0119758\tvalid_1's rmse: 0.0168388\n",
      "[2600]\ttraining's rmse: 0.0117412\tvalid_1's rmse: 0.0167113\n",
      "[2700]\ttraining's rmse: 0.0115043\tvalid_1's rmse: 0.0165762\n",
      "[2800]\ttraining's rmse: 0.011298\tvalid_1's rmse: 0.0164712\n",
      "[2900]\ttraining's rmse: 0.0110839\tvalid_1's rmse: 0.0163572\n",
      "[3000]\ttraining's rmse: 0.0108948\tvalid_1's rmse: 0.0162695\n",
      "[3100]\ttraining's rmse: 0.0107064\tvalid_1's rmse: 0.0161736\n",
      "[3200]\ttraining's rmse: 0.010534\tvalid_1's rmse: 0.0160949\n",
      "[3300]\ttraining's rmse: 0.010372\tvalid_1's rmse: 0.0160215\n",
      "[3400]\ttraining's rmse: 0.0102022\tvalid_1's rmse: 0.0159453\n",
      "[3500]\ttraining's rmse: 0.0100613\tvalid_1's rmse: 0.0158932\n",
      "[3600]\ttraining's rmse: 0.00991681\tvalid_1's rmse: 0.0158411\n",
      "[3700]\ttraining's rmse: 0.00978032\tvalid_1's rmse: 0.0157917\n",
      "[3800]\ttraining's rmse: 0.00965865\tvalid_1's rmse: 0.0157551\n",
      "[3900]\ttraining's rmse: 0.00952605\tvalid_1's rmse: 0.0157012\n",
      "[4000]\ttraining's rmse: 0.00939591\tvalid_1's rmse: 0.0156559\n",
      "[4100]\ttraining's rmse: 0.00927868\tvalid_1's rmse: 0.0156226\n",
      "[4200]\ttraining's rmse: 0.00915409\tvalid_1's rmse: 0.0155765\n",
      "[4300]\ttraining's rmse: 0.00902025\tvalid_1's rmse: 0.0155173\n",
      "[4400]\ttraining's rmse: 0.0089019\tvalid_1's rmse: 0.0154749\n",
      "[4500]\ttraining's rmse: 0.00880909\tvalid_1's rmse: 0.0154626\n",
      "[4600]\ttraining's rmse: 0.00870161\tvalid_1's rmse: 0.0154257\n",
      "[4700]\ttraining's rmse: 0.00858539\tvalid_1's rmse: 0.0153789\n",
      "[4800]\ttraining's rmse: 0.00847839\tvalid_1's rmse: 0.0153425\n",
      "[4900]\ttraining's rmse: 0.00836261\tvalid_1's rmse: 0.0152973\n",
      "[5000]\ttraining's rmse: 0.00825713\tvalid_1's rmse: 0.0152661\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4994]\ttraining's rmse: 0.00826155\tvalid_1's rmse: 0.0152649\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.153834\tvalid_1's rmse: 0.156322\n",
      "[200]\ttraining's rmse: 0.100038\tvalid_1's rmse: 0.104505\n",
      "[300]\ttraining's rmse: 0.0687006\tvalid_1's rmse: 0.074047\n",
      "[400]\ttraining's rmse: 0.0533216\tvalid_1's rmse: 0.0590268\n",
      "[500]\ttraining's rmse: 0.0432761\tvalid_1's rmse: 0.0490685\n",
      "[600]\ttraining's rmse: 0.0365329\tvalid_1's rmse: 0.0422895\n",
      "[700]\ttraining's rmse: 0.0313226\tvalid_1's rmse: 0.0369772\n",
      "[800]\ttraining's rmse: 0.027566\tvalid_1's rmse: 0.0331384\n",
      "[900]\ttraining's rmse: 0.0246225\tvalid_1's rmse: 0.0301074\n",
      "[1000]\ttraining's rmse: 0.0223539\tvalid_1's rmse: 0.0276986\n",
      "[1100]\ttraining's rmse: 0.0206625\tvalid_1's rmse: 0.0259288\n",
      "[1200]\ttraining's rmse: 0.0193\tvalid_1's rmse: 0.0245046\n",
      "[1300]\ttraining's rmse: 0.0181687\tvalid_1's rmse: 0.0232836\n",
      "[1400]\ttraining's rmse: 0.0172055\tvalid_1's rmse: 0.0222491\n",
      "[1500]\ttraining's rmse: 0.0164214\tvalid_1's rmse: 0.0213981\n",
      "[1600]\ttraining's rmse: 0.0157895\tvalid_1's rmse: 0.0207318\n",
      "[1700]\ttraining's rmse: 0.0152593\tvalid_1's rmse: 0.0201965\n",
      "[1800]\ttraining's rmse: 0.0147819\tvalid_1's rmse: 0.0197228\n",
      "[1900]\ttraining's rmse: 0.0143497\tvalid_1's rmse: 0.0193216\n",
      "[2000]\ttraining's rmse: 0.0139767\tvalid_1's rmse: 0.0189883\n",
      "[2100]\ttraining's rmse: 0.0136447\tvalid_1's rmse: 0.0187227\n",
      "[2200]\ttraining's rmse: 0.0133328\tvalid_1's rmse: 0.0184723\n",
      "[2300]\ttraining's rmse: 0.0130653\tvalid_1's rmse: 0.0182953\n",
      "[2400]\ttraining's rmse: 0.0128254\tvalid_1's rmse: 0.0181458\n",
      "[2500]\ttraining's rmse: 0.0125655\tvalid_1's rmse: 0.0179593\n",
      "[2600]\ttraining's rmse: 0.0123224\tvalid_1's rmse: 0.0177842\n",
      "[2700]\ttraining's rmse: 0.0121105\tvalid_1's rmse: 0.0176615\n",
      "[2800]\ttraining's rmse: 0.0118998\tvalid_1's rmse: 0.0175449\n",
      "[2900]\ttraining's rmse: 0.0117167\tvalid_1's rmse: 0.0174451\n",
      "[3000]\ttraining's rmse: 0.0115152\tvalid_1's rmse: 0.0173103\n",
      "[3100]\ttraining's rmse: 0.0113497\tvalid_1's rmse: 0.0172373\n",
      "[3200]\ttraining's rmse: 0.0111853\tvalid_1's rmse: 0.0171581\n",
      "[3300]\ttraining's rmse: 0.011021\tvalid_1's rmse: 0.0170638\n",
      "[3400]\ttraining's rmse: 0.0108747\tvalid_1's rmse: 0.0170031\n",
      "[3500]\ttraining's rmse: 0.0107195\tvalid_1's rmse: 0.0169411\n",
      "[3600]\ttraining's rmse: 0.0105876\tvalid_1's rmse: 0.0168847\n",
      "[3700]\ttraining's rmse: 0.0104607\tvalid_1's rmse: 0.016838\n",
      "[3800]\ttraining's rmse: 0.0103146\tvalid_1's rmse: 0.0167816\n",
      "[3900]\ttraining's rmse: 0.0101788\tvalid_1's rmse: 0.0167334\n",
      "[4000]\ttraining's rmse: 0.0100492\tvalid_1's rmse: 0.0166915\n",
      "[4100]\ttraining's rmse: 0.00993764\tvalid_1's rmse: 0.016655\n",
      "[4200]\ttraining's rmse: 0.00981796\tvalid_1's rmse: 0.0165986\n",
      "[4300]\ttraining's rmse: 0.00971082\tvalid_1's rmse: 0.0165668\n",
      "[4400]\ttraining's rmse: 0.00959303\tvalid_1's rmse: 0.0165184\n",
      "[4500]\ttraining's rmse: 0.00947445\tvalid_1's rmse: 0.0164782\n",
      "[4600]\ttraining's rmse: 0.00935486\tvalid_1's rmse: 0.0164262\n",
      "[4700]\ttraining's rmse: 0.00924744\tvalid_1's rmse: 0.0163927\n",
      "[4800]\ttraining's rmse: 0.0091398\tvalid_1's rmse: 0.0163524\n",
      "[4900]\ttraining's rmse: 0.00902013\tvalid_1's rmse: 0.0163048\n",
      "[5000]\ttraining's rmse: 0.00892143\tvalid_1's rmse: 0.016282\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00892143\tvalid_1's rmse: 0.016282\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.142837\tvalid_1's rmse: 0.147644\n",
      "[200]\ttraining's rmse: 0.0926013\tvalid_1's rmse: 0.0991104\n",
      "[300]\ttraining's rmse: 0.065683\tvalid_1's rmse: 0.0726075\n",
      "[400]\ttraining's rmse: 0.0507797\tvalid_1's rmse: 0.0576494\n",
      "[500]\ttraining's rmse: 0.0422087\tvalid_1's rmse: 0.0489583\n",
      "[600]\ttraining's rmse: 0.0360947\tvalid_1's rmse: 0.0427695\n",
      "[700]\ttraining's rmse: 0.031322\tvalid_1's rmse: 0.0379359\n",
      "[800]\ttraining's rmse: 0.0278885\tvalid_1's rmse: 0.0345846\n",
      "[900]\ttraining's rmse: 0.0252986\tvalid_1's rmse: 0.032103\n",
      "[1000]\ttraining's rmse: 0.0232011\tvalid_1's rmse: 0.0300282\n",
      "[1100]\ttraining's rmse: 0.0213993\tvalid_1's rmse: 0.0282116\n",
      "[1200]\ttraining's rmse: 0.0199224\tvalid_1's rmse: 0.026735\n",
      "[1300]\ttraining's rmse: 0.0187444\tvalid_1's rmse: 0.0255449\n",
      "[1400]\ttraining's rmse: 0.01773\tvalid_1's rmse: 0.0245116\n",
      "[1500]\ttraining's rmse: 0.0169039\tvalid_1's rmse: 0.023708\n",
      "[1600]\ttraining's rmse: 0.0161998\tvalid_1's rmse: 0.0230544\n",
      "[1700]\ttraining's rmse: 0.0155709\tvalid_1's rmse: 0.0224535\n",
      "[1800]\ttraining's rmse: 0.0149489\tvalid_1's rmse: 0.0218734\n",
      "[1900]\ttraining's rmse: 0.0144762\tvalid_1's rmse: 0.0214268\n",
      "[2000]\ttraining's rmse: 0.0140209\tvalid_1's rmse: 0.0210354\n",
      "[2100]\ttraining's rmse: 0.0136495\tvalid_1's rmse: 0.0207387\n",
      "[2200]\ttraining's rmse: 0.0132672\tvalid_1's rmse: 0.0204342\n",
      "[2300]\ttraining's rmse: 0.0129135\tvalid_1's rmse: 0.0201674\n",
      "[2400]\ttraining's rmse: 0.0125911\tvalid_1's rmse: 0.0199054\n",
      "[2500]\ttraining's rmse: 0.0122859\tvalid_1's rmse: 0.0196877\n",
      "[2600]\ttraining's rmse: 0.0120027\tvalid_1's rmse: 0.019489\n",
      "[2700]\ttraining's rmse: 0.011743\tvalid_1's rmse: 0.0193256\n",
      "[2800]\ttraining's rmse: 0.0115174\tvalid_1's rmse: 0.0191877\n",
      "[2900]\ttraining's rmse: 0.0112776\tvalid_1's rmse: 0.0190322\n",
      "[3000]\ttraining's rmse: 0.0110711\tvalid_1's rmse: 0.018917\n",
      "[3100]\ttraining's rmse: 0.0108849\tvalid_1's rmse: 0.0188168\n",
      "[3200]\ttraining's rmse: 0.0107023\tvalid_1's rmse: 0.0187349\n",
      "[3300]\ttraining's rmse: 0.0105085\tvalid_1's rmse: 0.0186365\n",
      "[3400]\ttraining's rmse: 0.010321\tvalid_1's rmse: 0.0185446\n",
      "[3500]\ttraining's rmse: 0.0101536\tvalid_1's rmse: 0.01848\n",
      "[3600]\ttraining's rmse: 0.00998193\tvalid_1's rmse: 0.0183825\n",
      "[3700]\ttraining's rmse: 0.00981625\tvalid_1's rmse: 0.0183035\n",
      "[3800]\ttraining's rmse: 0.00967003\tvalid_1's rmse: 0.0182335\n",
      "[3900]\ttraining's rmse: 0.00954399\tvalid_1's rmse: 0.0181856\n",
      "[4000]\ttraining's rmse: 0.00939199\tvalid_1's rmse: 0.0181226\n",
      "[4100]\ttraining's rmse: 0.00926617\tvalid_1's rmse: 0.0180766\n",
      "[4200]\ttraining's rmse: 0.00914447\tvalid_1's rmse: 0.0180285\n",
      "[4300]\ttraining's rmse: 0.00900598\tvalid_1's rmse: 0.0179604\n",
      "[4400]\ttraining's rmse: 0.0088915\tvalid_1's rmse: 0.0179222\n",
      "[4500]\ttraining's rmse: 0.00877828\tvalid_1's rmse: 0.017883\n",
      "[4600]\ttraining's rmse: 0.00864491\tvalid_1's rmse: 0.0178286\n",
      "[4700]\ttraining's rmse: 0.00851778\tvalid_1's rmse: 0.0177773\n",
      "[4800]\ttraining's rmse: 0.00841699\tvalid_1's rmse: 0.017745\n",
      "[4900]\ttraining's rmse: 0.00829918\tvalid_1's rmse: 0.0177024\n",
      "[5000]\ttraining's rmse: 0.00819423\tvalid_1's rmse: 0.017671\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\ttraining's rmse: 0.0081954\tvalid_1's rmse: 0.017671\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.140294\tvalid_1's rmse: 0.140149\n",
      "[200]\ttraining's rmse: 0.0902768\tvalid_1's rmse: 0.0916493\n",
      "[300]\ttraining's rmse: 0.062778\tvalid_1's rmse: 0.0651171\n",
      "[400]\ttraining's rmse: 0.0494638\tvalid_1's rmse: 0.0525173\n",
      "[500]\ttraining's rmse: 0.0406729\tvalid_1's rmse: 0.0442577\n",
      "[600]\ttraining's rmse: 0.0346173\tvalid_1's rmse: 0.0384969\n",
      "[700]\ttraining's rmse: 0.0298629\tvalid_1's rmse: 0.0339965\n",
      "[800]\ttraining's rmse: 0.0264602\tvalid_1's rmse: 0.030756\n",
      "[900]\ttraining's rmse: 0.0238002\tvalid_1's rmse: 0.0282435\n",
      "[1000]\ttraining's rmse: 0.0217106\tvalid_1's rmse: 0.0262602\n",
      "[1100]\ttraining's rmse: 0.0200976\tvalid_1's rmse: 0.0246976\n",
      "[1200]\ttraining's rmse: 0.0187748\tvalid_1's rmse: 0.0234521\n",
      "[1300]\ttraining's rmse: 0.0176333\tvalid_1's rmse: 0.0223331\n",
      "[1400]\ttraining's rmse: 0.0166733\tvalid_1's rmse: 0.0214197\n",
      "[1500]\ttraining's rmse: 0.0158911\tvalid_1's rmse: 0.0206646\n",
      "[1600]\ttraining's rmse: 0.0152197\tvalid_1's rmse: 0.0200154\n",
      "[1700]\ttraining's rmse: 0.0146454\tvalid_1's rmse: 0.0195069\n",
      "[1800]\ttraining's rmse: 0.0141372\tvalid_1's rmse: 0.019066\n",
      "[1900]\ttraining's rmse: 0.0136986\tvalid_1's rmse: 0.0187035\n",
      "[2000]\ttraining's rmse: 0.0132853\tvalid_1's rmse: 0.0183579\n",
      "[2100]\ttraining's rmse: 0.0129358\tvalid_1's rmse: 0.0180797\n",
      "[2200]\ttraining's rmse: 0.0126083\tvalid_1's rmse: 0.0178472\n",
      "[2300]\ttraining's rmse: 0.0123152\tvalid_1's rmse: 0.0176523\n",
      "[2400]\ttraining's rmse: 0.0120376\tvalid_1's rmse: 0.0174717\n",
      "[2500]\ttraining's rmse: 0.011787\tvalid_1's rmse: 0.0173161\n",
      "[2600]\ttraining's rmse: 0.0115396\tvalid_1's rmse: 0.01716\n",
      "[2700]\ttraining's rmse: 0.0113052\tvalid_1's rmse: 0.0170212\n",
      "[2800]\ttraining's rmse: 0.0110923\tvalid_1's rmse: 0.0169141\n",
      "[2900]\ttraining's rmse: 0.0108716\tvalid_1's rmse: 0.0167906\n",
      "[3000]\ttraining's rmse: 0.0106806\tvalid_1's rmse: 0.0166867\n",
      "[3100]\ttraining's rmse: 0.0105048\tvalid_1's rmse: 0.0166152\n",
      "[3200]\ttraining's rmse: 0.0103138\tvalid_1's rmse: 0.0165093\n",
      "[3300]\ttraining's rmse: 0.0101612\tvalid_1's rmse: 0.0164331\n",
      "[3400]\ttraining's rmse: 0.00998097\tvalid_1's rmse: 0.0163363\n",
      "[3500]\ttraining's rmse: 0.00982664\tvalid_1's rmse: 0.0162621\n",
      "[3600]\ttraining's rmse: 0.00967852\tvalid_1's rmse: 0.0161872\n",
      "[3700]\ttraining's rmse: 0.00953207\tvalid_1's rmse: 0.0161276\n",
      "[3800]\ttraining's rmse: 0.00939281\tvalid_1's rmse: 0.0160668\n",
      "[3900]\ttraining's rmse: 0.00926496\tvalid_1's rmse: 0.0160189\n",
      "[4000]\ttraining's rmse: 0.00912744\tvalid_1's rmse: 0.0159565\n",
      "[4100]\ttraining's rmse: 0.00899615\tvalid_1's rmse: 0.0158944\n",
      "[4200]\ttraining's rmse: 0.00887937\tvalid_1's rmse: 0.0158472\n",
      "[4300]\ttraining's rmse: 0.00875995\tvalid_1's rmse: 0.0158059\n",
      "[4400]\ttraining's rmse: 0.00865397\tvalid_1's rmse: 0.0157788\n",
      "[4500]\ttraining's rmse: 0.00853337\tvalid_1's rmse: 0.0157262\n",
      "[4600]\ttraining's rmse: 0.00842348\tvalid_1's rmse: 0.0156822\n",
      "[4700]\ttraining's rmse: 0.00832035\tvalid_1's rmse: 0.0156433\n",
      "[4800]\ttraining's rmse: 0.00821672\tvalid_1's rmse: 0.015611\n",
      "[4900]\ttraining's rmse: 0.0081249\tvalid_1's rmse: 0.0155814\n",
      "[5000]\ttraining's rmse: 0.00802107\tvalid_1's rmse: 0.0155487\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4995]\ttraining's rmse: 0.00802626\tvalid_1's rmse: 0.0155486\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.154972\tvalid_1's rmse: 0.157976\n",
      "[200]\ttraining's rmse: 0.100856\tvalid_1's rmse: 0.106017\n",
      "[300]\ttraining's rmse: 0.0690406\tvalid_1's rmse: 0.0750913\n",
      "[400]\ttraining's rmse: 0.0534837\tvalid_1's rmse: 0.0598034\n",
      "[500]\ttraining's rmse: 0.0433636\tvalid_1's rmse: 0.0495931\n",
      "[600]\ttraining's rmse: 0.0365971\tvalid_1's rmse: 0.0427068\n",
      "[700]\ttraining's rmse: 0.0313561\tvalid_1's rmse: 0.0372753\n",
      "[800]\ttraining's rmse: 0.0275651\tvalid_1's rmse: 0.0333381\n",
      "[900]\ttraining's rmse: 0.0245938\tvalid_1's rmse: 0.0302885\n",
      "[1000]\ttraining's rmse: 0.0223299\tvalid_1's rmse: 0.0278438\n",
      "[1100]\ttraining's rmse: 0.0206281\tvalid_1's rmse: 0.0260839\n",
      "[1200]\ttraining's rmse: 0.0192792\tvalid_1's rmse: 0.0246551\n",
      "[1300]\ttraining's rmse: 0.0181391\tvalid_1's rmse: 0.0234613\n",
      "[1400]\ttraining's rmse: 0.017184\tvalid_1's rmse: 0.0224648\n",
      "[1500]\ttraining's rmse: 0.0164294\tvalid_1's rmse: 0.0216453\n",
      "[1600]\ttraining's rmse: 0.0157984\tvalid_1's rmse: 0.0210371\n",
      "[1700]\ttraining's rmse: 0.0152859\tvalid_1's rmse: 0.0205708\n",
      "[1800]\ttraining's rmse: 0.0148073\tvalid_1's rmse: 0.020139\n",
      "[1900]\ttraining's rmse: 0.0143818\tvalid_1's rmse: 0.0197531\n",
      "[2000]\ttraining's rmse: 0.0139885\tvalid_1's rmse: 0.0194566\n",
      "[2100]\ttraining's rmse: 0.0136572\tvalid_1's rmse: 0.0192157\n",
      "[2200]\ttraining's rmse: 0.0133396\tvalid_1's rmse: 0.018981\n",
      "[2300]\ttraining's rmse: 0.0130706\tvalid_1's rmse: 0.0188086\n",
      "[2400]\ttraining's rmse: 0.0128004\tvalid_1's rmse: 0.0186257\n",
      "[2500]\ttraining's rmse: 0.0125236\tvalid_1's rmse: 0.0184409\n",
      "[2600]\ttraining's rmse: 0.0122722\tvalid_1's rmse: 0.018274\n",
      "[2700]\ttraining's rmse: 0.0120383\tvalid_1's rmse: 0.0181261\n",
      "[2800]\ttraining's rmse: 0.0118397\tvalid_1's rmse: 0.0180132\n",
      "[2900]\ttraining's rmse: 0.0116402\tvalid_1's rmse: 0.0179084\n",
      "[3000]\ttraining's rmse: 0.0114442\tvalid_1's rmse: 0.0178074\n",
      "[3100]\ttraining's rmse: 0.0112694\tvalid_1's rmse: 0.0177216\n",
      "[3200]\ttraining's rmse: 0.0110728\tvalid_1's rmse: 0.0176175\n",
      "[3300]\ttraining's rmse: 0.010903\tvalid_1's rmse: 0.0175225\n",
      "[3400]\ttraining's rmse: 0.0107443\tvalid_1's rmse: 0.0174637\n",
      "[3500]\ttraining's rmse: 0.0105905\tvalid_1's rmse: 0.017393\n",
      "[3600]\ttraining's rmse: 0.0104256\tvalid_1's rmse: 0.0173358\n",
      "[3700]\ttraining's rmse: 0.0102821\tvalid_1's rmse: 0.0172734\n",
      "[3800]\ttraining's rmse: 0.0101171\tvalid_1's rmse: 0.0171828\n",
      "[3900]\ttraining's rmse: 0.00997372\tvalid_1's rmse: 0.0171253\n",
      "[4000]\ttraining's rmse: 0.00985104\tvalid_1's rmse: 0.017085\n",
      "[4100]\ttraining's rmse: 0.00972319\tvalid_1's rmse: 0.0170386\n",
      "[4200]\ttraining's rmse: 0.00959871\tvalid_1's rmse: 0.016991\n",
      "[4300]\ttraining's rmse: 0.00947869\tvalid_1's rmse: 0.0169563\n",
      "[4400]\ttraining's rmse: 0.00936097\tvalid_1's rmse: 0.0169066\n",
      "[4500]\ttraining's rmse: 0.00923678\tvalid_1's rmse: 0.0168548\n",
      "[4600]\ttraining's rmse: 0.00911877\tvalid_1's rmse: 0.0168024\n",
      "[4700]\ttraining's rmse: 0.00900299\tvalid_1's rmse: 0.0167607\n",
      "[4800]\ttraining's rmse: 0.00888897\tvalid_1's rmse: 0.0167238\n",
      "[4900]\ttraining's rmse: 0.00878909\tvalid_1's rmse: 0.0166994\n",
      "[5000]\ttraining's rmse: 0.00868245\tvalid_1's rmse: 0.016662\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00868245\tvalid_1's rmse: 0.016662\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.141589\tvalid_1's rmse: 0.142305\n",
      "[200]\ttraining's rmse: 0.087748\tvalid_1's rmse: 0.0902008\n",
      "[300]\ttraining's rmse: 0.0607723\tvalid_1's rmse: 0.0640534\n",
      "[400]\ttraining's rmse: 0.0460986\tvalid_1's rmse: 0.0498083\n",
      "[500]\ttraining's rmse: 0.0366497\tvalid_1's rmse: 0.0404478\n",
      "[600]\ttraining's rmse: 0.0306915\tvalid_1's rmse: 0.0344435\n",
      "[700]\ttraining's rmse: 0.0268718\tvalid_1's rmse: 0.0305751\n",
      "[800]\ttraining's rmse: 0.0240685\tvalid_1's rmse: 0.0277221\n",
      "[900]\ttraining's rmse: 0.0219631\tvalid_1's rmse: 0.0256018\n",
      "[1000]\ttraining's rmse: 0.0202769\tvalid_1's rmse: 0.0238595\n",
      "[1100]\ttraining's rmse: 0.018883\tvalid_1's rmse: 0.0224417\n",
      "[1200]\ttraining's rmse: 0.0178538\tvalid_1's rmse: 0.021405\n",
      "[1300]\ttraining's rmse: 0.017046\tvalid_1's rmse: 0.0206021\n",
      "[1400]\ttraining's rmse: 0.016339\tvalid_1's rmse: 0.0199157\n",
      "[1500]\ttraining's rmse: 0.0157655\tvalid_1's rmse: 0.0193764\n",
      "[1600]\ttraining's rmse: 0.0152682\tvalid_1's rmse: 0.018938\n",
      "[1700]\ttraining's rmse: 0.0148443\tvalid_1's rmse: 0.0185852\n",
      "[1800]\ttraining's rmse: 0.0144522\tvalid_1's rmse: 0.0182881\n",
      "[1900]\ttraining's rmse: 0.0140809\tvalid_1's rmse: 0.0180191\n",
      "[2000]\ttraining's rmse: 0.0137324\tvalid_1's rmse: 0.0177592\n",
      "[2100]\ttraining's rmse: 0.0134387\tvalid_1's rmse: 0.0175487\n",
      "[2200]\ttraining's rmse: 0.0131626\tvalid_1's rmse: 0.0173805\n",
      "[2300]\ttraining's rmse: 0.0129029\tvalid_1's rmse: 0.017226\n",
      "[2400]\ttraining's rmse: 0.0126416\tvalid_1's rmse: 0.0170612\n",
      "[2500]\ttraining's rmse: 0.0123858\tvalid_1's rmse: 0.0168987\n",
      "[2600]\ttraining's rmse: 0.0121823\tvalid_1's rmse: 0.0167749\n",
      "[2700]\ttraining's rmse: 0.0119649\tvalid_1's rmse: 0.0166579\n",
      "[2800]\ttraining's rmse: 0.0117766\tvalid_1's rmse: 0.0165678\n",
      "[2900]\ttraining's rmse: 0.011591\tvalid_1's rmse: 0.0164774\n",
      "[3000]\ttraining's rmse: 0.0114185\tvalid_1's rmse: 0.0163967\n",
      "[3100]\ttraining's rmse: 0.011242\tvalid_1's rmse: 0.0162996\n",
      "[3200]\ttraining's rmse: 0.0110992\tvalid_1's rmse: 0.016243\n",
      "[3300]\ttraining's rmse: 0.010923\tvalid_1's rmse: 0.0161452\n",
      "[3400]\ttraining's rmse: 0.0107539\tvalid_1's rmse: 0.0160521\n",
      "[3500]\ttraining's rmse: 0.0106227\tvalid_1's rmse: 0.0159954\n",
      "[3600]\ttraining's rmse: 0.0104809\tvalid_1's rmse: 0.0159346\n",
      "[3700]\ttraining's rmse: 0.0103525\tvalid_1's rmse: 0.0158897\n",
      "[3800]\ttraining's rmse: 0.0102249\tvalid_1's rmse: 0.0158487\n",
      "[3900]\ttraining's rmse: 0.0100843\tvalid_1's rmse: 0.0157794\n",
      "[4000]\ttraining's rmse: 0.00995395\tvalid_1's rmse: 0.0157297\n",
      "[4100]\ttraining's rmse: 0.0098367\tvalid_1's rmse: 0.0156797\n",
      "[4200]\ttraining's rmse: 0.0097\tvalid_1's rmse: 0.0156201\n",
      "[4300]\ttraining's rmse: 0.00955564\tvalid_1's rmse: 0.0155409\n",
      "[4400]\ttraining's rmse: 0.00944158\tvalid_1's rmse: 0.015493\n",
      "[4500]\ttraining's rmse: 0.00931814\tvalid_1's rmse: 0.015442\n",
      "[4600]\ttraining's rmse: 0.0092047\tvalid_1's rmse: 0.0153962\n",
      "[4700]\ttraining's rmse: 0.00911292\tvalid_1's rmse: 0.0153752\n",
      "[4800]\ttraining's rmse: 0.00899001\tvalid_1's rmse: 0.015317\n",
      "[4900]\ttraining's rmse: 0.00888837\tvalid_1's rmse: 0.015285\n",
      "[5000]\ttraining's rmse: 0.0087733\tvalid_1's rmse: 0.0152387\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\ttraining's rmse: 0.00877389\tvalid_1's rmse: 0.0152386\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.141153\tvalid_1's rmse: 0.144051\n",
      "[200]\ttraining's rmse: 0.0862776\tvalid_1's rmse: 0.0905235\n",
      "[300]\ttraining's rmse: 0.0600039\tvalid_1's rmse: 0.0643781\n",
      "[400]\ttraining's rmse: 0.0461189\tvalid_1's rmse: 0.0502483\n",
      "[500]\ttraining's rmse: 0.0372211\tvalid_1's rmse: 0.0411013\n",
      "[600]\ttraining's rmse: 0.0315555\tvalid_1's rmse: 0.0353389\n",
      "[700]\ttraining's rmse: 0.0278499\tvalid_1's rmse: 0.0316114\n",
      "[800]\ttraining's rmse: 0.0251536\tvalid_1's rmse: 0.0289805\n",
      "[900]\ttraining's rmse: 0.0230484\tvalid_1's rmse: 0.0269291\n",
      "[1000]\ttraining's rmse: 0.0212979\tvalid_1's rmse: 0.025203\n",
      "[1100]\ttraining's rmse: 0.0198902\tvalid_1's rmse: 0.0238221\n",
      "[1200]\ttraining's rmse: 0.0188121\tvalid_1's rmse: 0.0227815\n",
      "[1300]\ttraining's rmse: 0.0179405\tvalid_1's rmse: 0.0219639\n",
      "[1400]\ttraining's rmse: 0.0171909\tvalid_1's rmse: 0.0212355\n",
      "[1500]\ttraining's rmse: 0.016561\tvalid_1's rmse: 0.0206755\n",
      "[1600]\ttraining's rmse: 0.0160143\tvalid_1's rmse: 0.0202294\n",
      "[1700]\ttraining's rmse: 0.0154847\tvalid_1's rmse: 0.0197841\n",
      "[1800]\ttraining's rmse: 0.0150428\tvalid_1's rmse: 0.0194368\n",
      "[1900]\ttraining's rmse: 0.0146741\tvalid_1's rmse: 0.0191824\n",
      "[2000]\ttraining's rmse: 0.014277\tvalid_1's rmse: 0.0188624\n",
      "[2100]\ttraining's rmse: 0.0139863\tvalid_1's rmse: 0.0186995\n",
      "[2200]\ttraining's rmse: 0.0136802\tvalid_1's rmse: 0.0185003\n",
      "[2300]\ttraining's rmse: 0.0133963\tvalid_1's rmse: 0.0183216\n",
      "[2400]\ttraining's rmse: 0.013127\tvalid_1's rmse: 0.0181556\n",
      "[2500]\ttraining's rmse: 0.0128756\tvalid_1's rmse: 0.0180167\n",
      "[2600]\ttraining's rmse: 0.0126536\tvalid_1's rmse: 0.0178895\n",
      "[2700]\ttraining's rmse: 0.0124318\tvalid_1's rmse: 0.0177725\n",
      "[2800]\ttraining's rmse: 0.0122195\tvalid_1's rmse: 0.017654\n",
      "[2900]\ttraining's rmse: 0.0120193\tvalid_1's rmse: 0.0175466\n",
      "[3000]\ttraining's rmse: 0.0118039\tvalid_1's rmse: 0.017418\n",
      "[3100]\ttraining's rmse: 0.0116044\tvalid_1's rmse: 0.0173063\n",
      "[3200]\ttraining's rmse: 0.0114478\tvalid_1's rmse: 0.0172507\n",
      "[3300]\ttraining's rmse: 0.0112708\tvalid_1's rmse: 0.0171525\n",
      "[3400]\ttraining's rmse: 0.0110909\tvalid_1's rmse: 0.0170702\n",
      "[3500]\ttraining's rmse: 0.0108918\tvalid_1's rmse: 0.0169554\n",
      "[3600]\ttraining's rmse: 0.0107456\tvalid_1's rmse: 0.0168968\n",
      "[3700]\ttraining's rmse: 0.0106118\tvalid_1's rmse: 0.0168407\n",
      "[3800]\ttraining's rmse: 0.0104571\tvalid_1's rmse: 0.0167669\n",
      "[3900]\ttraining's rmse: 0.0103098\tvalid_1's rmse: 0.0167059\n",
      "[4000]\ttraining's rmse: 0.0101679\tvalid_1's rmse: 0.0166457\n",
      "[4100]\ttraining's rmse: 0.010023\tvalid_1's rmse: 0.0166078\n",
      "[4200]\ttraining's rmse: 0.00988299\tvalid_1's rmse: 0.0165385\n",
      "[4300]\ttraining's rmse: 0.00977525\tvalid_1's rmse: 0.0165009\n",
      "[4400]\ttraining's rmse: 0.00965889\tvalid_1's rmse: 0.0164594\n",
      "[4500]\ttraining's rmse: 0.00953518\tvalid_1's rmse: 0.0164161\n",
      "[4600]\ttraining's rmse: 0.00941788\tvalid_1's rmse: 0.0163852\n",
      "[4700]\ttraining's rmse: 0.00929859\tvalid_1's rmse: 0.0163559\n",
      "[4800]\ttraining's rmse: 0.00918246\tvalid_1's rmse: 0.0163151\n",
      "[4900]\ttraining's rmse: 0.00907906\tvalid_1's rmse: 0.0162884\n",
      "[5000]\ttraining's rmse: 0.00896983\tvalid_1's rmse: 0.0162528\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00896983\tvalid_1's rmse: 0.0162528\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.142432\tvalid_1's rmse: 0.144499\n",
      "[200]\ttraining's rmse: 0.0919698\tvalid_1's rmse: 0.0952419\n",
      "[300]\ttraining's rmse: 0.0636839\tvalid_1's rmse: 0.0675931\n",
      "[400]\ttraining's rmse: 0.049859\tvalid_1's rmse: 0.0542888\n",
      "[500]\ttraining's rmse: 0.0407866\tvalid_1's rmse: 0.0454528\n",
      "[600]\ttraining's rmse: 0.0346461\tvalid_1's rmse: 0.0394829\n",
      "[700]\ttraining's rmse: 0.0299242\tvalid_1's rmse: 0.0348431\n",
      "[800]\ttraining's rmse: 0.0265914\tvalid_1's rmse: 0.0316368\n",
      "[900]\ttraining's rmse: 0.0239804\tvalid_1's rmse: 0.0291454\n",
      "[1000]\ttraining's rmse: 0.0219502\tvalid_1's rmse: 0.0272022\n",
      "[1100]\ttraining's rmse: 0.0204018\tvalid_1's rmse: 0.0257477\n",
      "[1200]\ttraining's rmse: 0.0191083\tvalid_1's rmse: 0.0245184\n",
      "[1300]\ttraining's rmse: 0.0179895\tvalid_1's rmse: 0.0234752\n",
      "[1400]\ttraining's rmse: 0.0170426\tvalid_1's rmse: 0.0225817\n",
      "[1500]\ttraining's rmse: 0.0162365\tvalid_1's rmse: 0.021844\n",
      "[1600]\ttraining's rmse: 0.0155473\tvalid_1's rmse: 0.0212234\n",
      "[1700]\ttraining's rmse: 0.0149472\tvalid_1's rmse: 0.0206981\n",
      "[1800]\ttraining's rmse: 0.0144321\tvalid_1's rmse: 0.0202862\n",
      "[1900]\ttraining's rmse: 0.0139539\tvalid_1's rmse: 0.0199125\n",
      "[2000]\ttraining's rmse: 0.013516\tvalid_1's rmse: 0.0195682\n",
      "[2100]\ttraining's rmse: 0.0131268\tvalid_1's rmse: 0.0192699\n",
      "[2200]\ttraining's rmse: 0.0127925\tvalid_1's rmse: 0.0190101\n",
      "[2300]\ttraining's rmse: 0.0124407\tvalid_1's rmse: 0.0187803\n",
      "[2400]\ttraining's rmse: 0.0121351\tvalid_1's rmse: 0.0185776\n",
      "[2500]\ttraining's rmse: 0.0118514\tvalid_1's rmse: 0.0184203\n",
      "[2600]\ttraining's rmse: 0.0115624\tvalid_1's rmse: 0.0182408\n",
      "[2700]\ttraining's rmse: 0.0113255\tvalid_1's rmse: 0.0180906\n",
      "[2800]\ttraining's rmse: 0.0110829\tvalid_1's rmse: 0.0179437\n",
      "[2900]\ttraining's rmse: 0.010868\tvalid_1's rmse: 0.0178192\n",
      "[3000]\ttraining's rmse: 0.0106613\tvalid_1's rmse: 0.0176921\n",
      "[3100]\ttraining's rmse: 0.0104643\tvalid_1's rmse: 0.0176198\n",
      "[3200]\ttraining's rmse: 0.0102591\tvalid_1's rmse: 0.0175046\n",
      "[3300]\ttraining's rmse: 0.0100807\tvalid_1's rmse: 0.0174076\n",
      "[3400]\ttraining's rmse: 0.0099025\tvalid_1's rmse: 0.0173192\n",
      "[3500]\ttraining's rmse: 0.00974002\tvalid_1's rmse: 0.0172449\n",
      "[3600]\ttraining's rmse: 0.00958695\tvalid_1's rmse: 0.0171879\n",
      "[3700]\ttraining's rmse: 0.00943823\tvalid_1's rmse: 0.0171315\n",
      "[3800]\ttraining's rmse: 0.0092836\tvalid_1's rmse: 0.0170558\n",
      "[3900]\ttraining's rmse: 0.00913802\tvalid_1's rmse: 0.0170031\n",
      "[4000]\ttraining's rmse: 0.0090077\tvalid_1's rmse: 0.0169572\n",
      "[4100]\ttraining's rmse: 0.00885359\tvalid_1's rmse: 0.0168892\n",
      "[4200]\ttraining's rmse: 0.00872087\tvalid_1's rmse: 0.0168377\n",
      "[4300]\ttraining's rmse: 0.00859586\tvalid_1's rmse: 0.0167903\n",
      "[4400]\ttraining's rmse: 0.00847862\tvalid_1's rmse: 0.016746\n",
      "[4500]\ttraining's rmse: 0.00836557\tvalid_1's rmse: 0.0166975\n",
      "[4600]\ttraining's rmse: 0.00825264\tvalid_1's rmse: 0.0166721\n",
      "[4700]\ttraining's rmse: 0.0081444\tvalid_1's rmse: 0.0166271\n",
      "[4800]\ttraining's rmse: 0.00804348\tvalid_1's rmse: 0.0165921\n",
      "[4900]\ttraining's rmse: 0.00793962\tvalid_1's rmse: 0.0165536\n",
      "[5000]\ttraining's rmse: 0.00783242\tvalid_1's rmse: 0.0165158\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4982]\ttraining's rmse: 0.00784605\tvalid_1's rmse: 0.0165133\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.147427\tvalid_1's rmse: 0.147599\n",
      "[200]\ttraining's rmse: 0.0959557\tvalid_1's rmse: 0.098096\n",
      "[300]\ttraining's rmse: 0.0665513\tvalid_1's rmse: 0.0697646\n",
      "[400]\ttraining's rmse: 0.0521583\tvalid_1's rmse: 0.0559233\n",
      "[500]\ttraining's rmse: 0.0427827\tvalid_1's rmse: 0.0468142\n",
      "[600]\ttraining's rmse: 0.0364502\tvalid_1's rmse: 0.0406295\n",
      "[700]\ttraining's rmse: 0.0314777\tvalid_1's rmse: 0.0357596\n",
      "[800]\ttraining's rmse: 0.0278339\tvalid_1's rmse: 0.0321645\n",
      "[900]\ttraining's rmse: 0.024934\tvalid_1's rmse: 0.0293485\n",
      "[1000]\ttraining's rmse: 0.0226386\tvalid_1's rmse: 0.0270615\n",
      "[1100]\ttraining's rmse: 0.020856\tvalid_1's rmse: 0.0253394\n",
      "[1200]\ttraining's rmse: 0.0194003\tvalid_1's rmse: 0.0238813\n",
      "[1300]\ttraining's rmse: 0.0181269\tvalid_1's rmse: 0.0226337\n",
      "[1400]\ttraining's rmse: 0.0170576\tvalid_1's rmse: 0.0215217\n",
      "[1500]\ttraining's rmse: 0.016172\tvalid_1's rmse: 0.0206213\n",
      "[1600]\ttraining's rmse: 0.0154342\tvalid_1's rmse: 0.0199268\n",
      "[1700]\ttraining's rmse: 0.0148084\tvalid_1's rmse: 0.0193136\n",
      "[1800]\ttraining's rmse: 0.0142391\tvalid_1's rmse: 0.0187889\n",
      "[1900]\ttraining's rmse: 0.0137554\tvalid_1's rmse: 0.0183425\n",
      "[2000]\ttraining's rmse: 0.0132943\tvalid_1's rmse: 0.0179182\n",
      "[2100]\ttraining's rmse: 0.0129035\tvalid_1's rmse: 0.0175861\n",
      "[2200]\ttraining's rmse: 0.0125612\tvalid_1's rmse: 0.0173152\n",
      "[2300]\ttraining's rmse: 0.0122373\tvalid_1's rmse: 0.0170664\n",
      "[2400]\ttraining's rmse: 0.011927\tvalid_1's rmse: 0.0168387\n",
      "[2500]\ttraining's rmse: 0.0116602\tvalid_1's rmse: 0.0166298\n",
      "[2600]\ttraining's rmse: 0.0113932\tvalid_1's rmse: 0.0164381\n",
      "[2700]\ttraining's rmse: 0.0111296\tvalid_1's rmse: 0.0162458\n",
      "[2800]\ttraining's rmse: 0.0109005\tvalid_1's rmse: 0.0161003\n",
      "[2900]\ttraining's rmse: 0.010681\tvalid_1's rmse: 0.0159624\n",
      "[3000]\ttraining's rmse: 0.0104832\tvalid_1's rmse: 0.015853\n",
      "[3100]\ttraining's rmse: 0.0102938\tvalid_1's rmse: 0.0157526\n",
      "[3200]\ttraining's rmse: 0.010105\tvalid_1's rmse: 0.0156324\n",
      "[3300]\ttraining's rmse: 0.00993476\tvalid_1's rmse: 0.0155329\n",
      "[3400]\ttraining's rmse: 0.00976614\tvalid_1's rmse: 0.0154443\n",
      "[3500]\ttraining's rmse: 0.00962175\tvalid_1's rmse: 0.0153824\n",
      "[3600]\ttraining's rmse: 0.00947531\tvalid_1's rmse: 0.0153065\n",
      "[3700]\ttraining's rmse: 0.00932546\tvalid_1's rmse: 0.0152464\n",
      "[3800]\ttraining's rmse: 0.00918908\tvalid_1's rmse: 0.0151826\n",
      "[3900]\ttraining's rmse: 0.00904141\tvalid_1's rmse: 0.0151176\n",
      "[4000]\ttraining's rmse: 0.00891198\tvalid_1's rmse: 0.0150569\n",
      "[4100]\ttraining's rmse: 0.00876926\tvalid_1's rmse: 0.0149849\n",
      "[4200]\ttraining's rmse: 0.00864786\tvalid_1's rmse: 0.0149311\n",
      "[4300]\ttraining's rmse: 0.0085334\tvalid_1's rmse: 0.0148878\n",
      "[4400]\ttraining's rmse: 0.00842181\tvalid_1's rmse: 0.0148379\n",
      "[4500]\ttraining's rmse: 0.00831513\tvalid_1's rmse: 0.0147943\n",
      "[4600]\ttraining's rmse: 0.00822196\tvalid_1's rmse: 0.014768\n",
      "[4700]\ttraining's rmse: 0.00811459\tvalid_1's rmse: 0.0147281\n",
      "[4800]\ttraining's rmse: 0.00801041\tvalid_1's rmse: 0.0146857\n",
      "[4900]\ttraining's rmse: 0.00791316\tvalid_1's rmse: 0.014657\n",
      "[5000]\ttraining's rmse: 0.00781191\tvalid_1's rmse: 0.0146189\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00781191\tvalid_1's rmse: 0.0146189\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.139028\tvalid_1's rmse: 0.141671\n",
      "[200]\ttraining's rmse: 0.0861084\tvalid_1's rmse: 0.0897617\n",
      "[300]\ttraining's rmse: 0.0608002\tvalid_1's rmse: 0.0651312\n",
      "[400]\ttraining's rmse: 0.0473553\tvalid_1's rmse: 0.0519887\n",
      "[500]\ttraining's rmse: 0.0386801\tvalid_1's rmse: 0.0434161\n",
      "[600]\ttraining's rmse: 0.0329662\tvalid_1's rmse: 0.0377509\n",
      "[700]\ttraining's rmse: 0.0290691\tvalid_1's rmse: 0.0338666\n",
      "[800]\ttraining's rmse: 0.0260925\tvalid_1's rmse: 0.03094\n",
      "[900]\ttraining's rmse: 0.0237548\tvalid_1's rmse: 0.0285902\n",
      "[1000]\ttraining's rmse: 0.0217869\tvalid_1's rmse: 0.0265994\n",
      "[1100]\ttraining's rmse: 0.0201169\tvalid_1's rmse: 0.0249005\n",
      "[1200]\ttraining's rmse: 0.0188239\tvalid_1's rmse: 0.0235788\n",
      "[1300]\ttraining's rmse: 0.0177512\tvalid_1's rmse: 0.0225231\n",
      "[1400]\ttraining's rmse: 0.0168728\tvalid_1's rmse: 0.0216579\n",
      "[1500]\ttraining's rmse: 0.0161412\tvalid_1's rmse: 0.0209508\n",
      "[1600]\ttraining's rmse: 0.0155051\tvalid_1's rmse: 0.0203851\n",
      "[1700]\ttraining's rmse: 0.0149804\tvalid_1's rmse: 0.0199381\n",
      "[1800]\ttraining's rmse: 0.0145214\tvalid_1's rmse: 0.0195664\n",
      "[1900]\ttraining's rmse: 0.0141054\tvalid_1's rmse: 0.0192252\n",
      "[2000]\ttraining's rmse: 0.0137414\tvalid_1's rmse: 0.0189451\n",
      "[2100]\ttraining's rmse: 0.0133814\tvalid_1's rmse: 0.0186813\n",
      "[2200]\ttraining's rmse: 0.0130844\tvalid_1's rmse: 0.0184929\n",
      "[2300]\ttraining's rmse: 0.0127943\tvalid_1's rmse: 0.0182924\n",
      "[2400]\ttraining's rmse: 0.0125422\tvalid_1's rmse: 0.018167\n",
      "[2500]\ttraining's rmse: 0.0122744\tvalid_1's rmse: 0.0180331\n",
      "[2600]\ttraining's rmse: 0.012052\tvalid_1's rmse: 0.0179525\n",
      "[2700]\ttraining's rmse: 0.0118195\tvalid_1's rmse: 0.0178189\n",
      "[2800]\ttraining's rmse: 0.011611\tvalid_1's rmse: 0.0176994\n",
      "[2900]\ttraining's rmse: 0.0113986\tvalid_1's rmse: 0.0175884\n",
      "[3000]\ttraining's rmse: 0.0112\tvalid_1's rmse: 0.0174684\n",
      "[3100]\ttraining's rmse: 0.0110232\tvalid_1's rmse: 0.0173836\n",
      "[3200]\ttraining's rmse: 0.0108647\tvalid_1's rmse: 0.0173186\n",
      "[3300]\ttraining's rmse: 0.0107077\tvalid_1's rmse: 0.01724\n",
      "[3400]\ttraining's rmse: 0.0105438\tvalid_1's rmse: 0.0171597\n",
      "[3500]\ttraining's rmse: 0.0103881\tvalid_1's rmse: 0.017092\n",
      "[3600]\ttraining's rmse: 0.0102247\tvalid_1's rmse: 0.0170203\n",
      "[3700]\ttraining's rmse: 0.0100725\tvalid_1's rmse: 0.0169508\n",
      "[3800]\ttraining's rmse: 0.00992369\tvalid_1's rmse: 0.016876\n",
      "[3900]\ttraining's rmse: 0.00979389\tvalid_1's rmse: 0.0168272\n",
      "[4000]\ttraining's rmse: 0.00965649\tvalid_1's rmse: 0.0167653\n",
      "[4100]\ttraining's rmse: 0.00951162\tvalid_1's rmse: 0.0166886\n",
      "[4200]\ttraining's rmse: 0.00938482\tvalid_1's rmse: 0.0166293\n",
      "[4300]\ttraining's rmse: 0.00925652\tvalid_1's rmse: 0.0165657\n",
      "[4400]\ttraining's rmse: 0.0091105\tvalid_1's rmse: 0.0164845\n",
      "[4500]\ttraining's rmse: 0.00899014\tvalid_1's rmse: 0.0164423\n",
      "[4600]\ttraining's rmse: 0.00886602\tvalid_1's rmse: 0.0163808\n",
      "[4700]\ttraining's rmse: 0.00876145\tvalid_1's rmse: 0.0163457\n",
      "[4800]\ttraining's rmse: 0.0086323\tvalid_1's rmse: 0.0162839\n",
      "[4900]\ttraining's rmse: 0.0085179\tvalid_1's rmse: 0.0162524\n",
      "[5000]\ttraining's rmse: 0.00841211\tvalid_1's rmse: 0.016203\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00841211\tvalid_1's rmse: 0.016203\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.149695\tvalid_1's rmse: 0.150999\n",
      "[200]\ttraining's rmse: 0.097048\tvalid_1's rmse: 0.10025\n",
      "[300]\ttraining's rmse: 0.0665768\tvalid_1's rmse: 0.0705168\n",
      "[400]\ttraining's rmse: 0.0515939\tvalid_1's rmse: 0.0558959\n",
      "[500]\ttraining's rmse: 0.0418158\tvalid_1's rmse: 0.0462628\n",
      "[600]\ttraining's rmse: 0.0353399\tvalid_1's rmse: 0.0398912\n",
      "[700]\ttraining's rmse: 0.0304176\tvalid_1's rmse: 0.0349686\n",
      "[800]\ttraining's rmse: 0.0268692\tvalid_1's rmse: 0.0314393\n",
      "[900]\ttraining's rmse: 0.0240476\tvalid_1's rmse: 0.0286511\n",
      "[1000]\ttraining's rmse: 0.0218946\tvalid_1's rmse: 0.0265077\n",
      "[1100]\ttraining's rmse: 0.0202463\tvalid_1's rmse: 0.0248718\n",
      "[1200]\ttraining's rmse: 0.0189202\tvalid_1's rmse: 0.0235591\n",
      "[1300]\ttraining's rmse: 0.0177587\tvalid_1's rmse: 0.0224134\n",
      "[1400]\ttraining's rmse: 0.0168135\tvalid_1's rmse: 0.0214776\n",
      "[1500]\ttraining's rmse: 0.0160001\tvalid_1's rmse: 0.0206888\n",
      "[1600]\ttraining's rmse: 0.0153374\tvalid_1's rmse: 0.0200755\n",
      "[1700]\ttraining's rmse: 0.014762\tvalid_1's rmse: 0.0195656\n",
      "[1800]\ttraining's rmse: 0.0142486\tvalid_1's rmse: 0.0191274\n",
      "[1900]\ttraining's rmse: 0.0138122\tvalid_1's rmse: 0.0187494\n",
      "[2000]\ttraining's rmse: 0.0133984\tvalid_1's rmse: 0.0184162\n",
      "[2100]\ttraining's rmse: 0.0130315\tvalid_1's rmse: 0.0181683\n",
      "[2200]\ttraining's rmse: 0.0127203\tvalid_1's rmse: 0.0179471\n",
      "[2300]\ttraining's rmse: 0.012429\tvalid_1's rmse: 0.0177529\n",
      "[2400]\ttraining's rmse: 0.0121718\tvalid_1's rmse: 0.0175797\n",
      "[2500]\ttraining's rmse: 0.0118984\tvalid_1's rmse: 0.0173962\n",
      "[2600]\ttraining's rmse: 0.0116439\tvalid_1's rmse: 0.0172396\n",
      "[2700]\ttraining's rmse: 0.0114066\tvalid_1's rmse: 0.0170892\n",
      "[2800]\ttraining's rmse: 0.0112042\tvalid_1's rmse: 0.0169784\n",
      "[2900]\ttraining's rmse: 0.0110093\tvalid_1's rmse: 0.0168672\n",
      "[3000]\ttraining's rmse: 0.010821\tvalid_1's rmse: 0.0167634\n",
      "[3100]\ttraining's rmse: 0.0106448\tvalid_1's rmse: 0.0166789\n",
      "[3200]\ttraining's rmse: 0.0104459\tvalid_1's rmse: 0.0165704\n",
      "[3300]\ttraining's rmse: 0.0102959\tvalid_1's rmse: 0.0165086\n",
      "[3400]\ttraining's rmse: 0.0101385\tvalid_1's rmse: 0.016419\n",
      "[3500]\ttraining's rmse: 0.00998368\tvalid_1's rmse: 0.0163517\n",
      "[3600]\ttraining's rmse: 0.00984536\tvalid_1's rmse: 0.0162979\n",
      "[3700]\ttraining's rmse: 0.00971259\tvalid_1's rmse: 0.016252\n",
      "[3800]\ttraining's rmse: 0.00953572\tvalid_1's rmse: 0.0161524\n",
      "[3900]\ttraining's rmse: 0.00940658\tvalid_1's rmse: 0.016105\n",
      "[4000]\ttraining's rmse: 0.00926921\tvalid_1's rmse: 0.0160405\n",
      "[4100]\ttraining's rmse: 0.00914491\tvalid_1's rmse: 0.0159858\n",
      "[4200]\ttraining's rmse: 0.00902173\tvalid_1's rmse: 0.0159256\n",
      "[4300]\ttraining's rmse: 0.00891149\tvalid_1's rmse: 0.0158901\n",
      "[4400]\ttraining's rmse: 0.00878906\tvalid_1's rmse: 0.0158401\n",
      "[4500]\ttraining's rmse: 0.00867861\tvalid_1's rmse: 0.0158005\n",
      "[4600]\ttraining's rmse: 0.00856826\tvalid_1's rmse: 0.0157664\n",
      "[4700]\ttraining's rmse: 0.00843584\tvalid_1's rmse: 0.0156981\n",
      "[4800]\ttraining's rmse: 0.00834033\tvalid_1's rmse: 0.0156685\n",
      "[4900]\ttraining's rmse: 0.00822326\tvalid_1's rmse: 0.015615\n",
      "[5000]\ttraining's rmse: 0.00812953\tvalid_1's rmse: 0.0155856\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00812953\tvalid_1's rmse: 0.0155856\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.148722\tvalid_1's rmse: 0.149796\n",
      "[200]\ttraining's rmse: 0.0960299\tvalid_1's rmse: 0.0988879\n",
      "[300]\ttraining's rmse: 0.0673367\tvalid_1's rmse: 0.0710479\n",
      "[400]\ttraining's rmse: 0.0513943\tvalid_1's rmse: 0.0555837\n",
      "[500]\ttraining's rmse: 0.0422495\tvalid_1's rmse: 0.0468106\n",
      "[600]\ttraining's rmse: 0.0357605\tvalid_1's rmse: 0.0406314\n",
      "[700]\ttraining's rmse: 0.0307975\tvalid_1's rmse: 0.0359508\n",
      "[800]\ttraining's rmse: 0.0273089\tvalid_1's rmse: 0.0327174\n",
      "[900]\ttraining's rmse: 0.0247514\tvalid_1's rmse: 0.0303918\n",
      "[1000]\ttraining's rmse: 0.0226432\tvalid_1's rmse: 0.0283796\n",
      "[1100]\ttraining's rmse: 0.0208595\tvalid_1's rmse: 0.0266648\n",
      "[1200]\ttraining's rmse: 0.0194435\tvalid_1's rmse: 0.0253003\n",
      "[1300]\ttraining's rmse: 0.0182926\tvalid_1's rmse: 0.0242098\n",
      "[1400]\ttraining's rmse: 0.0173249\tvalid_1's rmse: 0.0232685\n",
      "[1500]\ttraining's rmse: 0.0165827\tvalid_1's rmse: 0.0225815\n",
      "[1600]\ttraining's rmse: 0.0159214\tvalid_1's rmse: 0.0219662\n",
      "[1700]\ttraining's rmse: 0.0153527\tvalid_1's rmse: 0.0214658\n",
      "[1800]\ttraining's rmse: 0.0148074\tvalid_1's rmse: 0.0209669\n",
      "[1900]\ttraining's rmse: 0.0143554\tvalid_1's rmse: 0.0205835\n",
      "[2000]\ttraining's rmse: 0.0139308\tvalid_1's rmse: 0.0202061\n",
      "[2100]\ttraining's rmse: 0.0135846\tvalid_1's rmse: 0.0199481\n",
      "[2200]\ttraining's rmse: 0.0132572\tvalid_1's rmse: 0.019718\n",
      "[2300]\ttraining's rmse: 0.0129383\tvalid_1's rmse: 0.0194907\n",
      "[2400]\ttraining's rmse: 0.0126192\tvalid_1's rmse: 0.0192586\n",
      "[2500]\ttraining's rmse: 0.0123826\tvalid_1's rmse: 0.0191045\n",
      "[2600]\ttraining's rmse: 0.012125\tvalid_1's rmse: 0.0189208\n",
      "[2700]\ttraining's rmse: 0.011894\tvalid_1's rmse: 0.0187595\n",
      "[2800]\ttraining's rmse: 0.0116868\tvalid_1's rmse: 0.0186391\n",
      "[2900]\ttraining's rmse: 0.0114621\tvalid_1's rmse: 0.0185028\n",
      "[3000]\ttraining's rmse: 0.011267\tvalid_1's rmse: 0.0184035\n",
      "[3100]\ttraining's rmse: 0.0110884\tvalid_1's rmse: 0.0183103\n",
      "[3200]\ttraining's rmse: 0.0109192\tvalid_1's rmse: 0.0182246\n",
      "[3300]\ttraining's rmse: 0.0107551\tvalid_1's rmse: 0.0181522\n",
      "[3400]\ttraining's rmse: 0.0106073\tvalid_1's rmse: 0.0180865\n",
      "[3500]\ttraining's rmse: 0.0104634\tvalid_1's rmse: 0.0180319\n",
      "[3600]\ttraining's rmse: 0.0103188\tvalid_1's rmse: 0.0179673\n",
      "[3700]\ttraining's rmse: 0.0101872\tvalid_1's rmse: 0.017914\n",
      "[3800]\ttraining's rmse: 0.0100386\tvalid_1's rmse: 0.0178462\n",
      "[3900]\ttraining's rmse: 0.00988606\tvalid_1's rmse: 0.0177781\n",
      "[4000]\ttraining's rmse: 0.00975141\tvalid_1's rmse: 0.0177178\n",
      "[4100]\ttraining's rmse: 0.00961393\tvalid_1's rmse: 0.0176562\n",
      "[4200]\ttraining's rmse: 0.00949742\tvalid_1's rmse: 0.0176133\n",
      "[4300]\ttraining's rmse: 0.00938429\tvalid_1's rmse: 0.0175797\n",
      "[4400]\ttraining's rmse: 0.00925078\tvalid_1's rmse: 0.0175252\n",
      "[4500]\ttraining's rmse: 0.00912855\tvalid_1's rmse: 0.0174723\n",
      "[4600]\ttraining's rmse: 0.00900561\tvalid_1's rmse: 0.0174199\n",
      "[4700]\ttraining's rmse: 0.00890134\tvalid_1's rmse: 0.0173837\n",
      "[4800]\ttraining's rmse: 0.00879362\tvalid_1's rmse: 0.0173478\n",
      "[4900]\ttraining's rmse: 0.00867601\tvalid_1's rmse: 0.0173014\n",
      "[5000]\ttraining's rmse: 0.00857166\tvalid_1's rmse: 0.0172603\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00857166\tvalid_1's rmse: 0.0172603\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.1456\tvalid_1's rmse: 0.147909\n",
      "[200]\ttraining's rmse: 0.0939696\tvalid_1's rmse: 0.0975331\n",
      "[300]\ttraining's rmse: 0.0649855\tvalid_1's rmse: 0.0689855\n",
      "[400]\ttraining's rmse: 0.0507432\tvalid_1's rmse: 0.0550097\n",
      "[500]\ttraining's rmse: 0.0414379\tvalid_1's rmse: 0.0458056\n",
      "[600]\ttraining's rmse: 0.035107\tvalid_1's rmse: 0.0395635\n",
      "[700]\ttraining's rmse: 0.0302525\tvalid_1's rmse: 0.0347394\n",
      "[800]\ttraining's rmse: 0.0267993\tvalid_1's rmse: 0.0313167\n",
      "[900]\ttraining's rmse: 0.024084\tvalid_1's rmse: 0.0286336\n",
      "[1000]\ttraining's rmse: 0.0219969\tvalid_1's rmse: 0.0265157\n",
      "[1100]\ttraining's rmse: 0.0203958\tvalid_1's rmse: 0.0249092\n",
      "[1200]\ttraining's rmse: 0.0191015\tvalid_1's rmse: 0.0236002\n",
      "[1300]\ttraining's rmse: 0.0179621\tvalid_1's rmse: 0.0224396\n",
      "[1400]\ttraining's rmse: 0.0170145\tvalid_1's rmse: 0.0214651\n",
      "[1500]\ttraining's rmse: 0.0162225\tvalid_1's rmse: 0.0206373\n",
      "[1600]\ttraining's rmse: 0.0155763\tvalid_1's rmse: 0.0200047\n",
      "[1700]\ttraining's rmse: 0.0150073\tvalid_1's rmse: 0.0194847\n",
      "[1800]\ttraining's rmse: 0.0145\tvalid_1's rmse: 0.0190368\n",
      "[1900]\ttraining's rmse: 0.0140287\tvalid_1's rmse: 0.0186186\n",
      "[2000]\ttraining's rmse: 0.0136007\tvalid_1's rmse: 0.018259\n",
      "[2100]\ttraining's rmse: 0.0132031\tvalid_1's rmse: 0.0179484\n",
      "[2200]\ttraining's rmse: 0.0128683\tvalid_1's rmse: 0.0176879\n",
      "[2300]\ttraining's rmse: 0.012563\tvalid_1's rmse: 0.0174703\n",
      "[2400]\ttraining's rmse: 0.0122693\tvalid_1's rmse: 0.0172684\n",
      "[2500]\ttraining's rmse: 0.0119983\tvalid_1's rmse: 0.0170884\n",
      "[2600]\ttraining's rmse: 0.0117326\tvalid_1's rmse: 0.0169143\n",
      "[2700]\ttraining's rmse: 0.0114883\tvalid_1's rmse: 0.0167473\n",
      "[2800]\ttraining's rmse: 0.0112655\tvalid_1's rmse: 0.0165983\n",
      "[2900]\ttraining's rmse: 0.0110349\tvalid_1's rmse: 0.0164369\n",
      "[3000]\ttraining's rmse: 0.0108222\tvalid_1's rmse: 0.0163036\n",
      "[3100]\ttraining's rmse: 0.0106424\tvalid_1's rmse: 0.0162153\n",
      "[3200]\ttraining's rmse: 0.0104552\tvalid_1's rmse: 0.0161091\n",
      "[3300]\ttraining's rmse: 0.0102888\tvalid_1's rmse: 0.0160199\n",
      "[3400]\ttraining's rmse: 0.0100983\tvalid_1's rmse: 0.0159036\n",
      "[3500]\ttraining's rmse: 0.00993831\tvalid_1's rmse: 0.0158293\n",
      "[3600]\ttraining's rmse: 0.00977729\tvalid_1's rmse: 0.0157433\n",
      "[3700]\ttraining's rmse: 0.00963652\tvalid_1's rmse: 0.0156885\n",
      "[3800]\ttraining's rmse: 0.00947861\tvalid_1's rmse: 0.015609\n",
      "[3900]\ttraining's rmse: 0.00932817\tvalid_1's rmse: 0.0155353\n",
      "[4000]\ttraining's rmse: 0.00919069\tvalid_1's rmse: 0.0154709\n",
      "[4100]\ttraining's rmse: 0.00905134\tvalid_1's rmse: 0.015404\n",
      "[4200]\ttraining's rmse: 0.00892333\tvalid_1's rmse: 0.015334\n",
      "[4300]\ttraining's rmse: 0.00878587\tvalid_1's rmse: 0.0152642\n",
      "[4400]\ttraining's rmse: 0.00866266\tvalid_1's rmse: 0.0152058\n",
      "[4500]\ttraining's rmse: 0.0085385\tvalid_1's rmse: 0.0151536\n",
      "[4600]\ttraining's rmse: 0.00843345\tvalid_1's rmse: 0.0151071\n",
      "[4700]\ttraining's rmse: 0.00830428\tvalid_1's rmse: 0.0150362\n",
      "[4800]\ttraining's rmse: 0.00819462\tvalid_1's rmse: 0.0149968\n",
      "[4900]\ttraining's rmse: 0.00808876\tvalid_1's rmse: 0.0149512\n",
      "[5000]\ttraining's rmse: 0.00797719\tvalid_1's rmse: 0.0148915\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00797719\tvalid_1's rmse: 0.0148915\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.147591\tvalid_1's rmse: 0.149922\n",
      "[200]\ttraining's rmse: 0.095501\tvalid_1's rmse: 0.0995742\n",
      "[300]\ttraining's rmse: 0.0657359\tvalid_1's rmse: 0.0702886\n",
      "[400]\ttraining's rmse: 0.0510038\tvalid_1's rmse: 0.0556964\n",
      "[500]\ttraining's rmse: 0.0413333\tvalid_1's rmse: 0.0461477\n",
      "[600]\ttraining's rmse: 0.0347758\tvalid_1's rmse: 0.0396171\n",
      "[700]\ttraining's rmse: 0.0298116\tvalid_1's rmse: 0.0346777\n",
      "[800]\ttraining's rmse: 0.0262729\tvalid_1's rmse: 0.0311185\n",
      "[900]\ttraining's rmse: 0.0235616\tvalid_1's rmse: 0.0284107\n",
      "[1000]\ttraining's rmse: 0.0214818\tvalid_1's rmse: 0.0262947\n",
      "[1100]\ttraining's rmse: 0.0198997\tvalid_1's rmse: 0.0247043\n",
      "[1200]\ttraining's rmse: 0.0186286\tvalid_1's rmse: 0.0234435\n",
      "[1300]\ttraining's rmse: 0.0175912\tvalid_1's rmse: 0.0223857\n",
      "[1400]\ttraining's rmse: 0.0166696\tvalid_1's rmse: 0.0214749\n",
      "[1500]\ttraining's rmse: 0.015925\tvalid_1's rmse: 0.0207064\n",
      "[1600]\ttraining's rmse: 0.0153145\tvalid_1's rmse: 0.02015\n",
      "[1700]\ttraining's rmse: 0.0147855\tvalid_1's rmse: 0.0196634\n",
      "[1800]\ttraining's rmse: 0.0143194\tvalid_1's rmse: 0.0192636\n",
      "[1900]\ttraining's rmse: 0.0138968\tvalid_1's rmse: 0.0189157\n",
      "[2000]\ttraining's rmse: 0.0135087\tvalid_1's rmse: 0.0186037\n",
      "[2100]\ttraining's rmse: 0.0131774\tvalid_1's rmse: 0.0183668\n",
      "[2200]\ttraining's rmse: 0.0128793\tvalid_1's rmse: 0.018155\n",
      "[2300]\ttraining's rmse: 0.0125985\tvalid_1's rmse: 0.017963\n",
      "[2400]\ttraining's rmse: 0.0123198\tvalid_1's rmse: 0.0178095\n",
      "[2500]\ttraining's rmse: 0.0120491\tvalid_1's rmse: 0.0176258\n",
      "[2600]\ttraining's rmse: 0.0117916\tvalid_1's rmse: 0.0174658\n",
      "[2700]\ttraining's rmse: 0.0115483\tvalid_1's rmse: 0.0173154\n",
      "[2800]\ttraining's rmse: 0.0113148\tvalid_1's rmse: 0.0171788\n",
      "[2900]\ttraining's rmse: 0.0110798\tvalid_1's rmse: 0.0170254\n",
      "[3000]\ttraining's rmse: 0.0108776\tvalid_1's rmse: 0.0169153\n",
      "[3100]\ttraining's rmse: 0.0107011\tvalid_1's rmse: 0.0168231\n",
      "[3200]\ttraining's rmse: 0.0105149\tvalid_1's rmse: 0.0167453\n",
      "[3300]\ttraining's rmse: 0.010339\tvalid_1's rmse: 0.0166574\n",
      "[3400]\ttraining's rmse: 0.0101899\tvalid_1's rmse: 0.0165894\n",
      "[3500]\ttraining's rmse: 0.0100363\tvalid_1's rmse: 0.0165292\n",
      "[3600]\ttraining's rmse: 0.00986726\tvalid_1's rmse: 0.0164336\n",
      "[3700]\ttraining's rmse: 0.0097432\tvalid_1's rmse: 0.0163925\n",
      "[3800]\ttraining's rmse: 0.00959884\tvalid_1's rmse: 0.016327\n",
      "[3900]\ttraining's rmse: 0.00943752\tvalid_1's rmse: 0.0162422\n",
      "[4000]\ttraining's rmse: 0.00930233\tvalid_1's rmse: 0.0161931\n",
      "[4100]\ttraining's rmse: 0.00918527\tvalid_1's rmse: 0.016149\n",
      "[4200]\ttraining's rmse: 0.00905066\tvalid_1's rmse: 0.0160818\n",
      "[4300]\ttraining's rmse: 0.00893474\tvalid_1's rmse: 0.0160371\n",
      "[4400]\ttraining's rmse: 0.00882027\tvalid_1's rmse: 0.0159936\n",
      "[4500]\ttraining's rmse: 0.00871272\tvalid_1's rmse: 0.0159532\n",
      "[4600]\ttraining's rmse: 0.00859527\tvalid_1's rmse: 0.0159083\n",
      "[4700]\ttraining's rmse: 0.00847488\tvalid_1's rmse: 0.0158578\n",
      "[4800]\ttraining's rmse: 0.00838586\tvalid_1's rmse: 0.0158228\n",
      "[4900]\ttraining's rmse: 0.00827994\tvalid_1's rmse: 0.0157867\n",
      "[5000]\ttraining's rmse: 0.00816248\tvalid_1's rmse: 0.0157384\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00816248\tvalid_1's rmse: 0.0157384\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.14023\tvalid_1's rmse: 0.144636\n",
      "[200]\ttraining's rmse: 0.0866138\tvalid_1's rmse: 0.0920779\n",
      "[300]\ttraining's rmse: 0.0603681\tvalid_1's rmse: 0.0662884\n",
      "[400]\ttraining's rmse: 0.0462171\tvalid_1's rmse: 0.052193\n",
      "[500]\ttraining's rmse: 0.0369755\tvalid_1's rmse: 0.0427646\n",
      "[600]\ttraining's rmse: 0.0310702\tvalid_1's rmse: 0.0367711\n",
      "[700]\ttraining's rmse: 0.0272172\tvalid_1's rmse: 0.0329322\n",
      "[800]\ttraining's rmse: 0.0243254\tvalid_1's rmse: 0.0300235\n",
      "[900]\ttraining's rmse: 0.0221242\tvalid_1's rmse: 0.0277723\n",
      "[1000]\ttraining's rmse: 0.0203089\tvalid_1's rmse: 0.0258696\n",
      "[1100]\ttraining's rmse: 0.0188368\tvalid_1's rmse: 0.0243196\n",
      "[1200]\ttraining's rmse: 0.0177181\tvalid_1's rmse: 0.0231139\n",
      "[1300]\ttraining's rmse: 0.0168455\tvalid_1's rmse: 0.0222049\n",
      "[1400]\ttraining's rmse: 0.0161387\tvalid_1's rmse: 0.0214896\n",
      "[1500]\ttraining's rmse: 0.0155429\tvalid_1's rmse: 0.0209008\n",
      "[1600]\ttraining's rmse: 0.0150362\tvalid_1's rmse: 0.0204183\n",
      "[1700]\ttraining's rmse: 0.0145858\tvalid_1's rmse: 0.0199888\n",
      "[1800]\ttraining's rmse: 0.014205\tvalid_1's rmse: 0.0196821\n",
      "[1900]\ttraining's rmse: 0.0138383\tvalid_1's rmse: 0.0193732\n",
      "[2000]\ttraining's rmse: 0.013532\tvalid_1's rmse: 0.0191564\n",
      "[2100]\ttraining's rmse: 0.0132155\tvalid_1's rmse: 0.0189164\n",
      "[2200]\ttraining's rmse: 0.0129385\tvalid_1's rmse: 0.0187208\n",
      "[2300]\ttraining's rmse: 0.0126707\tvalid_1's rmse: 0.018562\n",
      "[2400]\ttraining's rmse: 0.0124345\tvalid_1's rmse: 0.0184192\n",
      "[2500]\ttraining's rmse: 0.0122231\tvalid_1's rmse: 0.0182866\n",
      "[2600]\ttraining's rmse: 0.0120068\tvalid_1's rmse: 0.0181557\n",
      "[2700]\ttraining's rmse: 0.0118223\tvalid_1's rmse: 0.018059\n",
      "[2800]\ttraining's rmse: 0.0116388\tvalid_1's rmse: 0.0179537\n",
      "[2900]\ttraining's rmse: 0.0114611\tvalid_1's rmse: 0.0178561\n",
      "[3000]\ttraining's rmse: 0.0112798\tvalid_1's rmse: 0.0177589\n",
      "[3100]\ttraining's rmse: 0.0111206\tvalid_1's rmse: 0.0176849\n",
      "[3200]\ttraining's rmse: 0.0109513\tvalid_1's rmse: 0.0176026\n",
      "[3300]\ttraining's rmse: 0.0108024\tvalid_1's rmse: 0.0175296\n",
      "[3400]\ttraining's rmse: 0.0106626\tvalid_1's rmse: 0.0174812\n",
      "[3500]\ttraining's rmse: 0.0105202\tvalid_1's rmse: 0.0174236\n",
      "[3600]\ttraining's rmse: 0.0103595\tvalid_1's rmse: 0.017338\n",
      "[3700]\ttraining's rmse: 0.0102226\tvalid_1's rmse: 0.017281\n",
      "[3800]\ttraining's rmse: 0.0100702\tvalid_1's rmse: 0.0171975\n",
      "[3900]\ttraining's rmse: 0.00995302\tvalid_1's rmse: 0.0171613\n",
      "[4000]\ttraining's rmse: 0.00982016\tvalid_1's rmse: 0.0171033\n",
      "[4100]\ttraining's rmse: 0.00971074\tvalid_1's rmse: 0.0170605\n",
      "[4200]\ttraining's rmse: 0.00956473\tvalid_1's rmse: 0.016981\n",
      "[4300]\ttraining's rmse: 0.00944146\tvalid_1's rmse: 0.0169332\n",
      "[4400]\ttraining's rmse: 0.00932251\tvalid_1's rmse: 0.0168774\n",
      "[4500]\ttraining's rmse: 0.00922041\tvalid_1's rmse: 0.0168532\n",
      "[4600]\ttraining's rmse: 0.00911095\tvalid_1's rmse: 0.0168043\n",
      "[4700]\ttraining's rmse: 0.00899736\tvalid_1's rmse: 0.0167656\n",
      "[4800]\ttraining's rmse: 0.00887644\tvalid_1's rmse: 0.0167119\n",
      "[4900]\ttraining's rmse: 0.00875973\tvalid_1's rmse: 0.0166668\n",
      "[5000]\ttraining's rmse: 0.00865674\tvalid_1's rmse: 0.0166275\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00865674\tvalid_1's rmse: 0.0166275\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.148507\tvalid_1's rmse: 0.149816\n",
      "[200]\ttraining's rmse: 0.0965367\tvalid_1's rmse: 0.099551\n",
      "[300]\ttraining's rmse: 0.0683776\tvalid_1's rmse: 0.0723162\n",
      "[400]\ttraining's rmse: 0.0528099\tvalid_1's rmse: 0.0571482\n",
      "[500]\ttraining's rmse: 0.043908\tvalid_1's rmse: 0.048487\n",
      "[600]\ttraining's rmse: 0.037575\tvalid_1's rmse: 0.0423192\n",
      "[700]\ttraining's rmse: 0.0326043\tvalid_1's rmse: 0.0374809\n",
      "[800]\ttraining's rmse: 0.02899\tvalid_1's rmse: 0.0339582\n",
      "[900]\ttraining's rmse: 0.0262612\tvalid_1's rmse: 0.0313296\n",
      "[1000]\ttraining's rmse: 0.0240055\tvalid_1's rmse: 0.0290927\n",
      "[1100]\ttraining's rmse: 0.0220495\tvalid_1's rmse: 0.0271635\n",
      "[1200]\ttraining's rmse: 0.0204714\tvalid_1's rmse: 0.0255888\n",
      "[1300]\ttraining's rmse: 0.0191945\tvalid_1's rmse: 0.0243183\n",
      "[1400]\ttraining's rmse: 0.0180965\tvalid_1's rmse: 0.0232109\n",
      "[1500]\ttraining's rmse: 0.0172252\tvalid_1's rmse: 0.0223355\n",
      "[1600]\ttraining's rmse: 0.0164683\tvalid_1's rmse: 0.021606\n",
      "[1700]\ttraining's rmse: 0.0158085\tvalid_1's rmse: 0.020995\n",
      "[1800]\ttraining's rmse: 0.0151992\tvalid_1's rmse: 0.0203835\n",
      "[1900]\ttraining's rmse: 0.0146974\tvalid_1's rmse: 0.0199261\n",
      "[2000]\ttraining's rmse: 0.0142051\tvalid_1's rmse: 0.0194795\n",
      "[2100]\ttraining's rmse: 0.0138125\tvalid_1's rmse: 0.0191614\n",
      "[2200]\ttraining's rmse: 0.0134359\tvalid_1's rmse: 0.0188522\n",
      "[2300]\ttraining's rmse: 0.0130781\tvalid_1's rmse: 0.0185677\n",
      "[2400]\ttraining's rmse: 0.0127154\tvalid_1's rmse: 0.0182826\n",
      "[2500]\ttraining's rmse: 0.0124243\tvalid_1's rmse: 0.01807\n",
      "[2600]\ttraining's rmse: 0.0121375\tvalid_1's rmse: 0.0178544\n",
      "[2700]\ttraining's rmse: 0.0118711\tvalid_1's rmse: 0.0176674\n",
      "[2800]\ttraining's rmse: 0.0116291\tvalid_1's rmse: 0.0175064\n",
      "[2900]\ttraining's rmse: 0.0113976\tvalid_1's rmse: 0.0173652\n",
      "[3000]\ttraining's rmse: 0.0111581\tvalid_1's rmse: 0.0172204\n",
      "[3100]\ttraining's rmse: 0.0109481\tvalid_1's rmse: 0.0170936\n",
      "[3200]\ttraining's rmse: 0.0107609\tvalid_1's rmse: 0.0169986\n",
      "[3300]\ttraining's rmse: 0.0105666\tvalid_1's rmse: 0.0168839\n",
      "[3400]\ttraining's rmse: 0.0103897\tvalid_1's rmse: 0.0167994\n",
      "[3500]\ttraining's rmse: 0.0102269\tvalid_1's rmse: 0.0167125\n",
      "[3600]\ttraining's rmse: 0.0100641\tvalid_1's rmse: 0.0166262\n",
      "[3700]\ttraining's rmse: 0.00992293\tvalid_1's rmse: 0.0165609\n",
      "[3800]\ttraining's rmse: 0.00978323\tvalid_1's rmse: 0.0164843\n",
      "[3900]\ttraining's rmse: 0.00962537\tvalid_1's rmse: 0.016393\n",
      "[4000]\ttraining's rmse: 0.00946822\tvalid_1's rmse: 0.0163232\n",
      "[4100]\ttraining's rmse: 0.00933322\tvalid_1's rmse: 0.0162618\n",
      "[4200]\ttraining's rmse: 0.00919975\tvalid_1's rmse: 0.0161934\n",
      "[4300]\ttraining's rmse: 0.00907944\tvalid_1's rmse: 0.016138\n",
      "[4400]\ttraining's rmse: 0.00894297\tvalid_1's rmse: 0.0160621\n",
      "[4500]\ttraining's rmse: 0.00881896\tvalid_1's rmse: 0.0160069\n",
      "[4600]\ttraining's rmse: 0.00869999\tvalid_1's rmse: 0.0159542\n",
      "[4700]\ttraining's rmse: 0.00858373\tvalid_1's rmse: 0.0158994\n",
      "[4800]\ttraining's rmse: 0.00848126\tvalid_1's rmse: 0.0158541\n",
      "[4900]\ttraining's rmse: 0.00836719\tvalid_1's rmse: 0.0158101\n",
      "[5000]\ttraining's rmse: 0.00825506\tvalid_1's rmse: 0.0157617\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00825506\tvalid_1's rmse: 0.0157617\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.142884\tvalid_1's rmse: 0.144206\n",
      "[200]\ttraining's rmse: 0.0889343\tvalid_1's rmse: 0.0914449\n",
      "[300]\ttraining's rmse: 0.0616185\tvalid_1's rmse: 0.0646482\n",
      "[400]\ttraining's rmse: 0.0468755\tvalid_1's rmse: 0.0501514\n",
      "[500]\ttraining's rmse: 0.0373518\tvalid_1's rmse: 0.0406915\n",
      "[600]\ttraining's rmse: 0.0312714\tvalid_1's rmse: 0.0346291\n",
      "[700]\ttraining's rmse: 0.0273022\tvalid_1's rmse: 0.030709\n",
      "[800]\ttraining's rmse: 0.024422\tvalid_1's rmse: 0.0278734\n",
      "[900]\ttraining's rmse: 0.0221898\tvalid_1's rmse: 0.0256942\n",
      "[1000]\ttraining's rmse: 0.0203538\tvalid_1's rmse: 0.0238901\n",
      "[1100]\ttraining's rmse: 0.0188556\tvalid_1's rmse: 0.0223765\n",
      "[1200]\ttraining's rmse: 0.017786\tvalid_1's rmse: 0.0212965\n",
      "[1300]\ttraining's rmse: 0.016877\tvalid_1's rmse: 0.0203955\n",
      "[1400]\ttraining's rmse: 0.0161532\tvalid_1's rmse: 0.0197021\n",
      "[1500]\ttraining's rmse: 0.0155172\tvalid_1's rmse: 0.0191122\n",
      "[1600]\ttraining's rmse: 0.0150093\tvalid_1's rmse: 0.0186596\n",
      "[1700]\ttraining's rmse: 0.0145635\tvalid_1's rmse: 0.0182668\n",
      "[1800]\ttraining's rmse: 0.0141583\tvalid_1's rmse: 0.017932\n",
      "[1900]\ttraining's rmse: 0.0137754\tvalid_1's rmse: 0.0176222\n",
      "[2000]\ttraining's rmse: 0.0133932\tvalid_1's rmse: 0.0173488\n",
      "[2100]\ttraining's rmse: 0.0130779\tvalid_1's rmse: 0.0171183\n",
      "[2200]\ttraining's rmse: 0.0127993\tvalid_1's rmse: 0.0169268\n",
      "[2300]\ttraining's rmse: 0.0125298\tvalid_1's rmse: 0.0167506\n",
      "[2400]\ttraining's rmse: 0.0122711\tvalid_1's rmse: 0.0165721\n",
      "[2500]\ttraining's rmse: 0.0120359\tvalid_1's rmse: 0.0164331\n",
      "[2600]\ttraining's rmse: 0.0118233\tvalid_1's rmse: 0.0163034\n",
      "[2700]\ttraining's rmse: 0.0115792\tvalid_1's rmse: 0.0161469\n",
      "[2800]\ttraining's rmse: 0.0113952\tvalid_1's rmse: 0.0160526\n",
      "[2900]\ttraining's rmse: 0.0112143\tvalid_1's rmse: 0.0159632\n",
      "[3000]\ttraining's rmse: 0.0110483\tvalid_1's rmse: 0.0158874\n",
      "[3100]\ttraining's rmse: 0.0108782\tvalid_1's rmse: 0.0158064\n",
      "[3200]\ttraining's rmse: 0.0107151\tvalid_1's rmse: 0.0157326\n",
      "[3300]\ttraining's rmse: 0.0105545\tvalid_1's rmse: 0.0156682\n",
      "[3400]\ttraining's rmse: 0.0104195\tvalid_1's rmse: 0.0156173\n",
      "[3500]\ttraining's rmse: 0.0102782\tvalid_1's rmse: 0.0155655\n",
      "[3600]\ttraining's rmse: 0.010129\tvalid_1's rmse: 0.0154943\n",
      "[3700]\ttraining's rmse: 0.00999\tvalid_1's rmse: 0.0154313\n",
      "[3800]\ttraining's rmse: 0.00984354\tvalid_1's rmse: 0.0153562\n",
      "[3900]\ttraining's rmse: 0.00970033\tvalid_1's rmse: 0.0152886\n",
      "[4000]\ttraining's rmse: 0.00956834\tvalid_1's rmse: 0.0152201\n",
      "[4100]\ttraining's rmse: 0.00945678\tvalid_1's rmse: 0.0151834\n",
      "[4200]\ttraining's rmse: 0.00931852\tvalid_1's rmse: 0.0151102\n",
      "[4300]\ttraining's rmse: 0.00920348\tvalid_1's rmse: 0.0150709\n",
      "[4400]\ttraining's rmse: 0.00908739\tvalid_1's rmse: 0.0150253\n",
      "[4500]\ttraining's rmse: 0.00899563\tvalid_1's rmse: 0.0150039\n",
      "[4600]\ttraining's rmse: 0.00887017\tvalid_1's rmse: 0.0149433\n",
      "[4700]\ttraining's rmse: 0.00876091\tvalid_1's rmse: 0.0148918\n",
      "[4800]\ttraining's rmse: 0.00865609\tvalid_1's rmse: 0.0148534\n",
      "[4900]\ttraining's rmse: 0.00856212\tvalid_1's rmse: 0.0148197\n",
      "[5000]\ttraining's rmse: 0.0084704\tvalid_1's rmse: 0.0147849\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.0084704\tvalid_1's rmse: 0.0147849\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.146542\tvalid_1's rmse: 0.149492\n",
      "[200]\ttraining's rmse: 0.0940384\tvalid_1's rmse: 0.0986699\n",
      "[300]\ttraining's rmse: 0.0648161\tvalid_1's rmse: 0.0699554\n",
      "[400]\ttraining's rmse: 0.0507948\tvalid_1's rmse: 0.0560969\n",
      "[500]\ttraining's rmse: 0.0416946\tvalid_1's rmse: 0.0470057\n",
      "[600]\ttraining's rmse: 0.0354539\tvalid_1's rmse: 0.0408906\n",
      "[700]\ttraining's rmse: 0.0305916\tvalid_1's rmse: 0.0360699\n",
      "[800]\ttraining's rmse: 0.0270852\tvalid_1's rmse: 0.0326116\n",
      "[900]\ttraining's rmse: 0.0243288\tvalid_1's rmse: 0.0299325\n",
      "[1000]\ttraining's rmse: 0.0221641\tvalid_1's rmse: 0.0277733\n",
      "[1100]\ttraining's rmse: 0.0204825\tvalid_1's rmse: 0.0261287\n",
      "[1200]\ttraining's rmse: 0.0191119\tvalid_1's rmse: 0.0247615\n",
      "[1300]\ttraining's rmse: 0.0179401\tvalid_1's rmse: 0.0236295\n",
      "[1400]\ttraining's rmse: 0.0169538\tvalid_1's rmse: 0.0226314\n",
      "[1500]\ttraining's rmse: 0.0161302\tvalid_1's rmse: 0.021825\n",
      "[1600]\ttraining's rmse: 0.0154541\tvalid_1's rmse: 0.0211865\n",
      "[1700]\ttraining's rmse: 0.0148684\tvalid_1's rmse: 0.0206477\n",
      "[1800]\ttraining's rmse: 0.0143467\tvalid_1's rmse: 0.0201897\n",
      "[1900]\ttraining's rmse: 0.0138882\tvalid_1's rmse: 0.0197862\n",
      "[2000]\ttraining's rmse: 0.0134383\tvalid_1's rmse: 0.0194067\n",
      "[2100]\ttraining's rmse: 0.01306\tvalid_1's rmse: 0.0190911\n",
      "[2200]\ttraining's rmse: 0.012733\tvalid_1's rmse: 0.0188642\n",
      "[2300]\ttraining's rmse: 0.0124292\tvalid_1's rmse: 0.0186481\n",
      "[2400]\ttraining's rmse: 0.0121333\tvalid_1's rmse: 0.0184505\n",
      "[2500]\ttraining's rmse: 0.0118597\tvalid_1's rmse: 0.0182706\n",
      "[2600]\ttraining's rmse: 0.0116021\tvalid_1's rmse: 0.0181068\n",
      "[2700]\ttraining's rmse: 0.0113639\tvalid_1's rmse: 0.0179703\n",
      "[2800]\ttraining's rmse: 0.0111405\tvalid_1's rmse: 0.0178396\n",
      "[2900]\ttraining's rmse: 0.0109205\tvalid_1's rmse: 0.0177247\n",
      "[3000]\ttraining's rmse: 0.0107267\tvalid_1's rmse: 0.0176252\n",
      "[3100]\ttraining's rmse: 0.0105466\tvalid_1's rmse: 0.0175364\n",
      "[3200]\ttraining's rmse: 0.0103537\tvalid_1's rmse: 0.0174286\n",
      "[3300]\ttraining's rmse: 0.0101856\tvalid_1's rmse: 0.0173565\n",
      "[3400]\ttraining's rmse: 0.0100053\tvalid_1's rmse: 0.017264\n",
      "[3500]\ttraining's rmse: 0.00984112\tvalid_1's rmse: 0.0171937\n",
      "[3600]\ttraining's rmse: 0.00969352\tvalid_1's rmse: 0.0171346\n",
      "[3700]\ttraining's rmse: 0.00955682\tvalid_1's rmse: 0.017081\n",
      "[3800]\ttraining's rmse: 0.00941571\tvalid_1's rmse: 0.0170199\n",
      "[3900]\ttraining's rmse: 0.00927597\tvalid_1's rmse: 0.0169694\n",
      "[4000]\ttraining's rmse: 0.00915307\tvalid_1's rmse: 0.0169226\n",
      "[4100]\ttraining's rmse: 0.0090144\tvalid_1's rmse: 0.0168521\n",
      "[4200]\ttraining's rmse: 0.0088747\tvalid_1's rmse: 0.0167835\n",
      "[4300]\ttraining's rmse: 0.00876028\tvalid_1's rmse: 0.0167436\n",
      "[4400]\ttraining's rmse: 0.00864608\tvalid_1's rmse: 0.0166971\n",
      "[4500]\ttraining's rmse: 0.0085363\tvalid_1's rmse: 0.016659\n",
      "[4600]\ttraining's rmse: 0.00842219\tvalid_1's rmse: 0.0166133\n",
      "[4700]\ttraining's rmse: 0.00830149\tvalid_1's rmse: 0.0165645\n",
      "[4800]\ttraining's rmse: 0.00820113\tvalid_1's rmse: 0.0165323\n",
      "[4900]\ttraining's rmse: 0.00810784\tvalid_1's rmse: 0.0165087\n",
      "[5000]\ttraining's rmse: 0.00797991\tvalid_1's rmse: 0.0164481\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00797991\tvalid_1's rmse: 0.0164481\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.145151\tvalid_1's rmse: 0.148004\n",
      "[200]\ttraining's rmse: 0.0936878\tvalid_1's rmse: 0.0980684\n",
      "[300]\ttraining's rmse: 0.0662126\tvalid_1's rmse: 0.0711005\n",
      "[400]\ttraining's rmse: 0.0510004\tvalid_1's rmse: 0.0559569\n",
      "[500]\ttraining's rmse: 0.0422339\tvalid_1's rmse: 0.0472159\n",
      "[600]\ttraining's rmse: 0.0360037\tvalid_1's rmse: 0.0410269\n",
      "[700]\ttraining's rmse: 0.0311367\tvalid_1's rmse: 0.0361618\n",
      "[800]\ttraining's rmse: 0.0276763\tvalid_1's rmse: 0.0328084\n",
      "[900]\ttraining's rmse: 0.0250901\tvalid_1's rmse: 0.0303188\n",
      "[1000]\ttraining's rmse: 0.022999\tvalid_1's rmse: 0.0282898\n",
      "[1100]\ttraining's rmse: 0.0211794\tvalid_1's rmse: 0.0264856\n",
      "[1200]\ttraining's rmse: 0.0197234\tvalid_1's rmse: 0.0250537\n",
      "[1300]\ttraining's rmse: 0.0185362\tvalid_1's rmse: 0.0238923\n",
      "[1400]\ttraining's rmse: 0.0175282\tvalid_1's rmse: 0.0228987\n",
      "[1500]\ttraining's rmse: 0.0167503\tvalid_1's rmse: 0.0221811\n",
      "[1600]\ttraining's rmse: 0.0160547\tvalid_1's rmse: 0.0215241\n",
      "[1700]\ttraining's rmse: 0.0154329\tvalid_1's rmse: 0.0209635\n",
      "[1800]\ttraining's rmse: 0.0148548\tvalid_1's rmse: 0.0204287\n",
      "[1900]\ttraining's rmse: 0.0143918\tvalid_1's rmse: 0.0200432\n",
      "[2000]\ttraining's rmse: 0.0139385\tvalid_1's rmse: 0.0196798\n",
      "[2100]\ttraining's rmse: 0.0135569\tvalid_1's rmse: 0.0194101\n",
      "[2200]\ttraining's rmse: 0.013198\tvalid_1's rmse: 0.0191266\n",
      "[2300]\ttraining's rmse: 0.0128756\tvalid_1's rmse: 0.0188851\n",
      "[2400]\ttraining's rmse: 0.0125516\tvalid_1's rmse: 0.018642\n",
      "[2500]\ttraining's rmse: 0.012265\tvalid_1's rmse: 0.0184407\n",
      "[2600]\ttraining's rmse: 0.0120092\tvalid_1's rmse: 0.0182741\n",
      "[2700]\ttraining's rmse: 0.0117667\tvalid_1's rmse: 0.0181194\n",
      "[2800]\ttraining's rmse: 0.01154\tvalid_1's rmse: 0.017982\n",
      "[2900]\ttraining's rmse: 0.0113197\tvalid_1's rmse: 0.0178561\n",
      "[3000]\ttraining's rmse: 0.0111228\tvalid_1's rmse: 0.0177435\n",
      "[3100]\ttraining's rmse: 0.0109212\tvalid_1's rmse: 0.017644\n",
      "[3200]\ttraining's rmse: 0.0107403\tvalid_1's rmse: 0.0175572\n",
      "[3300]\ttraining's rmse: 0.0105514\tvalid_1's rmse: 0.0174501\n",
      "[3400]\ttraining's rmse: 0.0103786\tvalid_1's rmse: 0.0173584\n",
      "[3500]\ttraining's rmse: 0.0102108\tvalid_1's rmse: 0.017276\n",
      "[3600]\ttraining's rmse: 0.0100599\tvalid_1's rmse: 0.0172264\n",
      "[3700]\ttraining's rmse: 0.0099091\tvalid_1's rmse: 0.0171543\n",
      "[3800]\ttraining's rmse: 0.00975524\tvalid_1's rmse: 0.0170836\n",
      "[3900]\ttraining's rmse: 0.00961556\tvalid_1's rmse: 0.0170199\n",
      "[4000]\ttraining's rmse: 0.00947658\tvalid_1's rmse: 0.016965\n",
      "[4100]\ttraining's rmse: 0.00934612\tvalid_1's rmse: 0.0169186\n",
      "[4200]\ttraining's rmse: 0.00921945\tvalid_1's rmse: 0.0168654\n",
      "[4300]\ttraining's rmse: 0.00909906\tvalid_1's rmse: 0.0168222\n",
      "[4400]\ttraining's rmse: 0.0089814\tvalid_1's rmse: 0.0167823\n",
      "[4500]\ttraining's rmse: 0.00885547\tvalid_1's rmse: 0.0167355\n",
      "[4600]\ttraining's rmse: 0.00874752\tvalid_1's rmse: 0.0166964\n",
      "[4700]\ttraining's rmse: 0.00863694\tvalid_1's rmse: 0.0166576\n",
      "[4800]\ttraining's rmse: 0.00852445\tvalid_1's rmse: 0.0166199\n",
      "[4900]\ttraining's rmse: 0.00840959\tvalid_1's rmse: 0.0165734\n",
      "[5000]\ttraining's rmse: 0.00829297\tvalid_1's rmse: 0.016529\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00829297\tvalid_1's rmse: 0.016529\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.146694\tvalid_1's rmse: 0.14885\n",
      "[200]\ttraining's rmse: 0.094697\tvalid_1's rmse: 0.0977012\n",
      "[300]\ttraining's rmse: 0.0649493\tvalid_1's rmse: 0.0683812\n",
      "[400]\ttraining's rmse: 0.0505287\tvalid_1's rmse: 0.0543297\n",
      "[500]\ttraining's rmse: 0.041038\tvalid_1's rmse: 0.0450594\n",
      "[600]\ttraining's rmse: 0.0347931\tvalid_1's rmse: 0.038964\n",
      "[700]\ttraining's rmse: 0.0300749\tvalid_1's rmse: 0.0343298\n",
      "[800]\ttraining's rmse: 0.0266918\tvalid_1's rmse: 0.0310385\n",
      "[900]\ttraining's rmse: 0.0240871\tvalid_1's rmse: 0.0285069\n",
      "[1000]\ttraining's rmse: 0.0220686\tvalid_1's rmse: 0.0265704\n",
      "[1100]\ttraining's rmse: 0.0205346\tvalid_1's rmse: 0.0251366\n",
      "[1200]\ttraining's rmse: 0.0192764\tvalid_1's rmse: 0.0239513\n",
      "[1300]\ttraining's rmse: 0.0182002\tvalid_1's rmse: 0.0229218\n",
      "[1400]\ttraining's rmse: 0.0172929\tvalid_1's rmse: 0.0221009\n",
      "[1500]\ttraining's rmse: 0.0165634\tvalid_1's rmse: 0.0214342\n",
      "[1600]\ttraining's rmse: 0.0159487\tvalid_1's rmse: 0.0208945\n",
      "[1700]\ttraining's rmse: 0.0153954\tvalid_1's rmse: 0.0204155\n",
      "[1800]\ttraining's rmse: 0.0149081\tvalid_1's rmse: 0.0200159\n",
      "[1900]\ttraining's rmse: 0.0144867\tvalid_1's rmse: 0.0196791\n",
      "[2000]\ttraining's rmse: 0.0141034\tvalid_1's rmse: 0.0193861\n",
      "[2100]\ttraining's rmse: 0.0137391\tvalid_1's rmse: 0.0191188\n",
      "[2200]\ttraining's rmse: 0.0134259\tvalid_1's rmse: 0.0188969\n",
      "[2300]\ttraining's rmse: 0.0131388\tvalid_1's rmse: 0.0187232\n",
      "[2400]\ttraining's rmse: 0.0128712\tvalid_1's rmse: 0.018567\n",
      "[2500]\ttraining's rmse: 0.0125967\tvalid_1's rmse: 0.0183906\n",
      "[2600]\ttraining's rmse: 0.0123543\tvalid_1's rmse: 0.0182345\n",
      "[2700]\ttraining's rmse: 0.0121067\tvalid_1's rmse: 0.0180828\n",
      "[2800]\ttraining's rmse: 0.0118736\tvalid_1's rmse: 0.0179221\n",
      "[2900]\ttraining's rmse: 0.0116861\tvalid_1's rmse: 0.0178203\n",
      "[3000]\ttraining's rmse: 0.0114873\tvalid_1's rmse: 0.0177117\n",
      "[3100]\ttraining's rmse: 0.0113116\tvalid_1's rmse: 0.0176307\n",
      "[3200]\ttraining's rmse: 0.0111452\tvalid_1's rmse: 0.0175566\n",
      "[3300]\ttraining's rmse: 0.0109708\tvalid_1's rmse: 0.0174641\n",
      "[3400]\ttraining's rmse: 0.0107909\tvalid_1's rmse: 0.0173728\n",
      "[3500]\ttraining's rmse: 0.0106363\tvalid_1's rmse: 0.017308\n",
      "[3600]\ttraining's rmse: 0.0104937\tvalid_1's rmse: 0.0172308\n",
      "[3700]\ttraining's rmse: 0.0103636\tvalid_1's rmse: 0.0171913\n",
      "[3800]\ttraining's rmse: 0.0102178\tvalid_1's rmse: 0.0171246\n",
      "[3900]\ttraining's rmse: 0.0100815\tvalid_1's rmse: 0.0170786\n",
      "[4000]\ttraining's rmse: 0.00995511\tvalid_1's rmse: 0.0170427\n",
      "[4100]\ttraining's rmse: 0.00981913\tvalid_1's rmse: 0.0169932\n",
      "[4200]\ttraining's rmse: 0.00969257\tvalid_1's rmse: 0.0169407\n",
      "[4300]\ttraining's rmse: 0.00956617\tvalid_1's rmse: 0.016899\n",
      "[4400]\ttraining's rmse: 0.00944439\tvalid_1's rmse: 0.0168604\n",
      "[4500]\ttraining's rmse: 0.00932451\tvalid_1's rmse: 0.0168185\n",
      "[4600]\ttraining's rmse: 0.00921162\tvalid_1's rmse: 0.0167758\n",
      "[4700]\ttraining's rmse: 0.00908429\tvalid_1's rmse: 0.0167227\n",
      "[4800]\ttraining's rmse: 0.00896937\tvalid_1's rmse: 0.0166781\n",
      "[4900]\ttraining's rmse: 0.00886799\tvalid_1's rmse: 0.0166509\n",
      "[5000]\ttraining's rmse: 0.00875657\tvalid_1's rmse: 0.0166106\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00875657\tvalid_1's rmse: 0.0166106\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.144006\tvalid_1's rmse: 0.145353\n",
      "[200]\ttraining's rmse: 0.0892032\tvalid_1's rmse: 0.0920158\n",
      "[300]\ttraining's rmse: 0.0617069\tvalid_1's rmse: 0.0651907\n",
      "[400]\ttraining's rmse: 0.0468928\tvalid_1's rmse: 0.0506377\n",
      "[500]\ttraining's rmse: 0.0373909\tvalid_1's rmse: 0.0411204\n",
      "[600]\ttraining's rmse: 0.0313354\tvalid_1's rmse: 0.0350443\n",
      "[700]\ttraining's rmse: 0.0274367\tvalid_1's rmse: 0.0312083\n",
      "[800]\ttraining's rmse: 0.0245705\tvalid_1's rmse: 0.0283969\n",
      "[900]\ttraining's rmse: 0.0223806\tvalid_1's rmse: 0.0262293\n",
      "[1000]\ttraining's rmse: 0.020601\tvalid_1's rmse: 0.0244614\n",
      "[1100]\ttraining's rmse: 0.019109\tvalid_1's rmse: 0.0229145\n",
      "[1200]\ttraining's rmse: 0.0180477\tvalid_1's rmse: 0.0218578\n",
      "[1300]\ttraining's rmse: 0.0171624\tvalid_1's rmse: 0.0209958\n",
      "[1400]\ttraining's rmse: 0.0164512\tvalid_1's rmse: 0.0203275\n",
      "[1500]\ttraining's rmse: 0.0158424\tvalid_1's rmse: 0.0197732\n",
      "[1600]\ttraining's rmse: 0.0153069\tvalid_1's rmse: 0.0192954\n",
      "[1700]\ttraining's rmse: 0.0148702\tvalid_1's rmse: 0.0189601\n",
      "[1800]\ttraining's rmse: 0.0144765\tvalid_1's rmse: 0.0186804\n",
      "[1900]\ttraining's rmse: 0.0140875\tvalid_1's rmse: 0.0184032\n",
      "[2000]\ttraining's rmse: 0.0136793\tvalid_1's rmse: 0.0181081\n",
      "[2100]\ttraining's rmse: 0.0133623\tvalid_1's rmse: 0.0178781\n",
      "[2200]\ttraining's rmse: 0.0130958\tvalid_1's rmse: 0.017724\n",
      "[2300]\ttraining's rmse: 0.0128026\tvalid_1's rmse: 0.0175284\n",
      "[2400]\ttraining's rmse: 0.0125532\tvalid_1's rmse: 0.0173788\n",
      "[2500]\ttraining's rmse: 0.0123241\tvalid_1's rmse: 0.017251\n",
      "[2600]\ttraining's rmse: 0.012071\tvalid_1's rmse: 0.0170898\n",
      "[2700]\ttraining's rmse: 0.0118246\tvalid_1's rmse: 0.0169472\n",
      "[2800]\ttraining's rmse: 0.0116399\tvalid_1's rmse: 0.0168634\n",
      "[2900]\ttraining's rmse: 0.0114238\tvalid_1's rmse: 0.0167339\n",
      "[3000]\ttraining's rmse: 0.0112523\tvalid_1's rmse: 0.0166614\n",
      "[3100]\ttraining's rmse: 0.0110797\tvalid_1's rmse: 0.0165667\n",
      "[3200]\ttraining's rmse: 0.0109214\tvalid_1's rmse: 0.016502\n",
      "[3300]\ttraining's rmse: 0.0107733\tvalid_1's rmse: 0.0164379\n",
      "[3400]\ttraining's rmse: 0.0106077\tvalid_1's rmse: 0.0163534\n",
      "[3500]\ttraining's rmse: 0.0104452\tvalid_1's rmse: 0.0162645\n",
      "[3600]\ttraining's rmse: 0.0103044\tvalid_1's rmse: 0.0162175\n",
      "[3700]\ttraining's rmse: 0.0101799\tvalid_1's rmse: 0.0161835\n",
      "[3800]\ttraining's rmse: 0.0100243\tvalid_1's rmse: 0.0161036\n",
      "[3900]\ttraining's rmse: 0.00987543\tvalid_1's rmse: 0.0160254\n",
      "[4000]\ttraining's rmse: 0.00974301\tvalid_1's rmse: 0.0159746\n",
      "[4100]\ttraining's rmse: 0.0095902\tvalid_1's rmse: 0.0158994\n",
      "[4200]\ttraining's rmse: 0.0094588\tvalid_1's rmse: 0.0158324\n",
      "[4300]\ttraining's rmse: 0.00935252\tvalid_1's rmse: 0.0157875\n",
      "[4400]\ttraining's rmse: 0.00924382\tvalid_1's rmse: 0.0157613\n",
      "[4500]\ttraining's rmse: 0.00914303\tvalid_1's rmse: 0.0157342\n",
      "[4600]\ttraining's rmse: 0.00902682\tvalid_1's rmse: 0.0156813\n",
      "[4700]\ttraining's rmse: 0.00891913\tvalid_1's rmse: 0.0156357\n",
      "[4800]\ttraining's rmse: 0.00882402\tvalid_1's rmse: 0.0156077\n",
      "[4900]\ttraining's rmse: 0.00871741\tvalid_1's rmse: 0.0155709\n",
      "[5000]\ttraining's rmse: 0.0086214\tvalid_1's rmse: 0.0155443\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4998]\ttraining's rmse: 0.00862285\tvalid_1's rmse: 0.0155435\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.143388\tvalid_1's rmse: 0.146746\n",
      "[200]\ttraining's rmse: 0.0919902\tvalid_1's rmse: 0.0965504\n",
      "[300]\ttraining's rmse: 0.0637515\tvalid_1's rmse: 0.0686144\n",
      "[400]\ttraining's rmse: 0.0501309\tvalid_1's rmse: 0.0550585\n",
      "[500]\ttraining's rmse: 0.0412311\tvalid_1's rmse: 0.0461409\n",
      "[600]\ttraining's rmse: 0.0352028\tvalid_1's rmse: 0.0402152\n",
      "[700]\ttraining's rmse: 0.0304384\tvalid_1's rmse: 0.0354709\n",
      "[800]\ttraining's rmse: 0.0269519\tvalid_1's rmse: 0.0320083\n",
      "[900]\ttraining's rmse: 0.0241507\tvalid_1's rmse: 0.0291705\n",
      "[1000]\ttraining's rmse: 0.0219888\tvalid_1's rmse: 0.0269907\n",
      "[1100]\ttraining's rmse: 0.0203269\tvalid_1's rmse: 0.0252853\n",
      "[1200]\ttraining's rmse: 0.0189755\tvalid_1's rmse: 0.0238853\n",
      "[1300]\ttraining's rmse: 0.0178022\tvalid_1's rmse: 0.0226597\n",
      "[1400]\ttraining's rmse: 0.0168297\tvalid_1's rmse: 0.0216327\n",
      "[1500]\ttraining's rmse: 0.016018\tvalid_1's rmse: 0.0207802\n",
      "[1600]\ttraining's rmse: 0.0153407\tvalid_1's rmse: 0.0200804\n",
      "[1700]\ttraining's rmse: 0.0147686\tvalid_1's rmse: 0.0195276\n",
      "[1800]\ttraining's rmse: 0.0142672\tvalid_1's rmse: 0.0190597\n",
      "[1900]\ttraining's rmse: 0.013823\tvalid_1's rmse: 0.0186332\n",
      "[2000]\ttraining's rmse: 0.0134047\tvalid_1's rmse: 0.0182432\n",
      "[2100]\ttraining's rmse: 0.0130459\tvalid_1's rmse: 0.0179394\n",
      "[2200]\ttraining's rmse: 0.0127235\tvalid_1's rmse: 0.0176714\n",
      "[2300]\ttraining's rmse: 0.0124382\tvalid_1's rmse: 0.0174688\n",
      "[2400]\ttraining's rmse: 0.0121556\tvalid_1's rmse: 0.0172594\n",
      "[2500]\ttraining's rmse: 0.0118931\tvalid_1's rmse: 0.0170642\n",
      "[2600]\ttraining's rmse: 0.0116502\tvalid_1's rmse: 0.0168638\n",
      "[2700]\ttraining's rmse: 0.011417\tvalid_1's rmse: 0.016704\n",
      "[2800]\ttraining's rmse: 0.0111821\tvalid_1's rmse: 0.0165243\n",
      "[2900]\ttraining's rmse: 0.0109925\tvalid_1's rmse: 0.0164026\n",
      "[3000]\ttraining's rmse: 0.0107891\tvalid_1's rmse: 0.0162783\n",
      "[3100]\ttraining's rmse: 0.0106036\tvalid_1's rmse: 0.0161589\n",
      "[3200]\ttraining's rmse: 0.0104179\tvalid_1's rmse: 0.0160465\n",
      "[3300]\ttraining's rmse: 0.0102475\tvalid_1's rmse: 0.0159497\n",
      "[3400]\ttraining's rmse: 0.0100854\tvalid_1's rmse: 0.015851\n",
      "[3500]\ttraining's rmse: 0.00993402\tvalid_1's rmse: 0.015769\n",
      "[3600]\ttraining's rmse: 0.00977445\tvalid_1's rmse: 0.0156749\n",
      "[3700]\ttraining's rmse: 0.00963139\tvalid_1's rmse: 0.0156059\n",
      "[3800]\ttraining's rmse: 0.00949084\tvalid_1's rmse: 0.0155325\n",
      "[3900]\ttraining's rmse: 0.00935482\tvalid_1's rmse: 0.0154644\n",
      "[4000]\ttraining's rmse: 0.0092259\tvalid_1's rmse: 0.0153987\n",
      "[4100]\ttraining's rmse: 0.00909374\tvalid_1's rmse: 0.0153313\n",
      "[4200]\ttraining's rmse: 0.00897543\tvalid_1's rmse: 0.0152729\n",
      "[4300]\ttraining's rmse: 0.00885406\tvalid_1's rmse: 0.0152276\n",
      "[4400]\ttraining's rmse: 0.00873637\tvalid_1's rmse: 0.0151784\n",
      "[4500]\ttraining's rmse: 0.00862533\tvalid_1's rmse: 0.0151231\n",
      "[4600]\ttraining's rmse: 0.00852205\tvalid_1's rmse: 0.0150823\n",
      "[4700]\ttraining's rmse: 0.00841529\tvalid_1's rmse: 0.0150403\n",
      "[4800]\ttraining's rmse: 0.00831381\tvalid_1's rmse: 0.0150015\n",
      "[4900]\ttraining's rmse: 0.008203\tvalid_1's rmse: 0.0149533\n",
      "[5000]\ttraining's rmse: 0.00809997\tvalid_1's rmse: 0.0149104\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00809997\tvalid_1's rmse: 0.0149104\n",
      "\n",
      "✅ Global Metrics:\n",
      "MAE: 0.0000\n",
      "RMSE: 0.0001\n",
      "MAPE: 3304860762385.5859\n",
      "SMAPE: 36.5701\n",
      "R2: 0.9960\n",
      "Peak_Error: 0.0001\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_trained/XGBoost_0.0.2_moving_avg/feature_and_target.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 179\u001b[39m\n\u001b[32m    176\u001b[39m     mlflow.log_metrics(global_metrics)\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# Save feature/target mapping\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_trained/XGBoost_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mversion\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/feature_and_target.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    180\u001b[39m     json.dump(feature_and_target, f, indent=\u001b[32m4\u001b[39m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ XGBoost training completed\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'model_trained/XGBoost_0.0.2_moving_avg/feature_and_target.json'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import mlflow\n",
    "import json\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true = np.where(y_true == 0, np.finfo(float).eps, y_true)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    denominator = np.where(denominator == 0, np.finfo(float).eps, denominator)\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / denominator)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_value = np.percentile(y_true, percentile)\n",
    "    peak_indices = y_true >= peak_value\n",
    "    if np.sum(peak_indices) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_indices] - y_pred[peak_indices]))\n",
    "\n",
    "# -----------------------------\n",
    "# Paths and parameters\n",
    "# -----------------------------\n",
    "root = './dataset/junctions/'\n",
    "version = '0.0.2_moving_avg'\n",
    "os.makedirs(f'./model_trained/LightGBM_{version}', exist_ok=True)\n",
    "mlflow.set_tracking_uri(\"file:model_trained/mlruns\")\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=['junction', 'MAE', 'RMSE', 'MAPE', 'SMAPE', 'R2', 'Peak_Error'])\n",
    "feature_and_target = {}\n",
    "\n",
    "lag_steps = [1,2,3]\n",
    "rolling_windows = [3,6,12]  # moving averages (in timesteps)\n",
    "\n",
    "# lists to store global predictions\n",
    "global_y_true, global_y_pred = [], []\n",
    "\n",
    "with mlflow.start_run(run_name=f\"LightGBM_{version}\"):\n",
    "\n",
    "    for filename in os.listdir(root)[:70]:\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        junction = os.path.splitext(filename)[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Lag features\n",
    "        # -----------------------------\n",
    "        for lag in lag_steps:\n",
    "            df[f'{junction}_lag{lag}'] = df[junction].shift(lag)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Moving average features\n",
    "        # -----------------------------\n",
    "        for w in rolling_windows:\n",
    "            df[f'{junction}_rollmean{w}'] = df[junction].rolling(window=w, min_periods=1).mean()\n",
    "\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        features = [col for col in df.columns if col != junction]\n",
    "        target = junction\n",
    "        feature_and_target[junction] = {\"target\": target, \"features\": features, 'lags':lag_steps ,'rolling':rolling_windows}\n",
    "\n",
    "        # -----------------------------\n",
    "        # Scaling\n",
    "        # -----------------------------\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        target_scaler = MinMaxScaler()\n",
    "        df[features] = feature_scaler.fit_transform(df[features])\n",
    "        df[[target]] = target_scaler.fit_transform(df[[target]])\n",
    "\n",
    "        # Save scalers\n",
    "        os.makedirs(f'model_trained/LightGBM_{version}/scalers', exist_ok=True)\n",
    "        with open(f'model_trained/LightGBM_{version}/scalers/{junction}_feature_scaler.save', 'wb') as f:\n",
    "            pickle.dump(feature_scaler, f)\n",
    "        with open(f'model_trained/LightGBM_{version}/scalers/{junction}_target_scaler.save', 'wb') as f:\n",
    "            pickle.dump(target_scaler, f)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Train/test split by scenario_id\n",
    "        # -----------------------------\n",
    "        scenario_ids = df['scenario_id'].unique()\n",
    "        split = int(0.8 * len(scenario_ids))\n",
    "        train_scenarios = scenario_ids[:split]\n",
    "        test_scenarios = scenario_ids[split:]\n",
    "        train_df = df[df['scenario_id'].isin(train_scenarios)]\n",
    "        test_df = df[df['scenario_id'].isin(test_scenarios)]\n",
    "\n",
    "        X_train, y_train = train_df[features], train_df[target]\n",
    "        X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Train LightGBM\n",
    "        # -----------------------------\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'learning_rate': 0.01,\n",
    "            'num_leaves': 31,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        start_time = time.time()\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=5000,\n",
    "            valid_sets=[train_data, valid_data],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=100),\n",
    "                lgb.log_evaluation(period=100)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # -----------------------------\n",
    "        # Prediction & evaluation\n",
    "        # -----------------------------\n",
    "        y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "        y_test_inv = target_scaler.inverse_transform(y_test.values.reshape(-1,1))\n",
    "        y_pred_inv = target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
    "\n",
    "        # store for global metrics\n",
    "        global_y_true.append(y_test_inv)\n",
    "        global_y_pred.append(y_pred_inv)\n",
    "\n",
    "        mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "        mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv)\n",
    "        smape_val = smape(y_test_inv, y_pred_inv)\n",
    "        r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "        peak_err = peak_error(y_test_inv, y_pred_inv)\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
    "            'junction': [junction],\n",
    "            'MAE': [mae],\n",
    "            'RMSE': [rmse],\n",
    "            'MAPE': [mape],\n",
    "            'SMAPE': [smape_val],\n",
    "            'R2': [r2],\n",
    "            'Peak_Error': [peak_err]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "        # Save model\n",
    "        model.save_model(f'model_trained/LightGBM_{version}/{junction}.txt')\n",
    "    # -----------------------------\n",
    "    # Global metrics across all junctions\n",
    "    # -----------------------------\n",
    "    all_y_true = np.vstack(global_y_true)\n",
    "    all_y_pred = np.vstack(global_y_pred)\n",
    "\n",
    "    global_metrics = {\n",
    "        'MAE': mean_absolute_error(all_y_true, all_y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(all_y_true, all_y_pred)),\n",
    "        'MAPE': mean_absolute_percentage_error(all_y_true, all_y_pred),\n",
    "        'SMAPE': smape(all_y_true, all_y_pred),\n",
    "        'R2': r2_score(all_y_true, all_y_pred),\n",
    "        'Peak_Error': peak_error(all_y_true, all_y_pred)\n",
    "    }\n",
    "\n",
    "    print(\"\\n✅ Global Metrics:\")\n",
    "    for k,v in global_metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    mlflow.log_metrics(global_metrics)\n",
    "\n",
    "# Save feature/target mapping\n",
    "with open(f'model_trained/LightGBM_{version}/feature_and_target.json', 'w') as f:\n",
    "    json.dump(feature_and_target, f, indent=4)\n",
    "\n",
    "print(\"✅ XGBoost training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1e2c9",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22113fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.25956\n",
      "[100]\tvalidation_0-rmse:0.24196\n",
      "[200]\tvalidation_0-rmse:0.23707\n",
      "[300]\tvalidation_0-rmse:0.23595\n",
      "[400]\tvalidation_0-rmse:0.23553\n",
      "[500]\tvalidation_0-rmse:0.23557\n",
      "[600]\tvalidation_0-rmse:0.23562\n",
      "[700]\tvalidation_0-rmse:0.23572\n",
      "[800]\tvalidation_0-rmse:0.23584\n",
      "[900]\tvalidation_0-rmse:0.23603\n",
      "[1000]\tvalidation_0-rmse:0.23627\n",
      "[1100]\tvalidation_0-rmse:0.23658\n",
      "[1200]\tvalidation_0-rmse:0.23676\n",
      "[1300]\tvalidation_0-rmse:0.23706\n",
      "[1400]\tvalidation_0-rmse:0.23728\n",
      "[1500]\tvalidation_0-rmse:0.23749\n",
      "[1600]\tvalidation_0-rmse:0.23775\n",
      "[1700]\tvalidation_0-rmse:0.23790\n",
      "[1800]\tvalidation_0-rmse:0.23809\n",
      "[1900]\tvalidation_0-rmse:0.23832\n",
      "[2000]\tvalidation_0-rmse:0.23854\n",
      "[2100]\tvalidation_0-rmse:0.23879\n",
      "[2200]\tvalidation_0-rmse:0.23904\n",
      "[2300]\tvalidation_0-rmse:0.23924\n",
      "[2400]\tvalidation_0-rmse:0.23943\n",
      "[2500]\tvalidation_0-rmse:0.23967\n",
      "[2600]\tvalidation_0-rmse:0.23981\n",
      "[2700]\tvalidation_0-rmse:0.23996\n",
      "[2800]\tvalidation_0-rmse:0.24010\n",
      "[2900]\tvalidation_0-rmse:0.24031\n",
      "[3000]\tvalidation_0-rmse:0.24051\n",
      "[3100]\tvalidation_0-rmse:0.24068\n",
      "[3200]\tvalidation_0-rmse:0.24083\n",
      "[3300]\tvalidation_0-rmse:0.24100\n",
      "[3400]\tvalidation_0-rmse:0.24120\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m     97\u001b[39m model = xgb.XGBRegressor(\n\u001b[32m     98\u001b[39m     n_estimators=\u001b[32m5000\u001b[39m,\n\u001b[32m     99\u001b[39m     learning_rate=\u001b[32m0.01\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    104\u001b[39m     verbosity=\u001b[32m0\u001b[39m\n\u001b[32m    105\u001b[39m )\n\u001b[32m    107\u001b[39m start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m    112\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m elapsed_sec = time.time() - start_time\n\u001b[32m    114\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjunction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_sec\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sec\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/sklearn.py:1247\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1245\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/core.py:2247\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2246\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     )\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import mlflow\n",
    "import json\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true = np.where(y_true == 0, np.finfo(float).eps, y_true)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    denominator = np.where(denominator == 0, np.finfo(float).eps, denominator)\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / denominator)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_value = np.percentile(y_true, percentile)\n",
    "    peak_indices = y_true >= peak_value\n",
    "    if np.sum(peak_indices) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_indices] - y_pred[peak_indices]))\n",
    "\n",
    "# -----------------------------\n",
    "# Paths and parameters\n",
    "# -----------------------------\n",
    "root = './dataset/junctions/'\n",
    "version = '0.0.1'\n",
    "os.makedirs(f'./model_trained/XGBoost_{version}', exist_ok=True)\n",
    "mlflow.set_tracking_uri(\"file:model_trained/mlruns\")\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=['junction', 'MAE', 'RMSE', 'MAPE', 'SMAPE', 'R2', 'Peak_Error'])\n",
    "feature_and_target = {}\n",
    "\n",
    "lag_steps = [1,2,3]\n",
    "\n",
    "# lists to store global predictions\n",
    "global_y_true, global_y_pred = [], []\n",
    "\n",
    "with mlflow.start_run(run_name=f\"XGBoost_{version}\"):\n",
    "\n",
    "    for filename in os.listdir(root)[:70]:\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        junction = os.path.splitext(filename)[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Create lag features\n",
    "        # -----------------------------\n",
    "        for lag in lag_steps:\n",
    "            df[f'{junction}_lag{lag}'] = df[junction].shift(lag)\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        features = [col for col in df.columns if col != junction]\n",
    "        target = junction\n",
    "        feature_and_target[junction] = {\"target\": target, \"features\": features, 'lags':lag_steps }\n",
    "\n",
    "        # -----------------------------\n",
    "        # Scaling\n",
    "        # -----------------------------\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        target_scaler = MinMaxScaler()\n",
    "\n",
    "        df[features] = feature_scaler.fit_transform(df[features])\n",
    "        df[[target]] = target_scaler.fit_transform(df[[target]])\n",
    "\n",
    "        os.makedirs(f'model_trained/XGBoost_{version}/scalers', exist_ok=True)\n",
    "        with open(f'model_trained/XGBoost_{version}/scalers/{junction}_feature_scaler.save', 'wb') as f:\n",
    "            pickle.dump(feature_scaler, f)\n",
    "        with open(f'model_trained/XGBoost_{version}/scalers/{junction}_target_scaler.save', 'wb') as f:\n",
    "            pickle.dump(target_scaler, f)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Train/test split by scenario_id\n",
    "        # -----------------------------\n",
    "        scenario_ids = df['scenario_id'].unique()\n",
    "        split = int(0.8 * len(scenario_ids))\n",
    "        train_scenarios = scenario_ids[:split]\n",
    "        test_scenarios = scenario_ids[split:]\n",
    "\n",
    "        train_df = df[df['scenario_id'].isin(train_scenarios)]\n",
    "        test_df = df[df['scenario_id'].isin(test_scenarios)]\n",
    "\n",
    "        X_train, y_train = train_df[features], train_df[target]\n",
    "        X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Train XGBoost Regressor\n",
    "        # -----------------------------\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=5000,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective='reg:squarederror',\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            verbose=100\n",
    "        )\n",
    "        elapsed_sec = time.time() - start_time\n",
    "        print(f\"{junction} training time: {elapsed_sec:.2f} sec\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Prediction & evaluation\n",
    "        # -----------------------------\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_test_inv = target_scaler.inverse_transform(y_test.values.reshape(-1,1))\n",
    "        y_pred_inv = target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
    "\n",
    "        # store for global metrics\n",
    "        global_y_true.append(y_test_inv)\n",
    "        global_y_pred.append(y_pred_inv)\n",
    "\n",
    "        mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "        mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv)\n",
    "        smape_val = smape(y_test_inv, y_pred_inv)\n",
    "        r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "        peak_err = peak_error(y_test_inv, y_pred_inv)\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
    "            'junction': [junction],\n",
    "            'MAE': [mae],\n",
    "            'RMSE': [rmse],\n",
    "            'MAPE': [mape],\n",
    "            'SMAPE': [smape_val],\n",
    "            'R2': [r2],\n",
    "            'Peak_Error': [peak_err]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "        # Save model\n",
    "        model.save_model(f'model_trained/XGBoost_{version}/{junction}.json')\n",
    "\n",
    "    # -----------------------------\n",
    "    # Global metrics across all junctions\n",
    "    # -----------------------------\n",
    "    all_y_true = np.vstack(global_y_true)\n",
    "    all_y_pred = np.vstack(global_y_pred)\n",
    "\n",
    "    global_metrics = {\n",
    "        'MAE': mean_absolute_error(all_y_true, all_y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(all_y_true, all_y_pred)),\n",
    "        'MAPE': mean_absolute_percentage_error(all_y_true, all_y_pred),\n",
    "        'SMAPE': smape(all_y_true, all_y_pred),\n",
    "        'R2': r2_score(all_y_true, all_y_pred),\n",
    "        'Peak_Error': peak_error(all_y_true, all_y_pred)\n",
    "    }\n",
    "\n",
    "    print(\"\\n✅ Global Metrics:\")\n",
    "    for k,v in global_metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    mlflow.log_metrics(global_metrics)\n",
    "\n",
    "# Save feature/target mapping\n",
    "with open(f'model_trained/XGBoost_{version}/feature_and_target.json', 'w') as f:\n",
    "    json.dump(feature_and_target, f, indent=4)\n",
    "\n",
    "print(\"✅ XGBoost training completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284d6a1",
   "metadata": {},
   "source": [
    "Prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d882854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13570/3798897201.py:63: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_prophet['y'] = train_prophet['y'].fillna(method='ffill')\n",
      "/tmp/ipykernel_13570/3798897201.py:64: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_prophet['y'] = test_prophet['y'].fillna(method='ffill')\n",
      "21:15:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:48 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J434 training time: 3.29 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13570/3798897201.py:98: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  metrics_df = pd.concat([metrics_df, pd.DataFrame({\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Prophet' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 109\u001b[39m\n\u001b[32m     98\u001b[39m     metrics_df = pd.concat([metrics_df, pd.DataFrame({\n\u001b[32m     99\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mjunction\u001b[39m\u001b[33m'\u001b[39m: [junction],\n\u001b[32m    100\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mMAE\u001b[39m\u001b[33m'\u001b[39m: [mae],\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mPeak_Error\u001b[39m\u001b[33m'\u001b[39m: [peak_err]\n\u001b[32m    106\u001b[39m     })], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmodel_trained/Prophet_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjunction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Global metrics\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    114\u001b[39m all_y_true = np.concatenate(global_y_true)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Prophet' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import os\n",
    "import pickle\n",
    "import mlflow\n",
    "import json\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true = np.where(y_true == 0, np.finfo(float).eps, y_true)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    denominator = np.where(denominator == 0, np.finfo(float).eps, denominator)\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / denominator)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_value = np.percentile(y_true, percentile)\n",
    "    peak_indices = y_true >= peak_value\n",
    "    if np.sum(peak_indices) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_indices] - y_pred[peak_indices]))\n",
    "\n",
    "# -----------------------------\n",
    "# Paths and parameters\n",
    "# -----------------------------\n",
    "root = './dataset/junctions/'\n",
    "version = '0.0.1'\n",
    "os.makedirs(f'./model_trained/Prophet_{version}', exist_ok=True)\n",
    "mlflow.set_tracking_uri(\"file:model_trained/mlruns\")\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=['junction', 'MAE', 'RMSE', 'MAPE', 'SMAPE', 'R2', 'Peak_Error'])\n",
    "feature_and_target = {}\n",
    "\n",
    "global_y_true, global_y_pred = [], []\n",
    "\n",
    "with mlflow.start_run(run_name=f\"Prophet_{version}\"):\n",
    "\n",
    "    for filename in os.listdir(root)[:70]:\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        junction = os.path.splitext(filename)[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Scenario-based train/test split\n",
    "        # -----------------------------\n",
    "        scenario_ids = df['scenario_id'].unique()\n",
    "        split_idx = int(0.8 * len(scenario_ids))\n",
    "        train_scenarios = scenario_ids[:split_idx]\n",
    "        test_scenarios = scenario_ids[split_idx:]\n",
    "\n",
    "        train_df = df[df['scenario_id'].isin(train_scenarios)]\n",
    "        test_df = df[df['scenario_id'].isin(test_scenarios)]\n",
    "\n",
    "        # Prepare for Prophet\n",
    "        train_prophet = train_df[['time_id', junction]].rename(columns={'time_id': 'ds', junction: 'y'})\n",
    "        test_prophet = test_df[['time_id', junction]].rename(columns={'time_id': 'ds', junction: 'y'})\n",
    "        train_prophet['y'] = train_prophet['y'].fillna(method='ffill')\n",
    "        test_prophet['y'] = test_prophet['y'].fillna(method='ffill')\n",
    "\n",
    "        feature_and_target[junction] = {\"target\": junction}\n",
    "\n",
    "        # -----------------------------\n",
    "        # Train Prophet\n",
    "        # -----------------------------\n",
    "        model = Prophet(daily_seasonality=False)\n",
    "        start_time = time.time()\n",
    "        model.fit(train_prophet)\n",
    "        elapsed_sec = time.time() - start_time\n",
    "        print(f\"{junction} training time: {elapsed_sec:.2f} sec\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Forecast test scenarios\n",
    "        # -----------------------------\n",
    "        future = test_prophet[['ds']]\n",
    "        forecast = model.predict(future)\n",
    "        y_pred = forecast['yhat'].values\n",
    "        y_true = test_prophet['y'].values\n",
    "\n",
    "        global_y_true.append(y_true)\n",
    "        global_y_pred.append(y_pred)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Metrics per junction\n",
    "        # -----------------------------\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "        smape_val = smape(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        peak_err = peak_error(y_true, y_pred)\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
    "            'junction': [junction],\n",
    "            'MAE': [mae],\n",
    "            'RMSE': [rmse],\n",
    "            'MAPE': [mape],\n",
    "            'SMAPE': [smape_val],\n",
    "            'R2': [r2],\n",
    "            'Peak_Error': [peak_err]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "        # Save model\n",
    "        with open(f'model_trained/Prophet_{version}/{junction}.pkl', 'wb') as f:\n",
    "            pickle.dump(model,f)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Global metrics\n",
    "    # -----------------------------\n",
    "    all_y_true = np.concatenate(global_y_true)\n",
    "    all_y_pred = np.concatenate(global_y_pred)\n",
    "\n",
    "    global_metrics = {\n",
    "        'MAE': mean_absolute_error(all_y_true, all_y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(all_y_true, all_y_pred)),\n",
    "        'MAPE': mean_absolute_percentage_error(all_y_true, all_y_pred),\n",
    "        'SMAPE': smape(all_y_true, all_y_pred),\n",
    "        'R2': r2_score(all_y_true, all_y_pred),\n",
    "        'Peak_Error': peak_error(all_y_true, all_y_pred)\n",
    "    }\n",
    "\n",
    "    print(\"\\n✅ Global Metrics:\")\n",
    "    for k,v in global_metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    mlflow.log_metrics(global_metrics)\n",
    "\n",
    "# Save feature/target mapping\n",
    "with open(f'model_trained/Prophet_{version}/feature_and_target.json', 'w') as f:\n",
    "    json.dump(feature_and_target, f, indent=4)\n",
    "\n",
    "print(\"✅ Prophet training completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digitaltwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
