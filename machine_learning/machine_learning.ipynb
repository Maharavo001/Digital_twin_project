{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa778bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wntr\n",
    "import pandas as pd\n",
    "\n",
    "inp_file = '/mnt/data/home/zayd/Digital_twin_project/inp_networks/Anytown.inp'\n",
    "wn= wntr.network.WaterNetworkModel(inp_file)\n",
    "\n",
    "junction = []\n",
    "for junc_name, junc in wn.junctions():\n",
    "    junction.append([junc_name,junc.coordinates[0],junc.coordinates[1]])\n",
    "df = pd.DataFrame(junction, columns=['name','X','Y'])\n",
    "df.to_csv('coordinate.csv', index=False)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db6615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv('coordinate.csv')\n",
    "\n",
    "coords = df[['X','Y']].values\n",
    "\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters= k,random_state = 42)\n",
    "df['zone'] = kmeans.fit_predict(coords)\n",
    "\n",
    "df.to_csv(\"neighbors.csv\", index=False)\n",
    "\n",
    "plt.scatter(df['X'],df['Y'], c = df['zone'],cmap = \"Set1\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Junction Zones\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7855e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "# Paths\n",
    "root_p = '/mnt/data/home/zayd/Digital_twin_project/machine_learning'\n",
    "out_dir = Path(f\"{root_p}/dataset/zones_v2\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load main demand data\n",
    "demand = pd.read_csv(f\"{root_p}/dataset/generated_v2.csv\")\n",
    "\n",
    "# Load junction → zone mapping (columns: ['name','zone'])\n",
    "links = pd.read_csv(\"zones.csv\")\n",
    "\n",
    "# Group junctions by zone\n",
    "zone_groups = links.groupby(\"zone\")[\"name\"].apply(list)\n",
    "\n",
    "# Process each zone\n",
    "for zone_id, junctions in zone_groups.items():\n",
    "    logging.info(f\"Processing Zone {zone_id} with {len(junctions)} junctions\")\n",
    "\n",
    "    # Start with time_id and day columns\n",
    "    zone_df = demand[[\"time_id\", \"day\"]].copy()\n",
    "\n",
    "    # Merge all junctions for this zone\n",
    "    for j in junctions:\n",
    "        if j in demand.columns:\n",
    "            zone_df = zone_df.merge(\n",
    "                demand[[\"time_id\", \"day\", j]].rename(columns={j: f\"{j}_demand\"}),\n",
    "                on=[\"time_id\", \"day\"],\n",
    "                how=\"left\"\n",
    "            )\n",
    "\n",
    "    # Save parquet per zone\n",
    "    out_path = out_dir / f\"zone_{zone_id}.parquet\"\n",
    "    zone_df.to_parquet(out_path, engine=\"pyarrow\", index=False)\n",
    "    logging.info(f\"Saved Zone {zone_id} → {out_path} with shape {zone_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cffc9a",
   "metadata": {},
   "source": [
    "LTSM (single junction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fecb3967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 13:02:36.971237: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-14 13:02:36.978457: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-14 13:02:37.002239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757851357.046008   11861 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757851357.061914   11861 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1757851357.092176   11861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757851357.092193   11861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757851357.092196   11861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757851357.092200   11861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-14 13:02:37.101470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 13:02:43.198850: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 628093120.0000 - val_loss: 26941644.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4388324.5000 - val_loss: 559751.5000\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 503659.9688 - val_loss: 586804.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 324005.0000 - val_loss: 660382.3750\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 256895.7812 - val_loss: 606876.9375\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 187354.0625 - val_loss: 620487.7500\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 91983.3984 - val_loss: 431693.5312\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 74122.4609 - val_loss: 428535.6250\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 76964.9141 - val_loss: 441944.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 57136.5234 - val_loss: 353659.7500\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 58483.7344 - val_loss: 294169.1875\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 60368.0000 - val_loss: 694225.1875\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 177768.1094 - val_loss: 517793.3750\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 99181.5938 - val_loss: 273946.3438\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 53353.8672 - val_loss: 237513.5781\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 91643.9531 - val_loss: 186422.0781\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 39237.0742 - val_loss: 162943.4844\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 22186.8906 - val_loss: 123459.6328\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 17994.2168 - val_loss: 109373.9609\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 15786.6221 - val_loss: 99557.5469\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 16060.4414 - val_loss: 87684.8281\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 11911.9766 - val_loss: 82097.1016\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 11188.0430 - val_loss: 76430.0312\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 10708.0312 - val_loss: 71235.7578\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8656.1523 - val_loss: 67601.4219\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9173.1855 - val_loss: 64610.1562\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9699.1074 - val_loss: 64166.9648\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8811.4121 - val_loss: 61076.5898\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6591.5464 - val_loss: 61538.5859\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4983.4756 - val_loss: 59821.3398\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7511.1704 - val_loss: 55078.1211\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7219.6660 - val_loss: 54101.7461\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5545.6582 - val_loss: 53254.8516\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4877.2080 - val_loss: 52834.9531\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4084.4104 - val_loss: 52432.4219\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4737.9941 - val_loss: 51701.3359\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5918.6812 - val_loss: 51044.3047\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 5566.1104 - val_loss: 50026.5312\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5018.6147 - val_loss: 49047.0352\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5602.0640 - val_loss: 47911.8438\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6632.6860 - val_loss: 46896.0859\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5974.7422 - val_loss: 45861.2500\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3919.8032 - val_loss: 44841.2734\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 6008.7461 - val_loss: 43561.8711\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4394.0659 - val_loss: 41897.3828\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4477.4028 - val_loss: 40153.5312\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 4664.9038 - val_loss: 38336.9844\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3862.5205 - val_loss: 36736.8516\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4750.9019 - val_loss: 35426.8359\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3839.6833 - val_loss: 33395.3086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 13:03:06 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J436' already exists. Creating a new version of this model...\n",
      "Created version '5' of model 'J436'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step \n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 129030984.0000 - val_loss: 8097523.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 31519680.0000 - val_loss: 62432500.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 34021944.0000 - val_loss: 8363672.5000\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5689235.5000 - val_loss: 2892160.7500\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1792532.5000 - val_loss: 821408.8750\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 321953.0000 - val_loss: 221121.3125\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 51571.0547 - val_loss: 123012.2578\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 22460.7871 - val_loss: 91922.9219\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 19311.4473 - val_loss: 81708.5234\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 19063.2383 - val_loss: 83247.2500\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 17669.2695 - val_loss: 76668.8438\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 19675.8496 - val_loss: 104049.6406\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 37107.2891 - val_loss: 104584.3906\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 24204.9707 - val_loss: 54307.5117\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 13037.6055 - val_loss: 45948.4062\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 10579.9873 - val_loss: 44865.4297\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10785.0098 - val_loss: 48605.5781\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11853.6230 - val_loss: 52803.1797\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 11411.5518 - val_loss: 51649.3086\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 11147.5654 - val_loss: 45236.4688\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 11058.4873 - val_loss: 51843.7891\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 11005.6689 - val_loss: 45478.0352\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 11100.2861 - val_loss: 44117.5781\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9928.5889 - val_loss: 60100.8516\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 10719.3076 - val_loss: 59421.1758\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 10578.3525 - val_loss: 60094.5312\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10642.7422 - val_loss: 58340.7812\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 10349.5605 - val_loss: 54753.3086\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10748.8574 - val_loss: 60191.7188\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 10639.2988 - val_loss: 53168.7148\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10574.5078 - val_loss: 52261.5586\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 10209.9805 - val_loss: 60260.5781\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 10592.6963 - val_loss: 59405.3945\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 10945.0723 - val_loss: 58995.8750\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 10486.4150 - val_loss: 61048.2031\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 10627.7441 - val_loss: 58841.4062\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10697.2812 - val_loss: 54177.6797\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 10834.5176 - val_loss: 58894.7305\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 10651.0420 - val_loss: 54348.0742\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 10506.2803 - val_loss: 54133.5000\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 10820.9072 - val_loss: 61814.0195\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 10854.6396 - val_loss: 61119.3984\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 10812.8252 - val_loss: 56926.8906\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 10459.5859 - val_loss: 41057.4297\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 10421.5439 - val_loss: 48272.7656\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 10934.1084 - val_loss: 41990.7812\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 10317.2100 - val_loss: 140880.4375\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 42861.8281 - val_loss: 96664.4922\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 12651.3359 - val_loss: 152402.7344\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9897.1396 - val_loss: 46353.3594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 13:03:45 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x71adc8474360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J27' already exists. Creating a new version of this model...\n",
      "Created version '5' of model 'J27'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x71ae496972e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step  \n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 2607721472.0000 - val_loss: 100466544.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 134718176.0000 - val_loss: 47268828.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 137202288.0000 - val_loss: 950255552.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 452845056.0000 - val_loss: 15830477.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 182231776.0000 - val_loss: 415572128.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 333624192.0000 - val_loss: 99268928.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 76611984.0000 - val_loss: 25062028.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 136844576.0000 - val_loss: 31918446.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 70385256.0000 - val_loss: 2287595.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1061915.2500 - val_loss: 15152.1348\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 60133.8477 - val_loss: 223387.5625\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 93130.1328 - val_loss: 64377.6992\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 14019.4375 - val_loss: 28586.0234\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6351.3882 - val_loss: 28498.0371\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 6074.0156 - val_loss: 24371.2227\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5472.3843 - val_loss: 18953.0605\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4964.5737 - val_loss: 21433.6621\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5300.3823 - val_loss: 22448.0645\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 5493.4365 - val_loss: 24575.8066\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5083.1782 - val_loss: 20035.6855\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 5467.4268 - val_loss: 49936.1992\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4489.5488 - val_loss: 49186.2344\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7197.1108 - val_loss: 25121.6973\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4154.7334 - val_loss: 25335.8867\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4051.7231 - val_loss: 21838.2520\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4148.0542 - val_loss: 24063.7109\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4171.0513 - val_loss: 20221.6895\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4203.5503 - val_loss: 20069.7754\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4306.8037 - val_loss: 19731.1934\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 4091.9336 - val_loss: 19998.7578\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4211.2583 - val_loss: 22209.4629\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4163.6479 - val_loss: 21877.6094\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4377.8096 - val_loss: 18925.5039\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3790.6196 - val_loss: 17831.3652\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4072.6453 - val_loss: 16815.0332\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3798.0198 - val_loss: 18486.6934\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 3628.0061 - val_loss: 12080.8965\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 3659.1775 - val_loss: 12070.6387\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4152.4146 - val_loss: 10836.5986\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3667.1890 - val_loss: 15713.2627\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 3754.2290 - val_loss: 20065.4727\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 3639.3015 - val_loss: 18685.4980\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 3663.1973 - val_loss: 17656.3672\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 4574.1030 - val_loss: 13022.1602\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3599.8672 - val_loss: 9321.1299\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3272.0295 - val_loss: 7770.7725\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3198.5337 - val_loss: 7737.9126\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3046.1970 - val_loss: 7328.8032\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 2992.3091 - val_loss: 8278.4834\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3027.2986 - val_loss: 7677.9131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 13:04:27 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J101' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J101'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step \n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 47683244.0000 - val_loss: 50151336.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 43439620.0000 - val_loss: 19427532.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 16751183.0000 - val_loss: 5076084.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4884320.0000 - val_loss: 1140119.6250\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1490096.5000 - val_loss: 32743.1426\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1307723.8750 - val_loss: 4112716.7500\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3091591.2500 - val_loss: 352786.1562\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 98838.8594 - val_loss: 47569.8398\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 26420.2793 - val_loss: 47911.0898\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6813.9683 - val_loss: 15358.9609\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2389.8435 - val_loss: 14772.5352\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3187.7646 - val_loss: 14631.6953\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 2618.9624 - val_loss: 13777.6504\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 2250.1267 - val_loss: 13224.0098\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2119.1116 - val_loss: 12942.1533\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2078.5310 - val_loss: 12469.3604\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2137.2002 - val_loss: 12077.2959\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 1689.3535 - val_loss: 11521.1123\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1889.5385 - val_loss: 11162.3037\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1743.0892 - val_loss: 10755.4727\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1478.4188 - val_loss: 9259.7979\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1518.2144 - val_loss: 8597.7373\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1381.3069 - val_loss: 8287.3467\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2304.7168 - val_loss: 8214.9688\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1344.3268 - val_loss: 8192.9746\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1401.2870 - val_loss: 7896.0024\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1116.1548 - val_loss: 7767.0151\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1206.2144 - val_loss: 7801.8706\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1007.3931 - val_loss: 7697.9863\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1139.6354 - val_loss: 3789.3955\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 688.5213 - val_loss: 2827.5696\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 607.8100 - val_loss: 2488.3550\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 506.3848 - val_loss: 2286.2822\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 479.5732 - val_loss: 2127.1777\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 511.7104 - val_loss: 1941.3030\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 432.6092 - val_loss: 1955.0205\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 429.3336 - val_loss: 2158.4646\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 571.4316 - val_loss: 1957.7047\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 464.2854 - val_loss: 1954.9797\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 439.8617 - val_loss: 1914.5878\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1279.2832 - val_loss: 28989.6719\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 16710.3906 - val_loss: 8756.8877\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4639.5752 - val_loss: 7657.0596\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1359.6842 - val_loss: 3287.7151\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 603.9086 - val_loss: 2820.6106\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 552.1179 - val_loss: 2874.7793\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 527.1987 - val_loss: 2772.3955\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 554.6014 - val_loss: 2735.6428\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 609.8368 - val_loss: 2718.4968\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 652.5834 - val_loss: 2575.3271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 13:05:06 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J309' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J309'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step \n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 48235.1133 - val_loss: 3956.7061\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2078.5593 - val_loss: 42.9811\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 35.5136 - val_loss: 2801.8928\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 15.2045 - val_loss: 30.9815\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1.9036 - val_loss: 19.6690\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6133 - val_loss: 30.6039\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.5388 - val_loss: 20.2450\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6668 - val_loss: 20.5879\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.6630 - val_loss: 20.6623\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5643 - val_loss: 20.6974\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4987 - val_loss: 20.6162\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5299 - val_loss: 20.6615\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6713 - val_loss: 20.6300\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.5153 - val_loss: 20.4861\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.5751 - val_loss: 20.2677\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6596 - val_loss: 3168.8169\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 1.7699 - val_loss: 3114.0225\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5614 - val_loss: 3025.7241\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6328 - val_loss: 2975.0945\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5840 - val_loss: 2898.5859\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.5049 - val_loss: 2825.5913\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6498 - val_loss: 2744.7715\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.4972 - val_loss: 28.2226\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.5272 - val_loss: 27.6283\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.5773 - val_loss: 26.9000\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4786 - val_loss: 26.5217\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5535 - val_loss: 13760.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1.6295 - val_loss: 12419.1162\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1.5003 - val_loss: 254376.4062\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.5712 - val_loss: 254392.0781\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5495 - val_loss: 254367.8125\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6704 - val_loss: 254334.5938\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5020 - val_loss: 254306.4375\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4185 - val_loss: 2760.7109\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.3405 - val_loss: 10742.8252\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.5551 - val_loss: 10343.3076\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.5172 - val_loss: 10196.7949\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4067 - val_loss: 10188.3623\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 1.5307 - val_loss: 10091.4180\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 1.5004 - val_loss: 10143.0605\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.4732 - val_loss: 10286.5000\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5653 - val_loss: 10379.8887\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1.4096 - val_loss: 256307.5625\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.5663 - val_loss: 256312.1562\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4830 - val_loss: 256256.8125\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4421 - val_loss: 256227.5000\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1.5728 - val_loss: 2605.1753\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4565 - val_loss: 2505.4768\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1.4744 - val_loss: 2450.2646\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 1.5111 - val_loss: 2382.6184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 13:05:46 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J280' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J280'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step \n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 25903434.0000 - val_loss: 19821914.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 21410670.0000 - val_loss: 15001326.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9776969.0000 - val_loss: 297055.0938\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3070787.0000 - val_loss: 45326.6211\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 115158.0938 - val_loss: 447741.9062\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 676348.3750 - val_loss: 492731.2812\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2222549.5000 - val_loss: 318357.9688\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 703454.2500 - val_loss: 161864.7969\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 81497.6875 - val_loss: 673445.5000\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 73587.1797 - val_loss: 205219.4219\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 36713.7891 - val_loss: 222306.2344\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 51232.7266 - val_loss: 493944.8750\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 70619.7500 - val_loss: 414641.4688\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 153048.9375 - val_loss: 131239.6875\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 223703.9375 - val_loss: 1330997.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 144326.6875 - val_loss: 91180.0469\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 15900.5742 - val_loss: 156718.2344\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 21215.9902 - val_loss: 185605.1250\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 27381.2910 - val_loss: 237204.3438\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 22946.9746 - val_loss: 56325.0352\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 10757.4023 - val_loss: 10664.5508\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 2608.2375 - val_loss: 18364.7754\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5140.3135 - val_loss: 14635.8916\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2408.2549 - val_loss: 12493.6650\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2718.7026 - val_loss: 10843.1475\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1630.8320 - val_loss: 10127.8398\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1599.5695 - val_loss: 7902.9111\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1008.7050 - val_loss: 7888.2251\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1312.0735 - val_loss: 8635.7422\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1209.8782 - val_loss: 16950.1777\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 160751.2500 - val_loss: 248857.5156\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2083769.8750 - val_loss: 8910554.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 12161892.0000 - val_loss: 62588448.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 37107184.0000 - val_loss: 433542.1562\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 10910245.0000 - val_loss: 806047.4375\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 776366.6875 - val_loss: 65344.3750\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6023955.5000 - val_loss: 18976708.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 14193647.0000 - val_loss: 78804.0156\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 308410.8125 - val_loss: 206117.5625\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 88078.1484 - val_loss: 49213.0547\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 8891.6582 - val_loss: 28271.7832\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4340.1787 - val_loss: 28468.7266\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2392.4417 - val_loss: 28065.9570\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2593.4575 - val_loss: 28180.6797\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1929.0018 - val_loss: 28123.1973\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1433.2422 - val_loss: 28179.3008\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1876.0273 - val_loss: 28128.3574\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1561.2931 - val_loss: 28138.9180\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 2064.8003 - val_loss: 28131.3906\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2891.1191 - val_loss: 28116.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 13:06:25 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J83' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J83'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step  \n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 1582830336.0000 - val_loss: 97411400.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 389443136.0000 - val_loss: 350541440.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 501617696.0000 - val_loss: 519535872.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 351113952.0000 - val_loss: 183979808.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 114602296.0000 - val_loss: 6908073.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5568979.5000 - val_loss: 3158840.2500\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 927600.8750 - val_loss: 4169144.2500\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 639085.4375 - val_loss: 1009340.5000\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 274829.6250 - val_loss: 2362509.2500\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 259099.7344 - val_loss: 2439898.5000\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1115652.2500 - val_loss: 6211583.5000\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4000851.2500 - val_loss: 734917.2500\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 840665.8125 - val_loss: 1682187.8750\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 396258.2188 - val_loss: 1911272.1250\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 388077.1250 - val_loss: 1609888.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 388005.6562 - val_loss: 1521727.8750\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 361959.3750 - val_loss: 1787677.5000\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 346442.3125 - val_loss: 1509768.6250\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 399472.9375 - val_loss: 1480756.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 400449.0000 - val_loss: 1735796.1250\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 344448.2812 - val_loss: 1579649.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 372234.9688 - val_loss: 1860631.6250\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2311115.7500 - val_loss: 808160.6250\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 865410.0000 - val_loss: 1142314.6250\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 362986.3125 - val_loss: 1416632.3750\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 331859.0938 - val_loss: 1615422.3750\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 338549.3125 - val_loss: 1663592.6250\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 319447.6250 - val_loss: 1609172.7500\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 337628.4062 - val_loss: 1472414.2500\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 329857.3125 - val_loss: 1531708.3750\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 322885.2500 - val_loss: 1439705.1250\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 311609.2500 - val_loss: 1336117.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 311952.0000 - val_loss: 1477387.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 321632.6562 - val_loss: 1367350.8750\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 295651.5938 - val_loss: 1243884.7500\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 315690.9375 - val_loss: 1196067.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 308790.9688 - val_loss: 1185119.8750\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 304490.1250 - val_loss: 1428512.5000\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 316389.3750 - val_loss: 1436936.5000\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 317911.0938 - val_loss: 1314504.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 275984.1875 - val_loss: 1436157.2500\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 305581.3438 - val_loss: 1160400.6250\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 281774.2500 - val_loss: 1118101.2500\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 288507.9062 - val_loss: 1160202.7500\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 286311.4375 - val_loss: 1113136.7500\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 299423.0625 - val_loss: 1179255.3750\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 278121.4688 - val_loss: 1189070.5000\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 288701.6250 - val_loss: 1307358.5000\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 279306.1250 - val_loss: 980322.5625\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 292514.7812 - val_loss: 1056781.3750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 13:07:05 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J90' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J90'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step \n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 184747056.0000 - val_loss: 849683.3750\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 20766866.0000 - val_loss: 683057.4375\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 389904.4688 - val_loss: 5557568.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 1122340.7500 - val_loss: 87908.7188\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 41937.8281 - val_loss: 28178.7832\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 21018.4512 - val_loss: 41937.2891\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 10289.3447 - val_loss: 55083.3633\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4417.0405 - val_loss: 66887.0625\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 11166.4482 - val_loss: 94544.8438\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9421.2119 - val_loss: 74943.2344\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9783.5986 - val_loss: 40632.1719\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 6233.0464 - val_loss: 40785.7617\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 5354.1807 - val_loss: 34550.7070\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3141.9287 - val_loss: 32639.9141\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1681.7786 - val_loss: 28077.9922\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 1993.2290 - val_loss: 26355.2793\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2442.5393 - val_loss: 26198.0742\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3085.5361 - val_loss: 61678.2109\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2659.0720 - val_loss: 33760.0039\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2454.5669 - val_loss: 21674.0859\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1747.7426 - val_loss: 66929.6641\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 73018512.0000 - val_loss: 3555343.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 2945111.5000 - val_loss: 10499.4326\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 298188.9062 - val_loss: 86011.2891\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 29025.0352 - val_loss: 6048.3345\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2679.7527 - val_loss: 3493.3684\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 586.1010 - val_loss: 2272.0105\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1201.9446 - val_loss: 2483.6426\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 997.3494 - val_loss: 2301.0215\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 220.1414 - val_loss: 3269.0466\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 134.1033 - val_loss: 3286.0847\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 210.2022 - val_loss: 3248.2922\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 389.3582 - val_loss: 2872.6475\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 134.6650 - val_loss: 3018.3301\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 82.9697 - val_loss: 2710.6018\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 275.8152 - val_loss: 1998.8687\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 451.8371 - val_loss: 34613.5195\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 35044.7383 - val_loss: 77404.1016\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 986.1993 - val_loss: 4525.0454\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 270.6935 - val_loss: 5494.9927\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 201.0075 - val_loss: 5486.5352\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 146.9632 - val_loss: 5492.4067\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 278.1263 - val_loss: 5480.4771\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 248.8263 - val_loss: 5490.7324\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 220.0098 - val_loss: 5335.7695\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 239.8361 - val_loss: 5304.7051\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 117.3169 - val_loss: 5304.6157\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 186.6817 - val_loss: 5305.3965\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 133.0534 - val_loss: 5308.8496\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 133.0797 - val_loss: 5301.8989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 13:07:45 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J303' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J303'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step  \n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 2201497856.0000 - val_loss: 1008084160.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 503693888.0000 - val_loss: 603693248.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 404948224.0000 - val_loss: 108297008.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 240764656.0000 - val_loss: 179760144.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 115614992.0000 - val_loss: 28617754.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 104436656.0000 - val_loss: 46801000.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 34804940.0000 - val_loss: 14961306.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 15082803.0000 - val_loss: 1769980.5000\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2701612.5000 - val_loss: 8620387.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6059135.0000 - val_loss: 4580112.5000\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 4938223.0000 - val_loss: 4522629.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4639151.0000 - val_loss: 4467875.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4650820.0000 - val_loss: 4418517.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4841803.0000 - val_loss: 4359904.5000\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4644801.0000 - val_loss: 4310039.5000\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4468559.5000 - val_loss: 4263289.5000\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4462811.0000 - val_loss: 4206855.5000\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4445650.0000 - val_loss: 4150198.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4493361.5000 - val_loss: 4090083.7500\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4281240.5000 - val_loss: 4031495.7500\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4195232.5000 - val_loss: 3944923.5000\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4077549.0000 - val_loss: 3874369.2500\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 3916594.7500 - val_loss: 3794264.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4009884.0000 - val_loss: 3729973.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4176044.7500 - val_loss: 3627059.5000\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3768738.7500 - val_loss: 3596862.7500\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 3868672.0000 - val_loss: 3533885.7500\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3810243.5000 - val_loss: 3466440.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 3667633.5000 - val_loss: 3390833.5000\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3567833.2500 - val_loss: 3320815.2500\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 3680137.5000 - val_loss: 3247835.5000\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3372497.7500 - val_loss: 3187438.7500\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3493837.2500 - val_loss: 3124615.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3230502.7500 - val_loss: 3050709.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3239320.5000 - val_loss: 2965714.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 3032795.7500 - val_loss: 2912127.7500\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3077953.0000 - val_loss: 2810812.2500\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2972552.7500 - val_loss: 2739611.2500\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2966459.5000 - val_loss: 2675224.7500\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2870543.5000 - val_loss: 2597921.7500\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2692059.0000 - val_loss: 2525627.2500\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2755790.7500 - val_loss: 2459672.7500\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2590999.2500 - val_loss: 2397713.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2593564.7500 - val_loss: 2337767.7500\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2551831.0000 - val_loss: 2278714.2500\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2381841.0000 - val_loss: 2220010.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2300161.5000 - val_loss: 2162219.7500\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2328500.0000 - val_loss: 2104881.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 2169924.2500 - val_loss: 2047326.8750\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2179314.2500 - val_loss: 1990773.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 13:08:24 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J227' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J227'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step  \n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 994969152.0000 - val_loss: 43075736.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 57661952.0000 - val_loss: 26266440.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 19810026.0000 - val_loss: 98320872.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 76301288.0000 - val_loss: 34240844.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 22900442.0000 - val_loss: 58223492.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 58758792.0000 - val_loss: 41306348.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 42111864.0000 - val_loss: 20445530.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 56654772.0000 - val_loss: 1823742.2500\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1424854.6250 - val_loss: 62556.2109\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 7442575.0000 - val_loss: 22502000.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 19490640.0000 - val_loss: 5846340.5000\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1823480.2500 - val_loss: 1054654.5000\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1005082.2500 - val_loss: 572529.5000\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 249273.3906 - val_loss: 238227.6562\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 49733.0352 - val_loss: 156559.3594\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 26090.3672 - val_loss: 106101.0703\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 12300.2441 - val_loss: 83870.5781\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8737.4873 - val_loss: 77671.4062\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9138.7686 - val_loss: 68653.3516\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8493.2422 - val_loss: 63225.3789\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7849.8159 - val_loss: 54231.3750\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4261.3081 - val_loss: 47391.5391\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4742.2632 - val_loss: 44774.0039\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3247.1047 - val_loss: 43387.7617\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4700.1411 - val_loss: 42473.2188\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2678.2905 - val_loss: 41690.4531\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1747.7794 - val_loss: 40780.7383\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4056.2407 - val_loss: 39509.9805\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3577.9900 - val_loss: 38505.3086\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 1407.3857 - val_loss: 37908.8281\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3670.9500 - val_loss: 38071.1992\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 2667.0669 - val_loss: 37607.4727\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2595.4656 - val_loss: 36747.3359\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 2441.9519 - val_loss: 33198.8320\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1395.5579 - val_loss: 31005.7578\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1733.1981 - val_loss: 30142.7578\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1912.2119 - val_loss: 29687.2500\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1818.1166 - val_loss: 29384.6270\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 936.6530 - val_loss: 28741.8984\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 1914.9547 - val_loss: 28517.1426\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 815.5044 - val_loss: 28330.4922\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2451.5132 - val_loss: 28649.8281\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 747.1727 - val_loss: 29996.4648\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1462.0461 - val_loss: 29623.3594\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 415.5427 - val_loss: 29477.5195\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 754.5038 - val_loss: 29446.7168\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2359.2817 - val_loss: 29304.4395\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1613.0695 - val_loss: 29290.4648\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 783.8372 - val_loss: 29358.6758\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1359.3282 - val_loss: 32447.9355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/14 13:09:03 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'J498' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'J498'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step \n",
      "✅ LSTM training with parent MLflow run, nested runs, and plots completed\n"
     ]
    }
   ],
   "source": [
    "import os, time, pickle, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    denominator = np.where(denominator == 0, np.finfo(float).eps, denominator)\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / denominator)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_value = np.percentile(y_true, percentile)\n",
    "    peak_indices = y_true >= peak_value\n",
    "    if np.sum(peak_indices) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_indices] - y_pred[peak_indices]))\n",
    "\n",
    "def create_sequences_by_scenario(df, seq_length, feature_cols, target_col):\n",
    "    X, y, seq_scenario_ids = [], [], []\n",
    "    for scenario_id, group in df.groupby('scenario_id'):\n",
    "        group = group.sort_index()\n",
    "        data = group[feature_cols + [target_col]].values\n",
    "        if len(data) <= seq_length: continue\n",
    "        for i in range(seq_length, len(data)):\n",
    "            X.append(data[i-seq_length:i, :-1])\n",
    "            y.append(data[i, -1])\n",
    "            seq_scenario_ids.append(scenario_id)\n",
    "    return np.array(X), np.array(y), np.array(seq_scenario_ids)\n",
    "\n",
    "# -----------------------------\n",
    "# Paths & parameters\n",
    "# -----------------------------\n",
    "root = '/mnt/data/home/zayd/Digital_twin_project/machine_learning/dataset/Ctown/junctions/'\n",
    "version = '0.1.0'\n",
    "MODEL_DIR = f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/LSTM_{version}'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "seq_length = 23\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "lag_steps = [1,2,3]\n",
    "rolling_windows = [3,6,12]\n",
    "\n",
    "mlflow.set_experiment(\"Digital_Twin_Experiments\")\n",
    "global_y_true, global_y_pred = [], []\n",
    "feature_and_target = {}\n",
    "\n",
    "# -----------------------------\n",
    "# Parent MLflow run\n",
    "# -----------------------------\n",
    "with mlflow.start_run(run_name=f\"LSTM_{version}\"):\n",
    "\n",
    "    mlflow.log_param(\"seq_length\", seq_length)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "\n",
    "    for filename in os.listdir(root)[:10]:\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        df = df.sort_values(by=[\"scenario_id\", \"time_id\"]).reset_index(drop=True)\n",
    "        junction_cols = [col for col in df.columns if col.startswith('J')]\n",
    "\n",
    "        # Convert from m³/s to L/s\n",
    "        df[junction_cols] = df[junction_cols] * 1000\n",
    "        junction = os.path.splitext(filename)[0]\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"{junction}\", nested=True):\n",
    "            raw_cols = df.columns.tolist()\n",
    "            target = junction\n",
    "            features = [c for c in df.columns if c != target]\n",
    "            feature_and_target[junction] = {\"target\": target, \"features\": features, \"raw_cols\": raw_cols}\n",
    "\n",
    "            # -----------------------------\n",
    "            # Lag and rolling features\n",
    "            # -----------------------------\n",
    "            for lag in lag_steps:\n",
    "                df[f'{junction}_lag{lag}'] = df[junction].shift(lag)\n",
    "            for w in rolling_windows:\n",
    "                df[f'{junction}_rollmean{w}'] = df[junction].rolling(window=w, min_periods=1).mean()\n",
    "            df = df.fillna(0)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Train/test split\n",
    "            # -----------------------------\n",
    "            scenario_ids = df['scenario_id'].unique()\n",
    "            split = int(0.8*len(scenario_ids))\n",
    "            train_df = df[df['scenario_id'].isin(scenario_ids[:split])]\n",
    "            test_df = df[df['scenario_id'].isin(scenario_ids[split:])]\n",
    "\n",
    "            # -----------------------------\n",
    "            # Scaling\n",
    "            # -----------------------------\n",
    "            feature_scaler = MinMaxScaler().fit(train_df[features])\n",
    "            target_scaler = MinMaxScaler().fit(train_df[[target]])\n",
    "\n",
    "            X_train_scaled = feature_scaler.transform(train_df[features])\n",
    "            y_train_scaled = target_scaler.transform(train_df[[target]])\n",
    "            X_test_scaled = feature_scaler.transform(test_df[features])\n",
    "            y_test_scaled = target_scaler.transform(test_df[[target]])\n",
    "\n",
    "            # Save scalers\n",
    "            with open(os.path.join(MODEL_DIR,f\"{junction}_feature_scaler.save\"),'wb') as f: pickle.dump(feature_scaler,f)\n",
    "            with open(os.path.join(MODEL_DIR,f\"{junction}_target_scaler.save\"),'wb') as f: pickle.dump(target_scaler,f)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Create sequences\n",
    "            # -----------------------------\n",
    "            X_train, y_train, _ = create_sequences_by_scenario(train_df, seq_length, features, target)\n",
    "            X_test, y_test, _ = create_sequences_by_scenario(test_df, seq_length, features, target)\n",
    "            n_features = X_train.shape[2]\n",
    "            n_targets = 1\n",
    "            mlflow.log_param(f\"n_features_{junction}\", n_features)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Build and train LSTM\n",
    "            # -----------------------------\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.LSTM(50, activation='relu', input_shape=(seq_length,n_features)),\n",
    "                tf.keras.layers.Dense(n_targets)\n",
    "            ])\n",
    "            model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "            start_time = time.time()\n",
    "            history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
    "            elapsed_sec = time.time() - start_time\n",
    "            mlflow.log_param(f\"{junction}_training_time_sec\", elapsed_sec)\n",
    "\n",
    "            # Save model\n",
    "            model_path = os.path.join(MODEL_DIR,f\"{junction}.keras\")\n",
    "            model.save(model_path)\n",
    "            mlflow.tensorflow.log_model(model, artifact_path=junction, registered_model_name=junction,\n",
    "                                        input_example=np.random.rand(1, seq_length, n_features))\n",
    "\n",
    "            # -----------------------------\n",
    "            # Predictions & inverse scaling\n",
    "            # -----------------------------\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_test_inv = target_scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "            y_pred_inv = target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
    "            global_y_true.append(y_test_inv)\n",
    "            global_y_pred.append(y_pred_inv)\n",
    "\n",
    "            # Metrics\n",
    "            mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "            smape_val = smape(y_test_inv, y_pred_inv)\n",
    "            r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "            peak_err = peak_error(y_test_inv, y_pred_inv)\n",
    "\n",
    "            mlflow.log_params({\n",
    "                \"algo\": f\"LSTM_{version}\",\n",
    "                \"junction\": junction,\n",
    "            }\n",
    "            )\n",
    "\n",
    "            mlflow.log_metrics({\n",
    "                \"MAE\": mae, \"RMSE\": rmse, \"SMAPE\": smape_val, \"R2_score\": r2, \"peak_error\": peak_err\n",
    "            })\n",
    "\n",
    "            # -----------------------------\n",
    "            # Plot loss\n",
    "            # -----------------------------\n",
    "            plt.figure(figsize=(10,6))\n",
    "            plt.plot(history.history['loss'], label='train_loss')\n",
    "            plt.plot(history.history['val_loss'], label='val_loss')\n",
    "            plt.legend(); plt.title(f'{junction} Loss Curve')\n",
    "            loss_plot_path = os.path.join(MODEL_DIR,f\"{junction}_loss_curve.png\")\n",
    "            plt.savefig(loss_plot_path); plt.close()\n",
    "            mlflow.log_artifact(loss_plot_path, artifact_path=f\"{junction}/plots\")\n",
    "\n",
    "            # -----------------------------\n",
    "            # Plot predictions\n",
    "            # -----------------------------\n",
    "            plt.figure(figsize=(14,7))\n",
    "            plt.plot(y_test_inv, label='Actual', color='orange')\n",
    "            plt.plot(y_pred_inv, label='Predicted', color='green')\n",
    "            plt.title(f'{junction} Forecast')\n",
    "            plt.xlabel('Time step'); plt.ylabel('Value'); plt.legend()\n",
    "            forecast_plot_path = os.path.join(MODEL_DIR,f\"{junction}_forecast.png\")\n",
    "            plt.savefig(forecast_plot_path); plt.close()\n",
    "            mlflow.log_artifact(forecast_plot_path, artifact_path=f\"{junction}/plots\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Global metrics\n",
    "    # -----------------------------\n",
    "    all_y_true = np.vstack(global_y_true)\n",
    "    all_y_pred = np.vstack(global_y_pred)\n",
    "    global_metrics = {\n",
    "        'MAE': mean_absolute_error(all_y_true, all_y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(all_y_true, all_y_pred)),\n",
    "        'SMAPE': smape(all_y_true, all_y_pred),\n",
    "        'R2_score': r2_score(all_y_true, all_y_pred),\n",
    "        'peak_error': peak_error(all_y_true, all_y_pred)\n",
    "    }\n",
    "    mlflow.log_metrics(global_metrics)\n",
    "    with open(os.path.join(MODEL_DIR,'feature_and_target.json'),'w') as f: json.dump(feature_and_target,f,indent=4)\n",
    "\n",
    "print(\"✅ LSTM training with parent MLflow run, nested runs, and plots completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e6e84",
   "metadata": {},
   "source": [
    "LSTM (zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d82667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import mlflow\n",
    "import time\n",
    "import json\n",
    "\n",
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "def create_sequences_multioutput(data, seq_length, target_indices):\n",
    "    \"\"\"\n",
    "    data: np.array of shape (n_samples, n_features)\n",
    "    seq_length: length of input sequences\n",
    "    target_indices: list of column indices for targets in data\n",
    "    Returns: X, y arrays for LSTM\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        X.append(data[i-seq_length:i, :])           # all columns (features + targets) as input\n",
    "        y.append(data[i, target_indices])           # select only target columns\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true = np.where(y_true == 0, np.finfo(float).eps, y_true)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = np.where((np.abs(y_true) + np.abs(y_pred)) == 0, np.finfo(float).eps, np.abs(y_true) + np.abs(y_pred))\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / denominator)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_value = np.percentile(y_true, percentile)\n",
    "    peak_indices = y_true >= peak_value\n",
    "    if np.sum(peak_indices) == 0: return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_indices] - y_pred[peak_indices]))\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "version = '1.0.1'\n",
    "seq_length = 12\n",
    "lag_steps = [1,2,3]\n",
    "rolling_windows = [3,6,12]\n",
    "root = '/mnt/data/home/zayd/Digital_twin_project/machine_learning/dataset/zones_v2/'\n",
    "trained_dir = f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/LSTM_{version}/'\n",
    "os.makedirs(trained_dir, exist_ok=True)\n",
    "trained_dir_list = os.listdir(trained_dir)\n",
    "metrics_df = pd.DataFrame(columns=['zone','MAE','RMSE','MAPE','SMAPE','R2','Peak_Error'])\n",
    "mlflow.set_tracking_uri(\"file:model_trained/mlruns\")\n",
    "mlflow.set_experiment(\"Digital_Twin_LSTM\")\n",
    "\n",
    "feature_and_target = {}\n",
    "\n",
    "# -----------------------------\n",
    "# Process zones\n",
    "# -----------------------------\n",
    "with mlflow.start_run(run_name=f\"LSTM_{version}\"):\n",
    "\n",
    "    for filename in os.listdir(root):\n",
    "        zone_name = os.path.splitext(filename)[0]\n",
    "        if f'{zone_name}.keras' in trained_dir_list:\n",
    "            print(f\"Zone {zone_name} already trained, skipping\")\n",
    "            continue\n",
    "        with mlflow.start_run(run_name=f\"Zone_{zone_name}\", nested=True):\n",
    "\n",
    "            df = pd.read_parquet(os.path.join(root, filename))\n",
    "            junctions = [col for col in df.columns if 'demand' in col]\n",
    "\n",
    "            # -----------------------------\n",
    "            # Lag & rolling features\n",
    "            # -----------------------------\n",
    "            for lag in lag_steps:\n",
    "                for j in junctions:\n",
    "                    df[f'{j}_lag{lag}'] = df[j].shift(lag)\n",
    "\n",
    "            for w in rolling_windows:\n",
    "                for j in junctions:\n",
    "                    df[f'{j}_rollmean{w}'] = df[j].rolling(window=w, min_periods=1).mean()\n",
    "\n",
    "            df = df.fillna(0)\n",
    "            target_cols = junctions.copy()\n",
    "            y_cols_idx = [df.columns.get_loc(c) for c in target_cols]\n",
    "            features = [col for col in df.columns if col not in target_cols]\n",
    "\n",
    "            # -----------------------------\n",
    "            # Train/Test split\n",
    "            # -----------------------------\n",
    "            df = df.sort_values(['day','time_id'])\n",
    "            split_idx = int(0.8*len(df))\n",
    "            train_df, test_df = df.iloc[:split_idx], df.iloc[split_idx:]\n",
    "\n",
    "            # Scale features\n",
    "            feature_scaler = MinMaxScaler()\n",
    "            target_scaler = MinMaxScaler()\n",
    "            X_train_scaled = feature_scaler.fit_transform(train_df[features])\n",
    "            y_train_scaled = target_scaler.fit_transform(train_df[target_cols])\n",
    "            X_test_scaled = feature_scaler.transform(test_df[features])\n",
    "            y_test_scaled = target_scaler.transform(test_df[target_cols])\n",
    "\n",
    "            train_scaled_df = pd.DataFrame(np.hstack((X_train_scaled, y_train_scaled)), columns=features+target_cols)\n",
    "            test_scaled_df = pd.DataFrame(np.hstack((X_test_scaled, y_test_scaled)), columns=features+target_cols)\n",
    "\n",
    "            # Save scalers\n",
    "            with open(os.path.join(trained_dir, f'{zone_name}_feature_scaler.save'), 'wb') as f:\n",
    "                pickle.dump(feature_scaler, f)\n",
    "            with open(os.path.join(trained_dir, f'{zone_name}_target_scaler.save'), 'wb') as f:\n",
    "                pickle.dump(target_scaler, f)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Create sequences\n",
    "            # -----------------------------\n",
    "            target_indices = [train_scaled_df.columns.get_loc(c) for c in target_cols]\n",
    "\n",
    "            X_train, y_train = create_sequences_multioutput(train_scaled_df.values, seq_length, target_indices)\n",
    "            X_test, y_test = create_sequences_multioutput(test_scaled_df.values, seq_length, target_indices)\n",
    "            n_features = X_train.shape[2]\n",
    "            n_targets = y_train.shape[1] if len(y_train.shape) > 1 else 1\n",
    "\n",
    "            feature_and_target[zone_name] = {\n",
    "                \"target\": target_cols,\n",
    "                \"features\": features,\n",
    "                \"raw_cols\": df.columns.tolist()\n",
    "            }\n",
    "\n",
    "            # Log parameters to MLflow\n",
    "            mlflow.log_param(\"zone\", zone_name)\n",
    "            mlflow.log_param(\"seq_length\", seq_length)\n",
    "            mlflow.log_param(\"n_features\", n_features)\n",
    "            mlflow.log_param(\"n_targets\", n_targets)\n",
    "            mlflow.log_param(\"lag_steps\", lag_steps)\n",
    "            mlflow.log_param(\"rolling_windows\", rolling_windows)\n",
    "            mlflow.log_param(\"epochs\", 50)\n",
    "            mlflow.log_param(\"batch_size\", 32)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Build & train multi-output LSTM\n",
    "            # -----------------------------\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.LSTM(50, activation='relu', input_shape=(seq_length, n_features)),\n",
    "                tf.keras.layers.Dense(n_targets)\n",
    "            ])\n",
    "            model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "            start = time.time()\n",
    "            history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "            elapsed_sec = time.time() - start\n",
    "            minutes, rem = divmod(elapsed_sec, 60)\n",
    "            seconds, milliseconds = divmod(rem, 1)\n",
    "            milliseconds = int(milliseconds * 1000)\n",
    "            formatted_time = f\"{int(minutes)}m:{int(seconds)}s:{milliseconds}ms\"\n",
    "            mlflow.log_param(f\"{zone_name}_training_time\", formatted_time)\n",
    "\n",
    "            # Save model\n",
    "            model.save(os.path.join(trained_dir, f\"{zone_name}.keras\"))\n",
    "            with open(os.path.join(trained_dir, f\"{zone_name}_feature_and_target.json\"), \"w\") as f:\n",
    "                json.dump(feature_and_target, f, indent=4)\n",
    "\n",
    "            # Log model summary\n",
    "            model_summary = []\n",
    "            model.summary(print_fn=lambda x: model_summary.append(x))\n",
    "            mlflow.log_text(\"\\n\".join(model_summary), artifact_file=f\"{zone_name}/model_summary.txt\")\n",
    "            mlflow.tensorflow.log_model(model, f\"model_{zone_name}\")\n",
    "\n",
    "            \n",
    "            # Predict & inverse scale\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_test_inv = target_scaler.inverse_transform(y_test)\n",
    "            y_pred_inv = target_scaler.inverse_transform(y_pred)\n",
    "\n",
    "            # Evaluate\n",
    "            mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "            mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv)\n",
    "            smape_val = smape(y_test_inv, y_pred_inv)\n",
    "            r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "            peak_err = peak_error(y_test_inv, y_pred_inv)\n",
    "\n",
    "            metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
    "                'zone':[zone_name],'MAE':[mae],'RMSE':[rmse],'MAPE':[mape],'SMAPE':[smape_val],'R2':[r2],'Peak_Error':[peak_err]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Log metrics\n",
    "            mlflow.log_metrics({\n",
    "                \"MAE\": mae,\n",
    "                \"RMSE\": rmse,\n",
    "                \"MAPE\": mape,\n",
    "                \"SMAPE\": smape_val,\n",
    "                \"R2\": r2,\n",
    "                \"Peak_Error\": peak_err\n",
    "            })\n",
    "            print(f\"✅ Zone {zone_name}: MAE={mae:.3f}, RMSE={rmse:.3f}, R2={r2:.3f}\")\n",
    "\n",
    "            # Save scalers in MLflow\n",
    "            mlflow.log_artifact(os.path.join(trained_dir, f'{zone_name}_feature_scaler.save'), artifact_path=f\"{zone_name}/scalers\")\n",
    "            mlflow.log_artifact(os.path.join(trained_dir, f'{zone_name}_target_scaler.save'), artifact_path=f\"{zone_name}/scalers\")\n",
    "\n",
    "            input_example = np.random.rand(1, seq_length, n_features)\n",
    "            mlflow.tensorflow.log_model(model, artifact_path=zone_name, registered_model_name=zone_name, input_example=input_example)\n",
    "\n",
    "            # Plot loss\n",
    "            plt.figure()\n",
    "            plt.plot(history.history['loss'], label='train_loss')\n",
    "            plt.plot(history.history['val_loss'], label='val_loss')\n",
    "            plt.legend(); plt.title(f'Loss over epochs - {zone_name}')\n",
    "            plt.savefig(os.path.join(trained_dir, f\"{zone_name}_loss_curve.png\"))\n",
    "            mlflow.log_artifact(os.path.join(trained_dir, f\"{zone_name}_loss_curve.png\"), artifact_path=f\"{zone_name}/plots\")\n",
    "            plt.close()\n",
    "\n",
    "metrics_df.set_index('zone', inplace=True)\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3364e46",
   "metadata": {},
   "source": [
    "LightGBM (single junction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4801530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.144858\tvalid_1's rmse: 0.148556\n",
      "[200]\ttraining's rmse: 0.0900521\tvalid_1's rmse: 0.0954436\n",
      "[300]\ttraining's rmse: 0.0623927\tvalid_1's rmse: 0.068747\n",
      "[400]\ttraining's rmse: 0.0454983\tvalid_1's rmse: 0.0523205\n",
      "[500]\ttraining's rmse: 0.0351123\tvalid_1's rmse: 0.0421116\n",
      "[600]\ttraining's rmse: 0.0289784\tvalid_1's rmse: 0.036022\n",
      "[700]\ttraining's rmse: 0.0250722\tvalid_1's rmse: 0.0321021\n",
      "[800]\ttraining's rmse: 0.0223724\tvalid_1's rmse: 0.0293711\n",
      "[900]\ttraining's rmse: 0.0203823\tvalid_1's rmse: 0.0273637\n",
      "[1000]\ttraining's rmse: 0.0190099\tvalid_1's rmse: 0.0260134\n",
      "[1100]\ttraining's rmse: 0.0180049\tvalid_1's rmse: 0.0251369\n",
      "[1200]\ttraining's rmse: 0.0171381\tvalid_1's rmse: 0.0243961\n",
      "[1300]\ttraining's rmse: 0.0164697\tvalid_1's rmse: 0.0238454\n",
      "[1400]\ttraining's rmse: 0.0158987\tvalid_1's rmse: 0.0234491\n",
      "[1500]\ttraining's rmse: 0.0154012\tvalid_1's rmse: 0.0231137\n",
      "[1600]\ttraining's rmse: 0.0149344\tvalid_1's rmse: 0.0228477\n",
      "[1700]\ttraining's rmse: 0.0145456\tvalid_1's rmse: 0.0226328\n",
      "[1800]\ttraining's rmse: 0.0141747\tvalid_1's rmse: 0.0224621\n",
      "[1900]\ttraining's rmse: 0.0138512\tvalid_1's rmse: 0.022311\n",
      "[2000]\ttraining's rmse: 0.0135107\tvalid_1's rmse: 0.0221562\n",
      "[2100]\ttraining's rmse: 0.0132233\tvalid_1's rmse: 0.02205\n",
      "[2200]\ttraining's rmse: 0.0129709\tvalid_1's rmse: 0.0219588\n",
      "[2300]\ttraining's rmse: 0.0127325\tvalid_1's rmse: 0.0218852\n",
      "[2400]\ttraining's rmse: 0.0125076\tvalid_1's rmse: 0.0218066\n",
      "[2500]\ttraining's rmse: 0.0122701\tvalid_1's rmse: 0.0217193\n",
      "[2600]\ttraining's rmse: 0.0120382\tvalid_1's rmse: 0.0216426\n",
      "[2700]\ttraining's rmse: 0.011826\tvalid_1's rmse: 0.0215772\n",
      "[2800]\ttraining's rmse: 0.0116134\tvalid_1's rmse: 0.0215218\n",
      "[2900]\ttraining's rmse: 0.0114294\tvalid_1's rmse: 0.0214485\n",
      "[3000]\ttraining's rmse: 0.0112524\tvalid_1's rmse: 0.0214053\n",
      "[3100]\ttraining's rmse: 0.0110752\tvalid_1's rmse: 0.0213568\n",
      "[3200]\ttraining's rmse: 0.0108935\tvalid_1's rmse: 0.0213129\n",
      "[3300]\ttraining's rmse: 0.0107219\tvalid_1's rmse: 0.0212581\n",
      "[3400]\ttraining's rmse: 0.0105691\tvalid_1's rmse: 0.0212166\n",
      "[3500]\ttraining's rmse: 0.0104236\tvalid_1's rmse: 0.0211686\n",
      "[3600]\ttraining's rmse: 0.0102458\tvalid_1's rmse: 0.0211074\n",
      "[3700]\ttraining's rmse: 0.0100838\tvalid_1's rmse: 0.0210689\n",
      "[3800]\ttraining's rmse: 0.00993656\tvalid_1's rmse: 0.0210159\n",
      "[3900]\ttraining's rmse: 0.00978952\tvalid_1's rmse: 0.0209661\n",
      "[4000]\ttraining's rmse: 0.00964377\tvalid_1's rmse: 0.0209193\n",
      "[4100]\ttraining's rmse: 0.00949882\tvalid_1's rmse: 0.0208658\n",
      "[4200]\ttraining's rmse: 0.00934593\tvalid_1's rmse: 0.0208101\n",
      "[4300]\ttraining's rmse: 0.00921785\tvalid_1's rmse: 0.0207803\n",
      "[4400]\ttraining's rmse: 0.00910508\tvalid_1's rmse: 0.0207559\n",
      "[4500]\ttraining's rmse: 0.00898692\tvalid_1's rmse: 0.0207325\n",
      "[4600]\ttraining's rmse: 0.00885443\tvalid_1's rmse: 0.0206873\n",
      "[4700]\ttraining's rmse: 0.00872914\tvalid_1's rmse: 0.0206489\n",
      "[4800]\ttraining's rmse: 0.00860098\tvalid_1's rmse: 0.0206127\n",
      "[4900]\ttraining's rmse: 0.00849504\tvalid_1's rmse: 0.0205815\n",
      "[5000]\ttraining's rmse: 0.00838557\tvalid_1's rmse: 0.0205586\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4960]\ttraining's rmse: 0.00842228\tvalid_1's rmse: 0.0205573\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.106932\tvalid_1's rmse: 0.107704\n",
      "[200]\ttraining's rmse: 0.0627902\tvalid_1's rmse: 0.0663105\n",
      "[300]\ttraining's rmse: 0.0507159\tvalid_1's rmse: 0.0562506\n",
      "[400]\ttraining's rmse: 0.0461194\tvalid_1's rmse: 0.0527561\n",
      "[500]\ttraining's rmse: 0.0430205\tvalid_1's rmse: 0.0502895\n",
      "[600]\ttraining's rmse: 0.0405943\tvalid_1's rmse: 0.0483537\n",
      "[700]\ttraining's rmse: 0.0385559\tvalid_1's rmse: 0.0467929\n",
      "[800]\ttraining's rmse: 0.0368163\tvalid_1's rmse: 0.0454945\n",
      "[900]\ttraining's rmse: 0.0352456\tvalid_1's rmse: 0.0442857\n",
      "[1000]\ttraining's rmse: 0.0337245\tvalid_1's rmse: 0.0431175\n",
      "[1100]\ttraining's rmse: 0.0322327\tvalid_1's rmse: 0.0418694\n",
      "[1200]\ttraining's rmse: 0.0309013\tvalid_1's rmse: 0.0408308\n",
      "[1300]\ttraining's rmse: 0.0296892\tvalid_1's rmse: 0.039884\n",
      "[1400]\ttraining's rmse: 0.0286027\tvalid_1's rmse: 0.0390151\n",
      "[1500]\ttraining's rmse: 0.0275798\tvalid_1's rmse: 0.0382012\n",
      "[1600]\ttraining's rmse: 0.0266007\tvalid_1's rmse: 0.0374247\n",
      "[1700]\ttraining's rmse: 0.0256966\tvalid_1's rmse: 0.0366995\n",
      "[1800]\ttraining's rmse: 0.0248503\tvalid_1's rmse: 0.0360391\n",
      "[1900]\ttraining's rmse: 0.0240731\tvalid_1's rmse: 0.0354163\n",
      "[2000]\ttraining's rmse: 0.0233146\tvalid_1's rmse: 0.034844\n",
      "[2100]\ttraining's rmse: 0.0225652\tvalid_1's rmse: 0.034253\n",
      "[2200]\ttraining's rmse: 0.0219056\tvalid_1's rmse: 0.0337719\n",
      "[2300]\ttraining's rmse: 0.0212586\tvalid_1's rmse: 0.0332682\n",
      "[2400]\ttraining's rmse: 0.0206439\tvalid_1's rmse: 0.0327594\n",
      "[2500]\ttraining's rmse: 0.0200494\tvalid_1's rmse: 0.0322887\n",
      "[2600]\ttraining's rmse: 0.0194877\tvalid_1's rmse: 0.0318446\n",
      "[2700]\ttraining's rmse: 0.0189379\tvalid_1's rmse: 0.03138\n",
      "[2800]\ttraining's rmse: 0.0184237\tvalid_1's rmse: 0.0309674\n",
      "[2900]\ttraining's rmse: 0.0179414\tvalid_1's rmse: 0.0305681\n",
      "[3000]\ttraining's rmse: 0.0174778\tvalid_1's rmse: 0.0302002\n",
      "[3100]\ttraining's rmse: 0.0170255\tvalid_1's rmse: 0.0298427\n",
      "[3200]\ttraining's rmse: 0.0166125\tvalid_1's rmse: 0.029517\n",
      "[3300]\ttraining's rmse: 0.0162032\tvalid_1's rmse: 0.0291989\n",
      "[3400]\ttraining's rmse: 0.015818\tvalid_1's rmse: 0.028898\n",
      "[3500]\ttraining's rmse: 0.0154478\tvalid_1's rmse: 0.0286008\n",
      "[3600]\ttraining's rmse: 0.0150884\tvalid_1's rmse: 0.0283224\n",
      "[3700]\ttraining's rmse: 0.014754\tvalid_1's rmse: 0.0280544\n",
      "[3800]\ttraining's rmse: 0.0144311\tvalid_1's rmse: 0.027812\n",
      "[3900]\ttraining's rmse: 0.0141058\tvalid_1's rmse: 0.0275471\n",
      "[4000]\ttraining's rmse: 0.0137987\tvalid_1's rmse: 0.0273064\n",
      "[4100]\ttraining's rmse: 0.0135148\tvalid_1's rmse: 0.0270913\n",
      "[4200]\ttraining's rmse: 0.0132349\tvalid_1's rmse: 0.026853\n",
      "[4300]\ttraining's rmse: 0.0129629\tvalid_1's rmse: 0.0266258\n",
      "[4400]\ttraining's rmse: 0.0127022\tvalid_1's rmse: 0.0264365\n",
      "[4500]\ttraining's rmse: 0.0124541\tvalid_1's rmse: 0.0262473\n",
      "[4600]\ttraining's rmse: 0.0122121\tvalid_1's rmse: 0.026064\n",
      "[4700]\ttraining's rmse: 0.0119855\tvalid_1's rmse: 0.0258984\n",
      "[4800]\ttraining's rmse: 0.0117584\tvalid_1's rmse: 0.0257175\n",
      "[4900]\ttraining's rmse: 0.011546\tvalid_1's rmse: 0.0255625\n",
      "[5000]\ttraining's rmse: 0.0113404\tvalid_1's rmse: 0.0254289\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.0113404\tvalid_1's rmse: 0.0254289\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.123717\tvalid_1's rmse: 0.125376\n",
      "[200]\ttraining's rmse: 0.0848031\tvalid_1's rmse: 0.0884842\n",
      "[300]\ttraining's rmse: 0.0713381\tvalid_1's rmse: 0.0763447\n",
      "[400]\ttraining's rmse: 0.0623018\tvalid_1's rmse: 0.0684546\n",
      "[500]\ttraining's rmse: 0.0560355\tvalid_1's rmse: 0.0629636\n",
      "[600]\ttraining's rmse: 0.0512978\tvalid_1's rmse: 0.0587002\n",
      "[700]\ttraining's rmse: 0.0474438\tvalid_1's rmse: 0.0552798\n",
      "[800]\ttraining's rmse: 0.0441559\tvalid_1's rmse: 0.0523779\n",
      "[900]\ttraining's rmse: 0.0410789\tvalid_1's rmse: 0.0495625\n",
      "[1000]\ttraining's rmse: 0.0385982\tvalid_1's rmse: 0.0472988\n",
      "[1100]\ttraining's rmse: 0.0364415\tvalid_1's rmse: 0.045334\n",
      "[1200]\ttraining's rmse: 0.0344727\tvalid_1's rmse: 0.0435042\n",
      "[1300]\ttraining's rmse: 0.0327146\tvalid_1's rmse: 0.0418727\n",
      "[1400]\ttraining's rmse: 0.0311132\tvalid_1's rmse: 0.04037\n",
      "[1500]\ttraining's rmse: 0.0296959\tvalid_1's rmse: 0.0390788\n",
      "[1600]\ttraining's rmse: 0.0283492\tvalid_1's rmse: 0.0377998\n",
      "[1700]\ttraining's rmse: 0.0271601\tvalid_1's rmse: 0.0366453\n",
      "[1800]\ttraining's rmse: 0.0260629\tvalid_1's rmse: 0.0356154\n",
      "[1900]\ttraining's rmse: 0.0251476\tvalid_1's rmse: 0.0347808\n",
      "[2000]\ttraining's rmse: 0.0242071\tvalid_1's rmse: 0.0338718\n",
      "[2100]\ttraining's rmse: 0.0233576\tvalid_1's rmse: 0.0330428\n",
      "[2200]\ttraining's rmse: 0.0226023\tvalid_1's rmse: 0.0323016\n",
      "[2300]\ttraining's rmse: 0.0219109\tvalid_1's rmse: 0.0316531\n",
      "[2400]\ttraining's rmse: 0.0212544\tvalid_1's rmse: 0.0310311\n",
      "[2500]\ttraining's rmse: 0.0206285\tvalid_1's rmse: 0.0304176\n",
      "[2600]\ttraining's rmse: 0.0200698\tvalid_1's rmse: 0.0298809\n",
      "[2700]\ttraining's rmse: 0.0195177\tvalid_1's rmse: 0.0293691\n",
      "[2800]\ttraining's rmse: 0.0190294\tvalid_1's rmse: 0.0289064\n",
      "[2900]\ttraining's rmse: 0.0185808\tvalid_1's rmse: 0.0284934\n",
      "[3000]\ttraining's rmse: 0.0181748\tvalid_1's rmse: 0.0281097\n",
      "[3100]\ttraining's rmse: 0.0177693\tvalid_1's rmse: 0.0277317\n",
      "[3200]\ttraining's rmse: 0.017395\tvalid_1's rmse: 0.027393\n",
      "[3300]\ttraining's rmse: 0.0170285\tvalid_1's rmse: 0.0270797\n",
      "[3400]\ttraining's rmse: 0.0166886\tvalid_1's rmse: 0.0267757\n",
      "[3500]\ttraining's rmse: 0.0163651\tvalid_1's rmse: 0.0265113\n",
      "[3600]\ttraining's rmse: 0.0160353\tvalid_1's rmse: 0.0262154\n",
      "[3700]\ttraining's rmse: 0.0157302\tvalid_1's rmse: 0.0259512\n",
      "[3800]\ttraining's rmse: 0.0154458\tvalid_1's rmse: 0.0257182\n",
      "[3900]\ttraining's rmse: 0.0151764\tvalid_1's rmse: 0.0254931\n",
      "[4000]\ttraining's rmse: 0.0149139\tvalid_1's rmse: 0.0252717\n",
      "[4100]\ttraining's rmse: 0.0146606\tvalid_1's rmse: 0.0250743\n",
      "[4200]\ttraining's rmse: 0.0144258\tvalid_1's rmse: 0.0248914\n",
      "[4300]\ttraining's rmse: 0.0141921\tvalid_1's rmse: 0.0246857\n",
      "[4400]\ttraining's rmse: 0.0139615\tvalid_1's rmse: 0.0245127\n",
      "[4500]\ttraining's rmse: 0.0137308\tvalid_1's rmse: 0.0243418\n",
      "[4600]\ttraining's rmse: 0.0135188\tvalid_1's rmse: 0.0241822\n",
      "[4700]\ttraining's rmse: 0.0133205\tvalid_1's rmse: 0.0240351\n",
      "[4800]\ttraining's rmse: 0.0131183\tvalid_1's rmse: 0.0238783\n",
      "[4900]\ttraining's rmse: 0.012925\tvalid_1's rmse: 0.0237313\n",
      "[5000]\ttraining's rmse: 0.012743\tvalid_1's rmse: 0.0236107\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.012743\tvalid_1's rmse: 0.0236107\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.119011\tvalid_1's rmse: 0.119985\n",
      "[200]\ttraining's rmse: 0.0774199\tvalid_1's rmse: 0.0809245\n",
      "[300]\ttraining's rmse: 0.0634734\tvalid_1's rmse: 0.06874\n",
      "[400]\ttraining's rmse: 0.0557931\tvalid_1's rmse: 0.0623066\n",
      "[500]\ttraining's rmse: 0.0513877\tvalid_1's rmse: 0.058734\n",
      "[600]\ttraining's rmse: 0.0475035\tvalid_1's rmse: 0.0555033\n",
      "[700]\ttraining's rmse: 0.0443031\tvalid_1's rmse: 0.0528335\n",
      "[800]\ttraining's rmse: 0.0414525\tvalid_1's rmse: 0.0503918\n",
      "[900]\ttraining's rmse: 0.0390772\tvalid_1's rmse: 0.048351\n",
      "[1000]\ttraining's rmse: 0.0366558\tvalid_1's rmse: 0.0462232\n",
      "[1100]\ttraining's rmse: 0.0345062\tvalid_1's rmse: 0.0443222\n",
      "[1200]\ttraining's rmse: 0.0325521\tvalid_1's rmse: 0.0425741\n",
      "[1300]\ttraining's rmse: 0.030694\tvalid_1's rmse: 0.0409018\n",
      "[1400]\ttraining's rmse: 0.0290857\tvalid_1's rmse: 0.0394868\n",
      "[1500]\ttraining's rmse: 0.0275377\tvalid_1's rmse: 0.0380371\n",
      "[1600]\ttraining's rmse: 0.0261136\tvalid_1's rmse: 0.0367105\n",
      "[1700]\ttraining's rmse: 0.0248597\tvalid_1's rmse: 0.0355628\n",
      "[1800]\ttraining's rmse: 0.0236276\tvalid_1's rmse: 0.0344098\n",
      "[1900]\ttraining's rmse: 0.0225024\tvalid_1's rmse: 0.0333679\n",
      "[2000]\ttraining's rmse: 0.0215201\tvalid_1's rmse: 0.0324397\n",
      "[2100]\ttraining's rmse: 0.0205573\tvalid_1's rmse: 0.0315249\n",
      "[2200]\ttraining's rmse: 0.0197264\tvalid_1's rmse: 0.0307424\n",
      "[2300]\ttraining's rmse: 0.0189714\tvalid_1's rmse: 0.0300212\n",
      "[2400]\ttraining's rmse: 0.018256\tvalid_1's rmse: 0.0293311\n",
      "[2500]\ttraining's rmse: 0.0176237\tvalid_1's rmse: 0.0287311\n",
      "[2600]\ttraining's rmse: 0.0170104\tvalid_1's rmse: 0.0281251\n",
      "[2700]\ttraining's rmse: 0.0164315\tvalid_1's rmse: 0.0275431\n",
      "[2800]\ttraining's rmse: 0.015856\tvalid_1's rmse: 0.0269966\n",
      "[2900]\ttraining's rmse: 0.0153511\tvalid_1's rmse: 0.0264924\n",
      "[3000]\ttraining's rmse: 0.0148579\tvalid_1's rmse: 0.0260019\n",
      "[3100]\ttraining's rmse: 0.014429\tvalid_1's rmse: 0.0255841\n",
      "[3200]\ttraining's rmse: 0.0139744\tvalid_1's rmse: 0.0251167\n",
      "[3300]\ttraining's rmse: 0.0135818\tvalid_1's rmse: 0.0247337\n",
      "[3400]\ttraining's rmse: 0.013228\tvalid_1's rmse: 0.0243861\n",
      "[3500]\ttraining's rmse: 0.0128943\tvalid_1's rmse: 0.0240778\n",
      "[3600]\ttraining's rmse: 0.0125752\tvalid_1's rmse: 0.0237512\n",
      "[3700]\ttraining's rmse: 0.0122828\tvalid_1's rmse: 0.0234773\n",
      "[3800]\ttraining's rmse: 0.012001\tvalid_1's rmse: 0.0232041\n",
      "[3900]\ttraining's rmse: 0.0117334\tvalid_1's rmse: 0.0229656\n",
      "[4000]\ttraining's rmse: 0.011479\tvalid_1's rmse: 0.0227126\n",
      "[4100]\ttraining's rmse: 0.0112253\tvalid_1's rmse: 0.0224626\n",
      "[4200]\ttraining's rmse: 0.0109989\tvalid_1's rmse: 0.0222452\n",
      "[4300]\ttraining's rmse: 0.0107803\tvalid_1's rmse: 0.0220321\n",
      "[4400]\ttraining's rmse: 0.010578\tvalid_1's rmse: 0.0218314\n",
      "[4500]\ttraining's rmse: 0.0103755\tvalid_1's rmse: 0.0216498\n",
      "[4600]\ttraining's rmse: 0.0101858\tvalid_1's rmse: 0.0214768\n",
      "[4700]\ttraining's rmse: 0.0100044\tvalid_1's rmse: 0.0212929\n",
      "[4800]\ttraining's rmse: 0.00983534\tvalid_1's rmse: 0.0211391\n",
      "[4900]\ttraining's rmse: 0.00967263\tvalid_1's rmse: 0.0209992\n",
      "[5000]\ttraining's rmse: 0.00951581\tvalid_1's rmse: 0.020856\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00951581\tvalid_1's rmse: 0.020856\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.103679\tvalid_1's rmse: 0.105156\n",
      "[200]\ttraining's rmse: 0.0534287\tvalid_1's rmse: 0.0562323\n",
      "[300]\ttraining's rmse: 0.0404703\tvalid_1's rmse: 0.0441297\n",
      "[400]\ttraining's rmse: 0.0369063\tvalid_1's rmse: 0.0411245\n",
      "[500]\ttraining's rmse: 0.0351157\tvalid_1's rmse: 0.0398516\n",
      "[600]\ttraining's rmse: 0.0338132\tvalid_1's rmse: 0.0390066\n",
      "[700]\ttraining's rmse: 0.0326878\tvalid_1's rmse: 0.0383392\n",
      "[800]\ttraining's rmse: 0.0316401\tvalid_1's rmse: 0.0377065\n",
      "[900]\ttraining's rmse: 0.0306214\tvalid_1's rmse: 0.0370837\n",
      "[1000]\ttraining's rmse: 0.0297107\tvalid_1's rmse: 0.0365557\n",
      "[1100]\ttraining's rmse: 0.0288512\tvalid_1's rmse: 0.0360446\n",
      "[1200]\ttraining's rmse: 0.0280242\tvalid_1's rmse: 0.0355216\n",
      "[1300]\ttraining's rmse: 0.0272498\tvalid_1's rmse: 0.0350517\n",
      "[1400]\ttraining's rmse: 0.0265068\tvalid_1's rmse: 0.0345575\n",
      "[1500]\ttraining's rmse: 0.0258094\tvalid_1's rmse: 0.0341299\n",
      "[1600]\ttraining's rmse: 0.0251171\tvalid_1's rmse: 0.0336819\n",
      "[1700]\ttraining's rmse: 0.0244766\tvalid_1's rmse: 0.0332341\n",
      "[1800]\ttraining's rmse: 0.023842\tvalid_1's rmse: 0.0328274\n",
      "[1900]\ttraining's rmse: 0.0232821\tvalid_1's rmse: 0.03247\n",
      "[2000]\ttraining's rmse: 0.022689\tvalid_1's rmse: 0.0320755\n",
      "[2100]\ttraining's rmse: 0.0221378\tvalid_1's rmse: 0.0316916\n",
      "[2200]\ttraining's rmse: 0.0216279\tvalid_1's rmse: 0.0313502\n",
      "[2300]\ttraining's rmse: 0.0211362\tvalid_1's rmse: 0.03104\n",
      "[2400]\ttraining's rmse: 0.0206441\tvalid_1's rmse: 0.0306874\n",
      "[2500]\ttraining's rmse: 0.0201696\tvalid_1's rmse: 0.0303784\n",
      "[2600]\ttraining's rmse: 0.0197227\tvalid_1's rmse: 0.0300699\n",
      "[2700]\ttraining's rmse: 0.0192799\tvalid_1's rmse: 0.0297629\n",
      "[2800]\ttraining's rmse: 0.0188672\tvalid_1's rmse: 0.0294727\n",
      "[2900]\ttraining's rmse: 0.0184706\tvalid_1's rmse: 0.0292068\n",
      "[3000]\ttraining's rmse: 0.0180898\tvalid_1's rmse: 0.0289511\n",
      "[3100]\ttraining's rmse: 0.0177146\tvalid_1's rmse: 0.0286927\n",
      "[3200]\ttraining's rmse: 0.0173537\tvalid_1's rmse: 0.0284522\n",
      "[3300]\ttraining's rmse: 0.0170066\tvalid_1's rmse: 0.0282163\n",
      "[3400]\ttraining's rmse: 0.0166761\tvalid_1's rmse: 0.027988\n",
      "[3500]\ttraining's rmse: 0.0163582\tvalid_1's rmse: 0.0277649\n",
      "[3600]\ttraining's rmse: 0.0160309\tvalid_1's rmse: 0.027523\n",
      "[3700]\ttraining's rmse: 0.0157149\tvalid_1's rmse: 0.0273004\n",
      "[3800]\ttraining's rmse: 0.0154191\tvalid_1's rmse: 0.0271023\n",
      "[3900]\ttraining's rmse: 0.0151261\tvalid_1's rmse: 0.02689\n",
      "[4000]\ttraining's rmse: 0.0148501\tvalid_1's rmse: 0.0267003\n",
      "[4100]\ttraining's rmse: 0.0145776\tvalid_1's rmse: 0.0265028\n",
      "[4200]\ttraining's rmse: 0.0143205\tvalid_1's rmse: 0.0263288\n",
      "[4300]\ttraining's rmse: 0.0140611\tvalid_1's rmse: 0.0261317\n",
      "[4400]\ttraining's rmse: 0.0138056\tvalid_1's rmse: 0.0259453\n",
      "[4500]\ttraining's rmse: 0.0135638\tvalid_1's rmse: 0.0257761\n",
      "[4600]\ttraining's rmse: 0.0133285\tvalid_1's rmse: 0.0256036\n",
      "[4700]\ttraining's rmse: 0.0131021\tvalid_1's rmse: 0.0254586\n",
      "[4800]\ttraining's rmse: 0.0128763\tvalid_1's rmse: 0.0252953\n",
      "[4900]\ttraining's rmse: 0.0126662\tvalid_1's rmse: 0.025148\n",
      "[5000]\ttraining's rmse: 0.0124642\tvalid_1's rmse: 0.0250189\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.0124642\tvalid_1's rmse: 0.0250189\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.133645\tvalid_1's rmse: 0.134407\n",
      "[200]\ttraining's rmse: 0.0904811\tvalid_1's rmse: 0.0945224\n",
      "[300]\ttraining's rmse: 0.0734905\tvalid_1's rmse: 0.0791941\n",
      "[400]\ttraining's rmse: 0.0643476\tvalid_1's rmse: 0.0709522\n",
      "[500]\ttraining's rmse: 0.0579313\tvalid_1's rmse: 0.0652406\n",
      "[600]\ttraining's rmse: 0.0530704\tvalid_1's rmse: 0.0608586\n",
      "[700]\ttraining's rmse: 0.0489428\tvalid_1's rmse: 0.0570547\n",
      "[800]\ttraining's rmse: 0.0449913\tvalid_1's rmse: 0.0534339\n",
      "[900]\ttraining's rmse: 0.0417192\tvalid_1's rmse: 0.0503579\n",
      "[1000]\ttraining's rmse: 0.0388392\tvalid_1's rmse: 0.0477317\n",
      "[1100]\ttraining's rmse: 0.036353\tvalid_1's rmse: 0.045431\n",
      "[1200]\ttraining's rmse: 0.0341723\tvalid_1's rmse: 0.0433933\n",
      "[1300]\ttraining's rmse: 0.032301\tvalid_1's rmse: 0.041721\n",
      "[1400]\ttraining's rmse: 0.0304379\tvalid_1's rmse: 0.039956\n",
      "[1500]\ttraining's rmse: 0.0288741\tvalid_1's rmse: 0.038522\n",
      "[1600]\ttraining's rmse: 0.027419\tvalid_1's rmse: 0.0371298\n",
      "[1700]\ttraining's rmse: 0.02609\tvalid_1's rmse: 0.0359268\n",
      "[1800]\ttraining's rmse: 0.0249498\tvalid_1's rmse: 0.0348679\n",
      "[1900]\ttraining's rmse: 0.0238756\tvalid_1's rmse: 0.0338646\n",
      "[2000]\ttraining's rmse: 0.0227786\tvalid_1's rmse: 0.0328212\n",
      "[2100]\ttraining's rmse: 0.0218492\tvalid_1's rmse: 0.0319791\n",
      "[2200]\ttraining's rmse: 0.0210316\tvalid_1's rmse: 0.0312247\n",
      "[2300]\ttraining's rmse: 0.0202594\tvalid_1's rmse: 0.0305118\n",
      "[2400]\ttraining's rmse: 0.0195012\tvalid_1's rmse: 0.0297876\n",
      "[2500]\ttraining's rmse: 0.0188303\tvalid_1's rmse: 0.0291586\n",
      "[2600]\ttraining's rmse: 0.0182254\tvalid_1's rmse: 0.0286051\n",
      "[2700]\ttraining's rmse: 0.017645\tvalid_1's rmse: 0.028081\n",
      "[2800]\ttraining's rmse: 0.0171201\tvalid_1's rmse: 0.0275974\n",
      "[2900]\ttraining's rmse: 0.0166004\tvalid_1's rmse: 0.0271216\n",
      "[3000]\ttraining's rmse: 0.0161128\tvalid_1's rmse: 0.0266507\n",
      "[3100]\ttraining's rmse: 0.0156816\tvalid_1's rmse: 0.0262708\n",
      "[3200]\ttraining's rmse: 0.0152631\tvalid_1's rmse: 0.0258969\n",
      "[3300]\ttraining's rmse: 0.0148559\tvalid_1's rmse: 0.0255099\n",
      "[3400]\ttraining's rmse: 0.0144847\tvalid_1's rmse: 0.0251654\n",
      "[3500]\ttraining's rmse: 0.0141495\tvalid_1's rmse: 0.0248748\n",
      "[3600]\ttraining's rmse: 0.0138121\tvalid_1's rmse: 0.0245749\n",
      "[3700]\ttraining's rmse: 0.0135052\tvalid_1's rmse: 0.0243082\n",
      "[3800]\ttraining's rmse: 0.0132247\tvalid_1's rmse: 0.0240852\n",
      "[3900]\ttraining's rmse: 0.0129569\tvalid_1's rmse: 0.0238637\n",
      "[4000]\ttraining's rmse: 0.0126987\tvalid_1's rmse: 0.023659\n",
      "[4100]\ttraining's rmse: 0.0124522\tvalid_1's rmse: 0.0234552\n",
      "[4200]\ttraining's rmse: 0.0122131\tvalid_1's rmse: 0.0232563\n",
      "[4300]\ttraining's rmse: 0.0119926\tvalid_1's rmse: 0.023078\n",
      "[4400]\ttraining's rmse: 0.0117684\tvalid_1's rmse: 0.022888\n",
      "[4500]\ttraining's rmse: 0.0115639\tvalid_1's rmse: 0.0227209\n",
      "[4600]\ttraining's rmse: 0.0113617\tvalid_1's rmse: 0.0225641\n",
      "[4700]\ttraining's rmse: 0.0111681\tvalid_1's rmse: 0.0224258\n",
      "[4800]\ttraining's rmse: 0.0109936\tvalid_1's rmse: 0.0223023\n",
      "[4900]\ttraining's rmse: 0.0108153\tvalid_1's rmse: 0.0221702\n",
      "[5000]\ttraining's rmse: 0.0106468\tvalid_1's rmse: 0.0220521\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.0106468\tvalid_1's rmse: 0.0220521\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.145414\tvalid_1's rmse: 0.148386\n",
      "[200]\ttraining's rmse: 0.0902986\tvalid_1's rmse: 0.0950891\n",
      "[300]\ttraining's rmse: 0.0633864\tvalid_1's rmse: 0.0689989\n",
      "[400]\ttraining's rmse: 0.0473939\tvalid_1's rmse: 0.053386\n",
      "[500]\ttraining's rmse: 0.0374578\tvalid_1's rmse: 0.043693\n",
      "[600]\ttraining's rmse: 0.0313037\tvalid_1's rmse: 0.0376708\n",
      "[700]\ttraining's rmse: 0.0270679\tvalid_1's rmse: 0.033598\n",
      "[800]\ttraining's rmse: 0.0240726\tvalid_1's rmse: 0.0306268\n",
      "[900]\ttraining's rmse: 0.0217897\tvalid_1's rmse: 0.0283051\n",
      "[1000]\ttraining's rmse: 0.0201469\tvalid_1's rmse: 0.0266787\n",
      "[1100]\ttraining's rmse: 0.0189555\tvalid_1's rmse: 0.0255447\n",
      "[1200]\ttraining's rmse: 0.0179288\tvalid_1's rmse: 0.0245346\n",
      "[1300]\ttraining's rmse: 0.0171479\tvalid_1's rmse: 0.0238234\n",
      "[1400]\ttraining's rmse: 0.0164881\tvalid_1's rmse: 0.0232386\n",
      "[1500]\ttraining's rmse: 0.0159464\tvalid_1's rmse: 0.0227768\n",
      "[1600]\ttraining's rmse: 0.0154269\tvalid_1's rmse: 0.0224158\n",
      "[1700]\ttraining's rmse: 0.0149729\tvalid_1's rmse: 0.0220932\n",
      "[1800]\ttraining's rmse: 0.0145868\tvalid_1's rmse: 0.0218082\n",
      "[1900]\ttraining's rmse: 0.014257\tvalid_1's rmse: 0.0216293\n",
      "[2000]\ttraining's rmse: 0.0139097\tvalid_1's rmse: 0.0214177\n",
      "[2100]\ttraining's rmse: 0.013611\tvalid_1's rmse: 0.0212461\n",
      "[2200]\ttraining's rmse: 0.0133485\tvalid_1's rmse: 0.0211084\n",
      "[2300]\ttraining's rmse: 0.0130872\tvalid_1's rmse: 0.0210198\n",
      "[2400]\ttraining's rmse: 0.0128413\tvalid_1's rmse: 0.0209153\n",
      "[2500]\ttraining's rmse: 0.0126217\tvalid_1's rmse: 0.0208406\n",
      "[2600]\ttraining's rmse: 0.0124047\tvalid_1's rmse: 0.0207807\n",
      "[2700]\ttraining's rmse: 0.0121796\tvalid_1's rmse: 0.0206632\n",
      "[2800]\ttraining's rmse: 0.0119791\tvalid_1's rmse: 0.0205662\n",
      "[2900]\ttraining's rmse: 0.011809\tvalid_1's rmse: 0.0205026\n",
      "[3000]\ttraining's rmse: 0.0116311\tvalid_1's rmse: 0.0204349\n",
      "[3100]\ttraining's rmse: 0.0114526\tvalid_1's rmse: 0.0203882\n",
      "[3200]\ttraining's rmse: 0.011284\tvalid_1's rmse: 0.0203215\n",
      "[3300]\ttraining's rmse: 0.0111298\tvalid_1's rmse: 0.0202803\n",
      "[3400]\ttraining's rmse: 0.0109665\tvalid_1's rmse: 0.0202286\n",
      "[3500]\ttraining's rmse: 0.0108107\tvalid_1's rmse: 0.0201984\n",
      "[3600]\ttraining's rmse: 0.0106499\tvalid_1's rmse: 0.0201502\n",
      "[3700]\ttraining's rmse: 0.0104856\tvalid_1's rmse: 0.0200637\n",
      "[3800]\ttraining's rmse: 0.0103596\tvalid_1's rmse: 0.0200256\n",
      "[3900]\ttraining's rmse: 0.0102309\tvalid_1's rmse: 0.0199923\n",
      "[4000]\ttraining's rmse: 0.0100866\tvalid_1's rmse: 0.0199505\n",
      "[4100]\ttraining's rmse: 0.00993693\tvalid_1's rmse: 0.0199101\n",
      "[4200]\ttraining's rmse: 0.00980877\tvalid_1's rmse: 0.0198532\n",
      "[4300]\ttraining's rmse: 0.00965179\tvalid_1's rmse: 0.0197808\n",
      "[4400]\ttraining's rmse: 0.00953629\tvalid_1's rmse: 0.0197585\n",
      "[4500]\ttraining's rmse: 0.0094168\tvalid_1's rmse: 0.0197129\n",
      "[4600]\ttraining's rmse: 0.00929792\tvalid_1's rmse: 0.0196661\n",
      "[4700]\ttraining's rmse: 0.00919397\tvalid_1's rmse: 0.0196356\n",
      "[4800]\ttraining's rmse: 0.00907347\tvalid_1's rmse: 0.019611\n",
      "[4900]\ttraining's rmse: 0.0089614\tvalid_1's rmse: 0.0195748\n",
      "[5000]\ttraining's rmse: 0.00884064\tvalid_1's rmse: 0.0195279\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00884064\tvalid_1's rmse: 0.0195279\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.14334\tvalid_1's rmse: 0.145298\n",
      "[200]\ttraining's rmse: 0.100811\tvalid_1's rmse: 0.1049\n",
      "[300]\ttraining's rmse: 0.0785627\tvalid_1's rmse: 0.0838215\n",
      "[400]\ttraining's rmse: 0.0653266\tvalid_1's rmse: 0.0713767\n",
      "[500]\ttraining's rmse: 0.0561799\tvalid_1's rmse: 0.0625988\n",
      "[600]\ttraining's rmse: 0.0495035\tvalid_1's rmse: 0.0562715\n",
      "[700]\ttraining's rmse: 0.0442854\tvalid_1's rmse: 0.0512403\n",
      "[800]\ttraining's rmse: 0.039958\tvalid_1's rmse: 0.0471007\n",
      "[900]\ttraining's rmse: 0.0360658\tvalid_1's rmse: 0.043327\n",
      "[1000]\ttraining's rmse: 0.0330414\tvalid_1's rmse: 0.0403982\n",
      "[1100]\ttraining's rmse: 0.0305162\tvalid_1's rmse: 0.0379441\n",
      "[1200]\ttraining's rmse: 0.0283152\tvalid_1's rmse: 0.0358006\n",
      "[1300]\ttraining's rmse: 0.0264129\tvalid_1's rmse: 0.0339929\n",
      "[1400]\ttraining's rmse: 0.0247194\tvalid_1's rmse: 0.0322859\n",
      "[1500]\ttraining's rmse: 0.0233154\tvalid_1's rmse: 0.0308911\n",
      "[1600]\ttraining's rmse: 0.0219931\tvalid_1's rmse: 0.0295659\n",
      "[1700]\ttraining's rmse: 0.0208535\tvalid_1's rmse: 0.0284549\n",
      "[1800]\ttraining's rmse: 0.0198395\tvalid_1's rmse: 0.0274362\n",
      "[1900]\ttraining's rmse: 0.019035\tvalid_1's rmse: 0.0266353\n",
      "[2000]\ttraining's rmse: 0.0182301\tvalid_1's rmse: 0.0258083\n",
      "[2100]\ttraining's rmse: 0.0175269\tvalid_1's rmse: 0.0250779\n",
      "[2200]\ttraining's rmse: 0.0169188\tvalid_1's rmse: 0.0244844\n",
      "[2300]\ttraining's rmse: 0.0163773\tvalid_1's rmse: 0.0239518\n",
      "[2400]\ttraining's rmse: 0.0158607\tvalid_1's rmse: 0.0234153\n",
      "[2500]\ttraining's rmse: 0.015402\tvalid_1's rmse: 0.022984\n",
      "[2600]\ttraining's rmse: 0.0149651\tvalid_1's rmse: 0.0225659\n",
      "[2700]\ttraining's rmse: 0.0145624\tvalid_1's rmse: 0.022196\n",
      "[2800]\ttraining's rmse: 0.0142158\tvalid_1's rmse: 0.0218759\n",
      "[2900]\ttraining's rmse: 0.0139001\tvalid_1's rmse: 0.0215817\n",
      "[3000]\ttraining's rmse: 0.0136065\tvalid_1's rmse: 0.0213438\n",
      "[3100]\ttraining's rmse: 0.0133198\tvalid_1's rmse: 0.0211169\n",
      "[3200]\ttraining's rmse: 0.0130588\tvalid_1's rmse: 0.0209071\n",
      "[3300]\ttraining's rmse: 0.0128088\tvalid_1's rmse: 0.0206935\n",
      "[3400]\ttraining's rmse: 0.0125739\tvalid_1's rmse: 0.0205213\n",
      "[3500]\ttraining's rmse: 0.012357\tvalid_1's rmse: 0.0203626\n",
      "[3600]\ttraining's rmse: 0.0121361\tvalid_1's rmse: 0.0201844\n",
      "[3700]\ttraining's rmse: 0.0119274\tvalid_1's rmse: 0.0200398\n",
      "[3800]\ttraining's rmse: 0.0117273\tvalid_1's rmse: 0.019915\n",
      "[3900]\ttraining's rmse: 0.0115368\tvalid_1's rmse: 0.0197873\n",
      "[4000]\ttraining's rmse: 0.0113546\tvalid_1's rmse: 0.019667\n",
      "[4100]\ttraining's rmse: 0.0111852\tvalid_1's rmse: 0.0195556\n",
      "[4200]\ttraining's rmse: 0.0110237\tvalid_1's rmse: 0.0194622\n",
      "[4300]\ttraining's rmse: 0.0108556\tvalid_1's rmse: 0.0193599\n",
      "[4400]\ttraining's rmse: 0.0107024\tvalid_1's rmse: 0.0192582\n",
      "[4500]\ttraining's rmse: 0.0105548\tvalid_1's rmse: 0.0191732\n",
      "[4600]\ttraining's rmse: 0.0104124\tvalid_1's rmse: 0.0190807\n",
      "[4700]\ttraining's rmse: 0.0102696\tvalid_1's rmse: 0.0189994\n",
      "[4800]\ttraining's rmse: 0.010133\tvalid_1's rmse: 0.0189345\n",
      "[4900]\ttraining's rmse: 0.0100058\tvalid_1's rmse: 0.0188581\n",
      "[5000]\ttraining's rmse: 0.00988142\tvalid_1's rmse: 0.0187913\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.00988142\tvalid_1's rmse: 0.0187913\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.104341\tvalid_1's rmse: 0.105768\n",
      "[200]\ttraining's rmse: 0.0530967\tvalid_1's rmse: 0.0559205\n",
      "[300]\ttraining's rmse: 0.0397853\tvalid_1's rmse: 0.0434804\n",
      "[400]\ttraining's rmse: 0.0362691\tvalid_1's rmse: 0.0405108\n",
      "[500]\ttraining's rmse: 0.0344843\tvalid_1's rmse: 0.0391365\n",
      "[600]\ttraining's rmse: 0.0331571\tvalid_1's rmse: 0.038246\n",
      "[700]\ttraining's rmse: 0.03204\tvalid_1's rmse: 0.0375668\n",
      "[800]\ttraining's rmse: 0.0310331\tvalid_1's rmse: 0.036957\n",
      "[900]\ttraining's rmse: 0.0301096\tvalid_1's rmse: 0.0363902\n",
      "[1000]\ttraining's rmse: 0.0291928\tvalid_1's rmse: 0.0358197\n",
      "[1100]\ttraining's rmse: 0.028285\tvalid_1's rmse: 0.0352252\n",
      "[1200]\ttraining's rmse: 0.0274596\tvalid_1's rmse: 0.0347096\n",
      "[1300]\ttraining's rmse: 0.0266825\tvalid_1's rmse: 0.0342064\n",
      "[1400]\ttraining's rmse: 0.0259514\tvalid_1's rmse: 0.0337189\n",
      "[1500]\ttraining's rmse: 0.0252599\tvalid_1's rmse: 0.0332899\n",
      "[1600]\ttraining's rmse: 0.0245856\tvalid_1's rmse: 0.0328452\n",
      "[1700]\ttraining's rmse: 0.0239699\tvalid_1's rmse: 0.0324832\n",
      "[1800]\ttraining's rmse: 0.023387\tvalid_1's rmse: 0.0321031\n",
      "[1900]\ttraining's rmse: 0.0228246\tvalid_1's rmse: 0.0317547\n",
      "[2000]\ttraining's rmse: 0.0222771\tvalid_1's rmse: 0.0313922\n",
      "[2100]\ttraining's rmse: 0.0217471\tvalid_1's rmse: 0.0310511\n",
      "[2200]\ttraining's rmse: 0.0212538\tvalid_1's rmse: 0.0307359\n",
      "[2300]\ttraining's rmse: 0.0207741\tvalid_1's rmse: 0.0304289\n",
      "[2400]\ttraining's rmse: 0.0203047\tvalid_1's rmse: 0.0301126\n",
      "[2500]\ttraining's rmse: 0.0198576\tvalid_1's rmse: 0.0298201\n",
      "[2600]\ttraining's rmse: 0.0194263\tvalid_1's rmse: 0.0295408\n",
      "[2700]\ttraining's rmse: 0.0190044\tvalid_1's rmse: 0.0292512\n",
      "[2800]\ttraining's rmse: 0.018593\tvalid_1's rmse: 0.0289745\n",
      "[2900]\ttraining's rmse: 0.0182015\tvalid_1's rmse: 0.0287183\n",
      "[3000]\ttraining's rmse: 0.0178184\tvalid_1's rmse: 0.0284552\n",
      "[3100]\ttraining's rmse: 0.0174467\tvalid_1's rmse: 0.028204\n",
      "[3200]\ttraining's rmse: 0.0171032\tvalid_1's rmse: 0.0279818\n",
      "[3300]\ttraining's rmse: 0.0167645\tvalid_1's rmse: 0.0277638\n",
      "[3400]\ttraining's rmse: 0.0164414\tvalid_1's rmse: 0.0275628\n",
      "[3500]\ttraining's rmse: 0.016116\tvalid_1's rmse: 0.0273383\n",
      "[3600]\ttraining's rmse: 0.0158073\tvalid_1's rmse: 0.0271338\n",
      "[3700]\ttraining's rmse: 0.0155095\tvalid_1's rmse: 0.0269404\n",
      "[3800]\ttraining's rmse: 0.0152207\tvalid_1's rmse: 0.0267557\n",
      "[3900]\ttraining's rmse: 0.0149296\tvalid_1's rmse: 0.0265394\n",
      "[4000]\ttraining's rmse: 0.0146555\tvalid_1's rmse: 0.0263696\n",
      "[4100]\ttraining's rmse: 0.0143976\tvalid_1's rmse: 0.0262167\n",
      "[4200]\ttraining's rmse: 0.0141421\tvalid_1's rmse: 0.0260419\n",
      "[4300]\ttraining's rmse: 0.0138912\tvalid_1's rmse: 0.0258676\n",
      "[4400]\ttraining's rmse: 0.0136499\tvalid_1's rmse: 0.0257045\n",
      "[4500]\ttraining's rmse: 0.0134205\tvalid_1's rmse: 0.0255605\n",
      "[4600]\ttraining's rmse: 0.0131901\tvalid_1's rmse: 0.0254093\n",
      "[4700]\ttraining's rmse: 0.0129742\tvalid_1's rmse: 0.0252834\n",
      "[4800]\ttraining's rmse: 0.0127558\tvalid_1's rmse: 0.0251405\n",
      "[4900]\ttraining's rmse: 0.0125483\tvalid_1's rmse: 0.0250122\n",
      "[5000]\ttraining's rmse: 0.0123523\tvalid_1's rmse: 0.0248927\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.0123523\tvalid_1's rmse: 0.0248927\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.137177\tvalid_1's rmse: 0.140894\n",
      "[200]\ttraining's rmse: 0.0968077\tvalid_1's rmse: 0.103034\n",
      "[300]\ttraining's rmse: 0.0782083\tvalid_1's rmse: 0.0858723\n",
      "[400]\ttraining's rmse: 0.0667096\tvalid_1's rmse: 0.0751059\n",
      "[500]\ttraining's rmse: 0.0584593\tvalid_1's rmse: 0.0671742\n",
      "[600]\ttraining's rmse: 0.0523484\tvalid_1's rmse: 0.061265\n",
      "[700]\ttraining's rmse: 0.0473976\tvalid_1's rmse: 0.0564864\n",
      "[800]\ttraining's rmse: 0.0433216\tvalid_1's rmse: 0.0525327\n",
      "[900]\ttraining's rmse: 0.0396041\tvalid_1's rmse: 0.0489288\n",
      "[1000]\ttraining's rmse: 0.036509\tvalid_1's rmse: 0.0459065\n",
      "[1100]\ttraining's rmse: 0.0338704\tvalid_1's rmse: 0.043317\n",
      "[1200]\ttraining's rmse: 0.0314547\tvalid_1's rmse: 0.0409081\n",
      "[1300]\ttraining's rmse: 0.0293759\tvalid_1's rmse: 0.0388719\n",
      "[1400]\ttraining's rmse: 0.0275134\tvalid_1's rmse: 0.0369624\n",
      "[1500]\ttraining's rmse: 0.0259267\tvalid_1's rmse: 0.0353711\n",
      "[1600]\ttraining's rmse: 0.024458\tvalid_1's rmse: 0.0338714\n",
      "[1700]\ttraining's rmse: 0.0231905\tvalid_1's rmse: 0.0325662\n",
      "[1800]\ttraining's rmse: 0.0220398\tvalid_1's rmse: 0.0313747\n",
      "[1900]\ttraining's rmse: 0.021082\tvalid_1's rmse: 0.030388\n",
      "[2000]\ttraining's rmse: 0.0201689\tvalid_1's rmse: 0.0294134\n",
      "[2100]\ttraining's rmse: 0.0193547\tvalid_1's rmse: 0.028556\n",
      "[2200]\ttraining's rmse: 0.0186448\tvalid_1's rmse: 0.027819\n",
      "[2300]\ttraining's rmse: 0.0180008\tvalid_1's rmse: 0.0271618\n",
      "[2400]\ttraining's rmse: 0.0173952\tvalid_1's rmse: 0.0265109\n",
      "[2500]\ttraining's rmse: 0.0168384\tvalid_1's rmse: 0.0259383\n",
      "[2600]\ttraining's rmse: 0.0163337\tvalid_1's rmse: 0.0253964\n",
      "[2700]\ttraining's rmse: 0.0158614\tvalid_1's rmse: 0.0249087\n",
      "[2800]\ttraining's rmse: 0.015439\tvalid_1's rmse: 0.0244634\n",
      "[2900]\ttraining's rmse: 0.0150382\tvalid_1's rmse: 0.0240589\n",
      "[3000]\ttraining's rmse: 0.0146747\tvalid_1's rmse: 0.0237203\n",
      "[3100]\ttraining's rmse: 0.0143264\tvalid_1's rmse: 0.0233699\n",
      "[3200]\ttraining's rmse: 0.0140139\tvalid_1's rmse: 0.023066\n",
      "[3300]\ttraining's rmse: 0.0137182\tvalid_1's rmse: 0.022781\n",
      "[3400]\ttraining's rmse: 0.013426\tvalid_1's rmse: 0.0225177\n",
      "[3500]\ttraining's rmse: 0.0131651\tvalid_1's rmse: 0.0222804\n",
      "[3600]\ttraining's rmse: 0.012912\tvalid_1's rmse: 0.0220376\n",
      "[3700]\ttraining's rmse: 0.0126551\tvalid_1's rmse: 0.0218027\n",
      "[3800]\ttraining's rmse: 0.0124373\tvalid_1's rmse: 0.0216114\n",
      "[3900]\ttraining's rmse: 0.0122198\tvalid_1's rmse: 0.0214088\n",
      "[4000]\ttraining's rmse: 0.012016\tvalid_1's rmse: 0.0212477\n",
      "[4100]\ttraining's rmse: 0.0118227\tvalid_1's rmse: 0.0210953\n",
      "[4200]\ttraining's rmse: 0.0116377\tvalid_1's rmse: 0.0209469\n",
      "[4300]\ttraining's rmse: 0.0114509\tvalid_1's rmse: 0.0208116\n",
      "[4400]\ttraining's rmse: 0.0112648\tvalid_1's rmse: 0.0206767\n",
      "[4500]\ttraining's rmse: 0.0111011\tvalid_1's rmse: 0.0205476\n",
      "[4600]\ttraining's rmse: 0.010943\tvalid_1's rmse: 0.0204325\n",
      "[4700]\ttraining's rmse: 0.0107882\tvalid_1's rmse: 0.0203373\n",
      "[4800]\ttraining's rmse: 0.010638\tvalid_1's rmse: 0.0202306\n",
      "[4900]\ttraining's rmse: 0.0104987\tvalid_1's rmse: 0.0201397\n",
      "[5000]\ttraining's rmse: 0.0103661\tvalid_1's rmse: 0.0200689\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.0103661\tvalid_1's rmse: 0.0200689\n",
      "✅ LightGBM training with parent MLflow run and plots completed\n"
     ]
    }
   ],
   "source": [
    "import os, time, pickle, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import mlflow\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    denominator = np.where(denominator == 0, np.finfo(float).eps, denominator)\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / denominator)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_value = np.percentile(y_true, percentile)\n",
    "    peak_indices = y_true >= peak_value\n",
    "    if np.sum(peak_indices) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_indices] - y_pred[peak_indices]))\n",
    "\n",
    "# -----------------------------\n",
    "# Paths and parameters\n",
    "# -----------------------------\n",
    "root = '/mnt/data/home/zayd/Digital_twin_project/machine_learning/dataset/Ctown/junctions/'\n",
    "version = '0.1.0'\n",
    "MODEL_DIR = f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/LightGBM_{version}'\n",
    "os.makedirs(f'{MODEL_DIR}/scalers', exist_ok=True)\n",
    "\n",
    "lag_steps = [1,2,3]\n",
    "rolling_windows = [3,6,12]\n",
    "\n",
    "mlflow.set_experiment(\"Digital_Twin_Experiments\")\n",
    "global_y_true, global_y_pred = [], []\n",
    "feature_and_target = {}\n",
    "\n",
    "# -----------------------------\n",
    "# Parent MLflow run\n",
    "# -----------------------------\n",
    "with mlflow.start_run(run_name=f\"LightGBM_{version}\"):  \n",
    "    for filename in os.listdir(root)[:10]:\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        df = df.sort_values(by=[\"scenario_id\", \"time_id\"]).reset_index(drop=True)\n",
    "        junction_cols = [col for col in df.columns if col.startswith('J')]\n",
    "\n",
    "        # Convert from m³/s to L/s\n",
    "        df[junction_cols] = df[junction_cols] * 1000\n",
    "        junction = os.path.splitext(filename)[0]\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"{junction}\", nested=True):\n",
    "            # -----------------------------\n",
    "            # Feature engineering\n",
    "            # -----------------------------\n",
    "            for lag in lag_steps: df[f'{junction}_lag{lag}'] = df[junction].shift(lag)\n",
    "            for w in rolling_windows: df[f'{junction}_rollmean{w}'] = df[junction].rolling(window=w, min_periods=1).mean()\n",
    "            df = df.fillna(0)\n",
    "\n",
    "            features = [c for c in df.columns if c != junction]\n",
    "            target = junction\n",
    "            feature_and_target[junction] = {\"target\": target, \"features\": features, \"lags\": lag_steps, \"rolling\": rolling_windows}\n",
    "\n",
    "            # -----------------------------\n",
    "            # Scaling\n",
    "            # -----------------------------\n",
    "            feature_scaler = MinMaxScaler().fit(df[features])\n",
    "            target_scaler = MinMaxScaler().fit(df[[target]])\n",
    "            df[features] = feature_scaler.transform(df[features])\n",
    "            df[[target]] = target_scaler.transform(df[[target]])\n",
    "\n",
    "            # Save scalers\n",
    "            with open(f'{MODEL_DIR}/scalers/{junction}_feature_scaler.save', 'wb') as f: pickle.dump(feature_scaler, f)\n",
    "            with open(f'{MODEL_DIR}/scalers/{junction}_target_scaler.save', 'wb') as f: pickle.dump(target_scaler, f)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Train/test split\n",
    "            # -----------------------------\n",
    "            scenario_ids = df['scenario_id'].unique()\n",
    "            split = int(0.8 * len(scenario_ids))\n",
    "            train_df = df[df['scenario_id'].isin(scenario_ids[:split])]\n",
    "            test_df = df[df['scenario_id'].isin(scenario_ids[split:])]\n",
    "            X_train, y_train = train_df[features], train_df[target]\n",
    "            X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "            # -----------------------------\n",
    "            # Train LightGBM\n",
    "            # -----------------------------\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "            params = {'objective':'regression','metric':'rmse','learning_rate':0.01,'num_leaves':31,\n",
    "                      'feature_fraction':0.8,'bagging_fraction':0.8,'bagging_freq':5,'verbose':-1}\n",
    "            start_time = time.time()\n",
    "            model = lgb.train(params, train_data, num_boost_round=5000, valid_sets=[train_data, valid_data],\n",
    "                              callbacks=[lgb.early_stopping(100), lgb.log_evaluation(period=100)])\n",
    "            elapsed_sec = time.time() - start_time\n",
    "\n",
    "            # -----------------------------\n",
    "            # Predictions & metrics\n",
    "            # -----------------------------\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "            y_test_inv = target_scaler.inverse_transform(y_test.values.reshape(-1,1))\n",
    "            y_pred_inv = target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
    "            global_y_true.append(y_test_inv)\n",
    "            global_y_pred.append(y_pred_inv)\n",
    "\n",
    "            # Log parameters & metrics for junction\n",
    "            mlflow.log_params({\n",
    "                \"algo\": f\"LightGBM_{version}\",\n",
    "                \"junction\": junction,\n",
    "                \"window\": rolling_windows,\n",
    "                \"stride\": lag_steps,\n",
    "                \"num_leaves\": 31,\n",
    "                \"learning_rate\": 0.01,\n",
    "                \"feature_fraction\": 0.8,\n",
    "                \"bagging_fraction\": 0.8,\n",
    "                \"bagging_freq\": 5\n",
    "            })\n",
    "            mlflow.log_metric(\"training_time_sec\", elapsed_sec)\n",
    "            mlflow.log_metric(\"MAE\", mean_absolute_error(y_test_inv, y_pred_inv))\n",
    "            mlflow.log_metric(\"RMSE\", np.sqrt(mean_squared_error(y_test_inv, y_pred_inv)))\n",
    "            mlflow.log_metric(\"SMAPE\", smape(y_test_inv, y_pred_inv))\n",
    "            mlflow.log_metric(\"R2_score\", r2_score(y_test_inv, y_pred_inv))\n",
    "            mlflow.log_metric(\"peak_error\", peak_error(y_test_inv, y_pred_inv))\n",
    "            \n",
    "\n",
    "            # -----------------------------\n",
    "            # Save model artifact\n",
    "            # -----------------------------\n",
    "            model_path = os.path.join(MODEL_DIR, f\"{junction}.txt\")\n",
    "            model.save_model(model_path)\n",
    "            mlflow.log_artifact(model_path)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Plot predictions\n",
    "            # -----------------------------\n",
    "            plt.figure(figsize=(12,6))\n",
    "            plt.plot(y_test_inv[:100], label='Actual', color='orange')\n",
    "            plt.plot(y_pred_inv[:100], label='Predicted', color='green')\n",
    "            plt.xlabel('Time step')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'{junction} LightGBM Forecast')\n",
    "            plt.legend()\n",
    "            plot_path = os.path.join(MODEL_DIR, f\"{junction}_forecast.png\")\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            mlflow.log_artifact(plot_path)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Log global metrics for parent run\n",
    "    # -----------------------------\n",
    "    all_y_true = np.vstack(global_y_true)\n",
    "    all_y_pred = np.vstack(global_y_pred)\n",
    "    global_metrics = {\n",
    "        'MAE': mean_absolute_error(all_y_true, all_y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(all_y_true, all_y_pred)),\n",
    "        'SMAPE': smape(all_y_true, all_y_pred),\n",
    "        'R2_score': r2_score(all_y_true, all_y_pred),\n",
    "        'peak_error': peak_error(all_y_true, all_y_pred)\n",
    "    }\n",
    "    mlflow.log_metrics(global_metrics)\n",
    "\n",
    "    # Save feature/target mapping\n",
    "    with open(f'{MODEL_DIR}/feature_and_target.json', 'w') as f: json.dump(feature_and_target, f, indent=4)\n",
    "\n",
    "print(\"✅ LightGBM training with parent MLflow run and plots completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393f5e9",
   "metadata": {},
   "source": [
    "LightGBM (hyperparameter tuning + expanding-window / forward-chaining CV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0deb622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, pickle, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import mlflow\n",
    "import optuna\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    denominator = np.where(denominator == 0, np.finfo(float).eps, denominator)\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / denominator)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_value = np.percentile(y_true, percentile)\n",
    "    peak_indices = y_true >= peak_value\n",
    "    if np.sum(peak_indices) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_indices] - y_pred[peak_indices]))\n",
    "\n",
    "# -----------------------------\n",
    "# Paths and parameters\n",
    "# -----------------------------\n",
    "root = '/mnt/data/home/zayd/Digital_twin_project/machine_learning/dataset/Ctown/junctions/'\n",
    "version = '0.0.5_hyperparams'\n",
    "MODEL_DIR = f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/LightGBM_{version}'\n",
    "os.makedirs(f'{MODEL_DIR}/scalers', exist_ok=True)\n",
    "\n",
    "lag_steps = [1,2,3]\n",
    "rolling_windows = [3,6,12]\n",
    "\n",
    "mlflow.set_experiment(\"Digital_Twin_Experiments\")\n",
    "global_y_true, global_y_pred = [], []\n",
    "feature_and_target = {}\n",
    "\n",
    "# -----------------------------\n",
    "# Parent MLflow run\n",
    "# -----------------------------\n",
    "with mlflow.start_run(run_name=f\"LightGBM_{version}\"):\n",
    "\n",
    "    for filename in os.listdir(root)[:10]:\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        df = df.sort_values(by=[\"scenario_id\", \"time_id\"]).reset_index(drop=True)\n",
    "        junction = os.path.splitext(filename)[0]\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"{junction}\", nested=True):\n",
    "\n",
    "            # -----------------------------\n",
    "            # Feature engineering\n",
    "            # -----------------------------\n",
    "            for lag in lag_steps: df[f'{junction}_lag{lag}'] = df[junction].shift(lag)\n",
    "            for w in rolling_windows: df[f'{junction}_rollmean{w}'] = df[junction].rolling(window=w, min_periods=1).mean()\n",
    "            df = df.fillna(0)\n",
    "\n",
    "            features = [c for c in df.columns if c != junction]\n",
    "            target = junction\n",
    "            feature_and_target[junction] = {\"target\": target, \"features\": features, \"lags\": lag_steps, \"rolling\": rolling_windows}\n",
    "\n",
    "            # -----------------------------\n",
    "            # Scaling\n",
    "            # -----------------------------\n",
    "            feature_scaler = MinMaxScaler().fit(df[features])\n",
    "            target_scaler = MinMaxScaler().fit(df[[target]])\n",
    "            df[features] = feature_scaler.transform(df[features])\n",
    "            df[[target]] = target_scaler.transform(df[[target]])\n",
    "\n",
    "            with open(f'{MODEL_DIR}/scalers/{junction}_feature_scaler.save', 'wb') as f: pickle.dump(feature_scaler, f)\n",
    "            with open(f'{MODEL_DIR}/scalers/{junction}_target_scaler.save', 'wb') as f: pickle.dump(target_scaler, f)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Prepare 30-day block CV splits for Optuna\n",
    "            # -----------------------------\n",
    "            scenario_ids = df['scenario_id'].unique()\n",
    "            block_size = 30\n",
    "            train_val_splits = []\n",
    "            start = 0\n",
    "            num_days = len(scenario_ids)\n",
    "\n",
    "            while (start + 2*block_size) <= (num_days - 1):  # reserve last block for holdout\n",
    "                train_days = scenario_ids[start : start + block_size]\n",
    "                val_days = scenario_ids[start + block_size : start + 2*block_size]\n",
    "                train_idx = df[df['scenario_id'].isin(train_days)].index\n",
    "                val_idx = df[df['scenario_id'].isin(val_days)].index\n",
    "                train_val_splits.append((train_idx, val_idx))\n",
    "                start += block_size\n",
    "\n",
    "            X = df[features]\n",
    "            y = df[target].values\n",
    "\n",
    "            # -----------------------------\n",
    "            # Optuna hyperparameter tuning\n",
    "            # -----------------------------\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    'objective': 'regression',\n",
    "                    'metric': 'rmse',\n",
    "                    'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n",
    "                    'num_leaves': trial.suggest_int('num_leaves', 16, 128),\n",
    "                    'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n",
    "                    'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n",
    "                    'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "                    'min_data_in_leaf': 5,\n",
    "                    'min_data_in_bin': 5,\n",
    "                    'verbose': -1\n",
    "                }\n",
    "                val_errors = []\n",
    "                for train_idx, val_idx in train_val_splits:\n",
    "                    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                    y_train, y_val = y[train_idx], y[val_idx]\n",
    "                    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "                    valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "                    model = lgb.train(params, train_data, num_boost_round=5000,\n",
    "                                      valid_sets=[valid_data],\n",
    "                                      callbacks=[lgb.early_stopping(100), lgb.log_evaluation(period=0)])\n",
    "                    y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "                    y_pred_inv = target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
    "                    y_val_inv = target_scaler.inverse_transform(y_val.reshape(-1,1))\n",
    "                    val_errors.append(mean_squared_error(y_val_inv, y_pred_inv))\n",
    "                return np.mean(val_errors)\n",
    "\n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=50)\n",
    "            best_params = study.best_params\n",
    "\n",
    "            # -----------------------------\n",
    "            # Log parameters\n",
    "            # -----------------------------\n",
    "            mlflow.log_params({\n",
    "                \"algo\": f\"LightGBM_{version}\",\n",
    "                \"junction\": junction,\n",
    "                \"window\": rolling_windows,\n",
    "                \"stride\": lag_steps\n",
    "            })\n",
    "            mlflow.log_params(best_params)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Final training with last 30-day holdout\n",
    "            # -----------------------------\n",
    "            final_val_days = scenario_ids[-block_size:]\n",
    "            train_days = scenario_ids[:-block_size]\n",
    "\n",
    "            train_idx = df[df['scenario_id'].isin(train_days)].index\n",
    "            val_idx = df[df['scenario_id'].isin(final_val_days)].index\n",
    "\n",
    "            X_train, y_train = df.loc[train_idx, features], df.loc[train_idx, target]\n",
    "            X_val, y_val = df.loc[val_idx, features], df.loc[val_idx, target]\n",
    "\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "            start_time = time.time()\n",
    "            best_model = lgb.train(best_params, train_data, num_boost_round=5000,\n",
    "                                   valid_sets=[train_data, valid_data],\n",
    "                                   callbacks=[lgb.early_stopping(100), lgb.log_evaluation(period=100)])\n",
    "            elapsed_sec = time.time() - start_time\n",
    "\n",
    "            # -----------------------------\n",
    "            # Predictions & metrics\n",
    "            # -----------------------------\n",
    "            y_pred = best_model.predict(X_val, num_iteration=best_model.best_iteration)\n",
    "            y_val_inv = target_scaler.inverse_transform(y_val.values.reshape(-1,1))\n",
    "            y_pred_inv = target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
    "\n",
    "            global_y_true.append(y_val_inv)\n",
    "            global_y_pred.append(y_pred_inv)\n",
    "\n",
    "            mlflow.log_metric(\"training_time_sec\", elapsed_sec)\n",
    "            mlflow.log_metric(\"MAE\", mean_absolute_error(y_val_inv, y_pred_inv))\n",
    "            mlflow.log_metric(\"RMSE\", np.sqrt(mean_squared_error(y_val_inv, y_pred_inv)))\n",
    "            mlflow.log_metric(\"SMAPE\", smape(y_val_inv, y_pred_inv))\n",
    "            mlflow.log_metric(\"R2_score\", r2_score(y_val_inv, y_pred_inv))\n",
    "            mlflow.log_metric(\"peak_error\", peak_error(y_val_inv, y_pred_inv))\n",
    "\n",
    "            # -----------------------------\n",
    "            # Save model artifact\n",
    "            # -----------------------------\n",
    "            model_path = os.path.join(MODEL_DIR, f\"{junction}_best_final.txt\")\n",
    "            best_model.save_model(model_path)\n",
    "            mlflow.log_artifact(model_path)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Plot predictions\n",
    "            # -----------------------------\n",
    "            plt.figure(figsize=(12,6))\n",
    "            plt.plot(y_val_inv[:100], label='Actual', color='orange')\n",
    "            plt.plot(y_pred_inv[:100], label='Predicted', color='green')\n",
    "            plt.xlabel('Time step')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'{junction} LightGBM Forecast')\n",
    "            plt.legend()\n",
    "            plot_path = os.path.join(MODEL_DIR, f\"{junction}_forecast.png\")\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            mlflow.log_artifact(plot_path)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Log global metrics\n",
    "    # -----------------------------\n",
    "    all_y_true = np.vstack(global_y_true)\n",
    "    all_y_pred = np.vstack(global_y_pred)\n",
    "    global_metrics = {\n",
    "        'MAE': mean_absolute_error(all_y_true, all_y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(all_y_true, all_y_pred)),\n",
    "        'SMAPE': smape(all_y_true, all_y_pred),\n",
    "        'R2_score': r2_score(all_y_true, all_y_pred),\n",
    "        'peak_error': peak_error(all_y_true, all_y_pred)\n",
    "    }\n",
    "    mlflow.log_metrics(global_metrics)\n",
    "\n",
    "    # Save feature/target mapping\n",
    "    with open(f'{MODEL_DIR}/feature_and_target.json', 'w') as f: json.dump(feature_and_target, f, indent=4)\n",
    "\n",
    "print(\"✅ LightGBM training with 30-day block CV, Optuna tuning, last-block holdout, and MLflow logging completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c135d075",
   "metadata": {},
   "source": [
    "LightGBM (zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98910b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Paths and parameters\n",
    "# -----------------------------\n",
    "root = '/mnt/data/home/zayd/Digital_twin_project/machine_learning/dataset/zones_v2/'\n",
    "version = '1.0.1'\n",
    "os.makedirs(f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/LightGBM_{version}', exist_ok=True)\n",
    "mlflow.set_tracking_uri(\"file:model_trained/mlruns\")\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=['zone', 'MAE', 'RMSE', 'MAPE', 'SMAPE', 'R2', 'Peak_Error'])\n",
    "feature_and_target = {}\n",
    "\n",
    "lag_steps = [1,2,3]\n",
    "rolling_windows = [3,6,12]  # moving averages (in timesteps)\n",
    "\n",
    "# lists to store global predictions\n",
    "global_y_true, global_y_pred = [], []\n",
    "\n",
    "with mlflow.start_run(run_name=f\"LightGBM_{version}\"):\n",
    "\n",
    "    for filename in os.listdir(root)[:10]:\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        zone = os.path.splitext(filename)[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Lag features\n",
    "        # -----------------------------\n",
    "        for lag in lag_steps:\n",
    "            df[f'{zone}_lag{lag}'] = df[zone].shift(lag)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Moving average features\n",
    "        # -----------------------------\n",
    "        for w in rolling_windows:\n",
    "            df[f'{zone}_rollmean{w}'] = df[zone].rolling(window=w, min_periods=1).mean()\n",
    "\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        features = [col for col in df.columns if col != zone]\n",
    "        target = zone\n",
    "        feature_and_target[zone] = {\"target\": target, \"features\": features, 'lags':lag_steps ,'rolling':rolling_windows}\n",
    "\n",
    "        # -----------------------------\n",
    "        # Scaling\n",
    "        # -----------------------------\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        target_scaler = MinMaxScaler()\n",
    "        df[features] = feature_scaler.fit_transform(df[features])\n",
    "        df[[target]] = target_scaler.fit_transform(df[[target]])\n",
    "\n",
    "        # Save scalers\n",
    "        os.makedirs(f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/LightGBM_{version}/scalers', exist_ok=True)\n",
    "        with open(f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/LightGBM_{version}/scalers/{zone}_feature_scaler.save', 'wb') as f:\n",
    "            pickle.dump(feature_scaler, f)\n",
    "        with open(f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/LightGBM_{version}/scalers/{zone}_target_scaler.save', 'wb') as f:\n",
    "            pickle.dump(target_scaler, f)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Train/test split by time\n",
    "        # -----------------------------\n",
    "        time_ids = df['time'].unique()\n",
    "        split = int(0.8 * len(time_ids))\n",
    "        train_times = time_ids[:split]\n",
    "        test_times = time_ids[split:]\n",
    "        train_df = df[df['time'].isin(train_times)]\n",
    "        test_df = df[df['time'].isin(test_times)]\n",
    "\n",
    "        X_train, y_train = train_df[features], train_df[target]\n",
    "        X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Train LightGBM\n",
    "        # -----------------------------\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'learning_rate': 0.01,\n",
    "            'num_leaves': 31,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        start_time = time.time()\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=5000,\n",
    "            valid_sets=[train_data, valid_data],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=100),\n",
    "                lgb.log_evaluation(period=100)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # -----------------------------\n",
    "        # Prediction & evaluation\n",
    "        # -----------------------------\n",
    "        y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "        y_test_inv = target_scaler.inverse_transform(y_test.values.reshape(-1,1))\n",
    "        y_pred_inv = target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
    "\n",
    "        global_y_true.append(y_test_inv)\n",
    "        global_y_pred.append(y_pred_inv)\n",
    "\n",
    "        mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "        mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv)\n",
    "        smape_val = smape(y_test_inv, y_pred_inv)\n",
    "        r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "        peak_err = peak_error(y_test_inv, y_pred_inv)\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
    "            'zone': [zone],\n",
    "            'MAE': [mae],\n",
    "            'RMSE': [rmse],\n",
    "            'MAPE': [mape],\n",
    "            'SMAPE': [smape_val],\n",
    "            'R2': [r2],\n",
    "            'Peak_Error': [peak_err]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "        # Save model\n",
    "        model.save_model(f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/LightGBM_{version}/{zone}.txt')\n",
    "        print(metrics_df.iloc[-1])\n",
    "        # Plot predictions\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(test_times[:100], y_test_inv[:100], label='Actual', color='orange')\n",
    "        plt.plot(test_times[:100], y_pred_inv[:100], label='Predicted', color='green')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Water Consumption')\n",
    "        plt.title(f'LightGBM Forecast vs Actual - Zone {zone}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    # -----------------------------\n",
    "    # Global metrics across all zones\n",
    "    # -----------------------------\n",
    "    all_y_true = np.vstack(global_y_true)\n",
    "    all_y_pred = np.vstack(global_y_pred)\n",
    "\n",
    "    global_metrics = {\n",
    "        'MAE': mean_absolute_error(all_y_true, all_y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(all_y_true, all_y_pred)),\n",
    "        'MAPE': mean_absolute_percentage_error(all_y_true, all_y_pred),\n",
    "        'SMAPE': smape(all_y_true, all_y_pred),\n",
    "        'R2': r2_score(all_y_true, all_y_pred),\n",
    "        'Peak_Error': peak_error(all_y_true, all_y_pred)\n",
    "    }\n",
    "\n",
    "    print(\"\\n✅ Global Metrics:\")\n",
    "    for k,v in global_metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    mlflow.log_metrics(global_metrics)\n",
    "\n",
    "# Save feature/target mapping\n",
    "with open(f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/LightGBM_{version}/feature_and_target.json', 'w') as f:\n",
    "    json.dump(feature_and_target, f, indent=4)\n",
    "\n",
    "print(\"✅ LightGBM Zone training completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4549c6",
   "metadata": {},
   "source": [
    "Lightgbm (parameteres hypertuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def smape(y_true, y_pred):\n",
    "    denom = np.abs(y_true)+np.abs(y_pred)\n",
    "    denom = np.where(denom==0, np.finfo(float).eps, denom)\n",
    "    return 100/len(y_true)*np.sum(2*np.abs(y_pred-y_true)/denom)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_val = np.percentile(y_true, percentile)\n",
    "    peak_idx = y_true>=peak_val\n",
    "    if np.sum(peak_idx)==0: return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_idx]-y_pred[peak_idx]))\n",
    "\n",
    "# -----------------------------\n",
    "# Paths & parameters\n",
    "# -----------------------------\n",
    "root = '/mnt/data/home/zayd/Digital_twin_project/machine_learning/dataset/Ctown/junctions/'\n",
    "version = '0.0.5_best'\n",
    "MODEL_DIR = f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/lgbm_model_{version}'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(f'{MODEL_DIR}/scalers', exist_ok=True)\n",
    "\n",
    "lag_steps = [1,2,3]\n",
    "rolling_windows = [3,6,12]\n",
    "\n",
    "feature_and_target = {}\n",
    "mlflow.set_experiment(\"Digital_Twin_Experiments\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"lgbm_model_{version}\"):\n",
    "\n",
    "    for filename in os.listdir(root)[:10]:\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        df = df.sort_values(by=[\"scenario_id\", \"time_id\"]).reset_index(drop=True)\n",
    "        junction_cols = [col for col in df.columns if col.startswith('J')]\n",
    "\n",
    "        # Convert from m³/s to L/s\n",
    "        df[junction_cols] = df[junction_cols] * 1000\n",
    "        junction = os.path.splitext(filename)[0]\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"{junction}\", nested=True):\n",
    "            # Lag & rolling features\n",
    "            for lag in lag_steps:\n",
    "                df[f'{junction}_lag{lag}'] = df[junction].shift(lag)\n",
    "            for w in rolling_windows:\n",
    "                df[f'{junction}_rollmean{w}'] = df[junction].rolling(window=w, min_periods=1).mean()\n",
    "            df = df.fillna(0)\n",
    "\n",
    "            features = [c for c in df.columns if c != junction]\n",
    "            target = junction\n",
    "            feature_and_target[junction] = {\"target\": target, \"features\": features, \"lags\": lag_steps}\n",
    "\n",
    "            # Scaling\n",
    "            feature_scaler = MinMaxScaler().fit(df[features])\n",
    "            target_scaler = MinMaxScaler().fit(df[[target]])\n",
    "            df[features] = feature_scaler.transform(df[features])\n",
    "            df[[target]] = target_scaler.transform(df[[target]])\n",
    "\n",
    "            # Save scalers\n",
    "            for scaler_name, scaler_obj in zip(['feature','target'], [feature_scaler,target_scaler]):\n",
    "                path = f'{MODEL_DIR}/scalers/{junction}_{scaler_name}_scaler.save'\n",
    "                with open(path,'wb') as f: pickle.dump(scaler_obj,f)\n",
    "                mlflow.log_artifact(path, artifact_path=f\"{junction}/scalers\")\n",
    "\n",
    "            X, y = df[features].values, df[target].values\n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "            # LightGBM + hyperparameter tuning\n",
    "            lgbm = lgb.LGBMRegressor(random_state=42)\n",
    "            param_grid = {\n",
    "                'n_estimators': [500, 1000],\n",
    "                'learning_rate': [0.01, 0.05],\n",
    "                'num_leaves': [31, 63],\n",
    "                'max_depth': [-1, 10]\n",
    "            }\n",
    "            gsearch = GridSearchCV(\n",
    "                lgbm, param_grid, cv=tscv,\n",
    "                scoring='neg_mean_squared_error', verbose=0\n",
    "            )\n",
    "            gsearch.fit(X, y)\n",
    "\n",
    "            # Best params & refit\n",
    "            best_params = gsearch.best_params_\n",
    "            best_model = lgb.LGBMRegressor(random_state=42, **best_params)\n",
    "            best_model.fit(X, y)\n",
    "\n",
    "            # Evaluate on last fold\n",
    "            train_idx, test_idx = list(tscv.split(X))[-1]\n",
    "            X_test, y_test = X[train_idx], y[train_idx]  # correction\n",
    "            X_test, y_test = X[test_idx], y[test_idx]\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            y_test_inv = target_scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "            y_pred_inv = target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
    "\n",
    "            mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "            smape_val = smape(y_test_inv, y_pred_inv)\n",
    "            r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "            peak_err = peak_error(y_test_inv, y_pred_inv)\n",
    "\n",
    "            # Log to MLflow\n",
    "            model_path = f\"{MODEL_DIR}/{junction}_{version}.pkl\"\n",
    "            with open(model_path,'wb') as f: pickle.dump(best_model,f)\n",
    "            mlflow.log_artifact(model_path)\n",
    "\n",
    "            mlflow.log_params({\n",
    "                \"algo\": f\"LightGBM_{version}\",\n",
    "                \"junction\": junction,\n",
    "                \"window\": rolling_windows,\n",
    "                \"stride\": lag_steps,\n",
    "                **best_params\n",
    "            })\n",
    "\n",
    "            mlflow.log_metrics({\n",
    "                \"MAE\": mae, \"RMSE\": rmse, \"SMAPE\": smape_val, \"R2\": r2, \"peak_error\": peak_err\n",
    "            })\n",
    "\n",
    "    # Save feature/target mapping\n",
    "    feature_target_path = f'{MODEL_DIR}/feature_and_target.json'\n",
    "    with open(feature_target_path,'w') as f:\n",
    "        json.dump(feature_and_target, f, indent=4)\n",
    "    mlflow.log_artifact(feature_target_path)\n",
    "\n",
    "print(\"✅ Best LightGBM model per junction logged with MLflow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1e2c9",
   "metadata": {},
   "source": [
    "XGBoost (single junction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22113fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J436 training time: 27.38 sec\n",
      "J27 training time: 26.27 sec\n",
      "J101 training time: 16.52 sec\n",
      "J309 training time: 15.01 sec\n",
      "J280 training time: 22.81 sec\n",
      "J83 training time: 19.14 sec\n",
      "J90 training time: 20.03 sec\n",
      "J303 training time: 26.14 sec\n",
      "J227 training time: 17.15 sec\n",
      "J498 training time: 15.20 sec\n",
      "✅ XGBoost training and MLflow logging completed\n"
     ]
    }
   ],
   "source": [
    "import os, time, pickle, json, mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# -----------------------------\n",
    "# Custom metrics\n",
    "# -----------------------------\n",
    "def smape(y_true, y_pred):\n",
    "    denom = np.abs(y_true) + np.abs(y_pred)\n",
    "    denom = np.where(denom == 0, np.finfo(float).eps, denom)\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true)/denom)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_val = np.percentile(y_true, percentile)\n",
    "    peak_idx = y_true >= peak_val\n",
    "    if np.sum(peak_idx) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_idx] - y_pred[peak_idx]))\n",
    "\n",
    "# -----------------------------\n",
    "# Paths and parameters\n",
    "# -----------------------------\n",
    "root = '/mnt/data/home/zayd/Digital_twin_project/machine_learning/dataset/Ctown/junctions/'\n",
    "version = '0.1.0'\n",
    "MODEL_DIR = f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/XGBoost_{version}'\n",
    "os.makedirs(f'{MODEL_DIR}/scalers', exist_ok=True)\n",
    "\n",
    "lag_steps = [1,2]\n",
    "rolling_windows = [4,12,24]\n",
    "\n",
    "mlflow.set_experiment(\"Digital_Twin_Experiments\")  \n",
    "\n",
    "global_y_true, global_y_pred = [], []\n",
    "feature_and_target = {}\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=f\"XGBoost_{version}\"):\n",
    "    for filename in os.listdir(root)[:10]:\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        df = df.sort_values(by=[\"scenario_id\", \"time_id\"]).reset_index(drop=True)\n",
    "        junction_cols = [col for col in df.columns if col.startswith('J')]\n",
    "\n",
    "        # Convert from m³/s to L/s\n",
    "        df[junction_cols] = df[junction_cols] * 1000\n",
    "        junction = os.path.splitext(filename)[0]\n",
    "        with mlflow.start_run(run_name=f\"{junction}\",nested=True):\n",
    "\n",
    "            # Create lag features\n",
    "            for lag in lag_steps:\n",
    "                df[f'{junction}_lag{lag}'] = df[junction].shift(lag)\n",
    "            # Create moving average features\n",
    "            for w in rolling_windows:\n",
    "                df[f'{junction}_rollmean{w}'] = df[junction].rolling(window=w, min_periods=1).mean()\n",
    "            df = df.fillna(0)\n",
    "\n",
    "            features = [c for c in df.columns if c != junction]\n",
    "            target = junction\n",
    "            feature_and_target[junction] = {\"target\": target, \"features\": features, 'lags': lag_steps}\n",
    "\n",
    "            # Scaling\n",
    "            feature_scaler = MinMaxScaler().fit(df[features])\n",
    "            target_scaler = MinMaxScaler().fit(df[[target]])\n",
    "            df[features] = feature_scaler.transform(df[features])\n",
    "            df[[target]] = target_scaler.transform(df[[target]])\n",
    "\n",
    "            # Save scalers\n",
    "            with open(f'{MODEL_DIR}/scalers/{junction}_feature_scaler.save', 'wb') as f:\n",
    "                pickle.dump(feature_scaler, f)\n",
    "            with open(f'{MODEL_DIR}/scalers/{junction}_target_scaler.save', 'wb') as f:\n",
    "                pickle.dump(target_scaler, f)\n",
    "\n",
    "            # Train/test split\n",
    "            scenario_ids = df['scenario_id'].unique()\n",
    "            split = int(0.8 * len(scenario_ids))\n",
    "            train_df = df[df['scenario_id'].isin(scenario_ids[:split])]\n",
    "            test_df = df[df['scenario_id'].isin(scenario_ids[split:])]\n",
    "\n",
    "            X_train, y_train = train_df[features], train_df[target]\n",
    "            X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "            # Train XGBoost\n",
    "            model = xgb.XGBRegressor(\n",
    "                n_estimators=5000, learning_rate=0.01, max_depth=6,\n",
    "                subsample=0.8, colsample_bytree=0.8, objective='reg:squarederror',\n",
    "                verbosity=1\n",
    "            )\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "            elapsed_sec = time.time() - start_time\n",
    "            print(f\"{junction} training time: {elapsed_sec:.2f} sec\")\n",
    "\n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_test_inv = target_scaler.inverse_transform(y_test.values.reshape(-1,1))\n",
    "            y_pred_inv = target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
    "            global_y_true.append(y_test_inv)\n",
    "            global_y_pred.append(y_pred_inv)\n",
    "\n",
    "            # -----------------------------\n",
    "            # MLflow logging\n",
    "            # -----------------------------\n",
    "            mlflow.log_params({\n",
    "                \"algo\": f\"XGBoost_{version}\",\n",
    "                \"junction\": junction,\n",
    "                \"window\": rolling_windows,\n",
    "                \"stride\": lag_steps,\n",
    "                \"n_estimators\": 5000,\n",
    "                \"learning_rate\": 0.01,\n",
    "                \"max_depth\": 6,\n",
    "                \"subsample\": 0.8,\n",
    "                \"colsample_bytree\": 0.8\n",
    "            })\n",
    "            mlflow.log_metric(\"RMSE\", np.sqrt(mean_squared_error(y_test_inv, y_pred_inv)))  # RMSE\n",
    "            mlflow.log_metric(\"SMAPE\", smape(y_test_inv, y_pred_inv))\n",
    "            mlflow.log_metric(\"R2_score\", r2_score(y_test_inv, y_pred_inv))\n",
    "            mlflow.log_metric(\"peak_error\", peak_error(y_test_inv, y_pred_inv))\n",
    "\n",
    "            # Save model\n",
    "            save_path = os.path.join(MODEL_DIR, f\"{junction}.json\")\n",
    "            model.save_model(save_path)\n",
    "            mlflow.log_artifact(save_path)\n",
    "\n",
    "# Save feature/target mapping\n",
    "with open(f'{MODEL_DIR}/feature_and_target.json', 'w') as f:\n",
    "    json.dump(feature_and_target, f, indent=4)\n",
    "\n",
    "print(\"✅ XGBoost training and MLflow logging completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894e24a0",
   "metadata": {},
   "source": [
    "z score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b42651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J436 training time: 18.24 sec\n",
      "J27 training time: 17.56 sec\n",
      "J101 training time: 17.64 sec\n",
      "J309 training time: 16.81 sec\n",
      "J280 training time: 17.11 sec\n",
      "J83 training time: 19.58 sec\n",
      "J90 training time: 17.89 sec\n",
      "J303 training time: 19.10 sec\n",
      "J227 training time: 22.62 sec\n",
      "J498 training time: 22.14 sec\n",
      "✅ XGBoost training and MLflow logging completed\n"
     ]
    }
   ],
   "source": [
    "import os, time, pickle, json, mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# -----------------------------\n",
    "# Custom metrics\n",
    "# -----------------------------\n",
    "def smape(y_true, y_pred):\n",
    "    denom = np.abs(y_true) + np.abs(y_pred)\n",
    "    denom = np.where(denom == 0, np.finfo(float).eps, denom)\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true)/denom)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_val = np.percentile(y_true, percentile)\n",
    "    peak_idx = y_true >= peak_val\n",
    "    if np.sum(peak_idx) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_idx] - y_pred[peak_idx]))\n",
    "\n",
    "# -----------------------------\n",
    "# Paths and parameters\n",
    "# -----------------------------\n",
    "root = '/mnt/data/home/zayd/Digital_twin_project/machine_learning/dataset/Ctown/junctions/'\n",
    "version = '0.1.0_zscore'\n",
    "MODEL_DIR = f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/XGBoost_{version}'\n",
    "os.makedirs(f'{MODEL_DIR}/scalers', exist_ok=True)\n",
    "\n",
    "lag_steps = [1,2]\n",
    "rolling_windows = [4,12,24]\n",
    "\n",
    "mlflow.set_experiment(\"Digital_Twin_Experiments\")  \n",
    "\n",
    "global_y_true, global_y_pred = [], []\n",
    "feature_and_target = {}\n",
    "\n",
    "with mlflow.start_run(run_name=f\"XGBoost_{version}\"):\n",
    "    for filename in os.listdir(root)[:10]:\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        df = df.sort_values(by=[\"scenario_id\", \"time_id\"]).reset_index(drop=True)\n",
    "        junction_cols = [col for col in df.columns if col.startswith('J')]\n",
    "\n",
    "        # Convert from m³/s to L/s\n",
    "        df[junction_cols] = df[junction_cols] * 1000\n",
    "        junction = os.path.splitext(filename)[0]\n",
    "        with mlflow.start_run(run_name=f\"{junction}\",nested=True):\n",
    "\n",
    "            # Create lag features\n",
    "            for lag in lag_steps:\n",
    "                df[f'{junction}_lag{lag}'] = df[junction].shift(lag)\n",
    "            # Create moving average features\n",
    "            for w in rolling_windows:\n",
    "                df[f'{junction}_rollmean{w}'] = df[junction].rolling(window=w, min_periods=1).mean()\n",
    "            df = df.fillna(0)\n",
    "\n",
    "            features = [c for c in df.columns if c != junction]\n",
    "            target = junction\n",
    "            feature_and_target[junction] = {\"target\": target, \"features\": features, 'lags': lag_steps}\n",
    "\n",
    "            # -----------------------------\n",
    "            # Z-score scaling\n",
    "            # -----------------------------\n",
    "            feature_scaler = StandardScaler().fit(df[features])\n",
    "            target_scaler = StandardScaler().fit(df[[target]])\n",
    "            df[features] = feature_scaler.transform(df[features])\n",
    "            df[[target]] = target_scaler.transform(df[[target]])\n",
    "\n",
    "            # Save scalers\n",
    "            with open(f'{MODEL_DIR}/scalers/{junction}_feature_scaler.save', 'wb') as f:\n",
    "                pickle.dump(feature_scaler, f)\n",
    "            with open(f'{MODEL_DIR}/scalers/{junction}_target_scaler.save', 'wb') as f:\n",
    "                pickle.dump(target_scaler, f)\n",
    "\n",
    "            # Train/test split\n",
    "            scenario_ids = df['scenario_id'].unique()\n",
    "            split = int(0.8 * len(scenario_ids))\n",
    "            train_df = df[df['scenario_id'].isin(scenario_ids[:split])]\n",
    "            test_df = df[df['scenario_id'].isin(scenario_ids[split:])]\n",
    "\n",
    "            X_train, y_train = train_df[features], train_df[target]\n",
    "            X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "            # Train XGBoost\n",
    "            model = xgb.XGBRegressor(\n",
    "                n_estimators=5000, learning_rate=0.01, max_depth=6,\n",
    "                subsample=0.8, colsample_bytree=0.8, objective='reg:squarederror',\n",
    "                verbosity=1\n",
    "            )\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "            elapsed_sec = time.time() - start_time\n",
    "            print(f\"{junction} training time: {elapsed_sec:.2f} sec\")\n",
    "\n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_test_inv = target_scaler.inverse_transform(y_test.values.reshape(-1,1))\n",
    "            y_pred_inv = target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
    "            global_y_true.append(y_test_inv)\n",
    "            global_y_pred.append(y_pred_inv)\n",
    "\n",
    "            # -----------------------------\n",
    "            # MLflow logging\n",
    "            # -----------------------------\n",
    "            mlflow.log_params({\n",
    "                \"algo\": f\"XGBoost_{version}\",\n",
    "                \"junction\": junction,\n",
    "                \"window\": rolling_windows,\n",
    "                \"stride\": lag_steps,\n",
    "                \"n_estimators\": 5000,\n",
    "                \"learning_rate\": 0.01,\n",
    "                \"max_depth\": 6,\n",
    "                \"subsample\": 0.8,\n",
    "                \"colsample_bytree\": 0.8\n",
    "            })\n",
    "            mlflow.log_metric(\"MAE\", mean_absolute_error(y_test_inv, y_pred_inv))\n",
    "            mlflow.log_metric(\"RMSE\", np.sqrt(mean_squared_error(y_test_inv, y_pred_inv)))\n",
    "            mlflow.log_metric(\"SMAPE\", smape(y_test_inv, y_pred_inv))\n",
    "            mlflow.log_metric(\"R2_score\", r2_score(y_test_inv, y_pred_inv))\n",
    "            mlflow.log_metric(\"peak_error\", peak_error(y_test_inv, y_pred_inv))\n",
    "\n",
    "            # Save model\n",
    "            save_path = os.path.join(MODEL_DIR, f\"{junction}.json\")\n",
    "            model.save_model(save_path)\n",
    "            mlflow.log_artifact(save_path)\n",
    "\n",
    "# Save feature/target mapping\n",
    "with open(f'{MODEL_DIR}/feature_and_target.json', 'w') as f:\n",
    "    json.dump(feature_and_target, f, indent=4)\n",
    "\n",
    "print(\"✅ XGBoost training and MLflow logging completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582aa89e",
   "metadata": {},
   "source": [
    "XGBoost (zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70915b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import mlflow\n",
    "import json\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.where(y_true == 0, np.finfo(float).eps, y_true)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    denominator = np.where(denominator == 0, np.finfo(float).eps, denominator)\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / denominator)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_value = np.percentile(y_true, percentile)\n",
    "    peak_indices = y_true >= peak_value\n",
    "    if np.sum(peak_indices) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_indices] - y_pred[peak_indices]))\n",
    "\n",
    "# -----------------------------\n",
    "# Paths and parameters\n",
    "# -----------------------------\n",
    "root = '/mnt/data/home/zayd/Digital_twin_project/machine_learning/dataset/zones_v2/'\n",
    "version = '0.0.5_zscore---'\n",
    "os.makedirs(f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/XGBoost_{version}', exist_ok=True)\n",
    "mlflow.set_tracking_uri(\"file:model_trained/mlruns\")\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=['zone', 'MAE', 'RMSE', 'MAPE', 'SMAPE', 'R2', 'Peak_Error'])\n",
    "feature_and_target = {}\n",
    "\n",
    "lag_steps = [1,2]\n",
    "rolling_windows = [4,12,24]\n",
    "\n",
    "global_y_true, global_y_pred = [], []\n",
    "\n",
    "with mlflow.start_run(run_name=f\"XGBoost_{version}\"):\n",
    "\n",
    "    for filename in os.listdir(root)[:1]:\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        zone_name = os.path.splitext(filename)[0]\n",
    "        print(f\"Processing zone: {zone_name}\")\n",
    "\n",
    "        junction_cols = [col for col in df.columns if 'demand' in col]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Create lag and rolling features\n",
    "        # -----------------------------\n",
    "        for lag in lag_steps:\n",
    "            for junc in junction_cols:\n",
    "                df[f'{junc}_lag{lag}'] = df[junc].shift(lag)\n",
    "\n",
    "        for w in rolling_windows:\n",
    "            for junc in junction_cols:\n",
    "                df[f'{junc}_rollmean{w}'] = df[junc].rolling(window=w, min_periods=1).mean()\n",
    "\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Z-score scaling\n",
    "        # -----------------------------\n",
    "        numeric_cols = [c for c in df.columns if c not in ['time_id','scenario_id']]\n",
    "        scalers = {}\n",
    "        for col in numeric_cols:\n",
    "            scaler = StandardScaler()\n",
    "            df[[col]] = scaler.fit_transform(df[[col]])\n",
    "            scalers[col] = scaler\n",
    "\n",
    "        # Save scalers\n",
    "        os.makedirs(f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/XGBoost_{version}/scalers/{zone_name}', exist_ok=True)\n",
    "        for col, scaler in scalers.items():\n",
    "            with open(f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/XGBoost_{version}/scalers/{zone_name}/{col}_scaler.save','wb') as f:\n",
    "                pickle.dump(scaler,f)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Train/test split by day\n",
    "        # -----------------------------\n",
    "        unique_days = df['day'].unique()\n",
    "        split = int(0.8 * len(unique_days))\n",
    "        train_days = unique_days[:split]\n",
    "        test_days = unique_days[split:]\n",
    "\n",
    "        train_df = df[df['day'].isin(train_days)]\n",
    "        test_df = df[df['day'].isin(test_days)]\n",
    "\n",
    "        for target in junction_cols:\n",
    "            features = [c for c in numeric_cols if c != target]\n",
    "\n",
    "            X_train, y_train = train_df[features], train_df[target]\n",
    "            X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "            # -----------------------------\n",
    "            # Train XGBoost\n",
    "            # -----------------------------\n",
    "            model = xgb.XGBRegressor(\n",
    "                n_estimators=5000,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=6,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                objective='reg:squarederror',\n",
    "                verbosity=0\n",
    "            )\n",
    "\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=100)\n",
    "            elapsed_sec = time.time() - start_time\n",
    "            print(f\"{target} training time: {elapsed_sec:.2f} sec\")\n",
    "\n",
    "            # -----------------------------\n",
    "            # Predict & inverse scale target\n",
    "            # -----------------------------\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_test_inv = scalers[target].inverse_transform(y_test.values.reshape(-1,1))\n",
    "            y_pred_inv = scalers[target].inverse_transform(y_pred.reshape(-1,1))\n",
    "\n",
    "            global_y_true.append(y_test_inv)\n",
    "            global_y_pred.append(y_pred_inv)\n",
    "\n",
    "            # Metrics\n",
    "            mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "            mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv)\n",
    "            smape_val = smape(y_test_inv, y_pred_inv)\n",
    "            r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "            peak_err = peak_error(y_test_inv, y_pred_inv)\n",
    "\n",
    "            metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
    "                'zone':[f\"{zone_name}__{target}\"],\n",
    "                'MAE':[mae],\n",
    "                'RMSE':[rmse],\n",
    "                'MAPE':[mape],\n",
    "                'SMAPE':[smape_val],\n",
    "                'R2':[r2],\n",
    "                'Peak_Error':[peak_err]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "            # Save model\n",
    "            os.makedirs(f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/XGBoost_{version}/{zone_name}', exist_ok=True)\n",
    "            model.save_model(f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/XGBoost_{version}/{zone_name}/{target}.json')\n",
    "\n",
    "            # MLflow logs\n",
    "            mlflow.log_metric(f\"{target}_MAE\", mae)\n",
    "            mlflow.log_metric(f\"{target}_RMSE\", rmse)\n",
    "            mlflow.log_metric(f\"{target}_MAPE\", mape)\n",
    "            mlflow.log_metric(f\"{target}_SMAPE\", smape_val)\n",
    "            mlflow.log_metric(f\"{target}_R2\", r2)\n",
    "            mlflow.log_metric(f\"{target}_Peak_Error\", peak_err)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Plot predictions (fixed length issue)\n",
    "            # -----------------------------\n",
    "            plt.figure(figsize=(14, 7))\n",
    "            plt.plot(test_df.index[:100], y_test_inv[:100], label='Actual', color='orange')\n",
    "            plt.plot(test_df.index[:100], y_pred_inv[:100], label='Predicted', color='green')\n",
    "            plt.xlabel('Row index')\n",
    "            plt.ylabel('Water Consumption')\n",
    "            plt.title(f'{zone_name} | {target} Forecast')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Global metrics across all junctions\n",
    "# -----------------------------\n",
    "all_y_true = np.vstack(global_y_true)\n",
    "all_y_pred = np.vstack(global_y_pred)\n",
    "\n",
    "global_metrics = {\n",
    "    'MAE': mean_absolute_error(all_y_true, all_y_pred),\n",
    "    'RMSE': np.sqrt(mean_squared_error(all_y_true, all_y_pred)),\n",
    "    'MAPE': mean_absolute_percentage_error(all_y_true, all_y_pred),\n",
    "    'SMAPE': smape(all_y_true, all_y_pred),\n",
    "    'R2': r2_score(all_y_true, all_y_pred),\n",
    "    'Peak_Error': peak_error(all_y_true, all_y_pred)\n",
    "}\n",
    "\n",
    "print(\"\\n✅ Global Metrics:\")\n",
    "for k,v in global_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "mlflow.log_metrics(global_metrics)\n",
    "\n",
    "# Save feature/target mapping\n",
    "with open(f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/XGBoost_{version}/feature_and_target.json','w') as f:\n",
    "    json.dump(feature_and_target, f, indent=4)\n",
    "\n",
    "print(\"✅ XGBoost training for zones completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b5ec3b",
   "metadata": {},
   "source": [
    "stacking ensemble (single junction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "818c4c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/.config/matplotlib is not a writable directory\n",
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-9uyrbmyf because there was an issue with the default path (/home/zayd/.config/matplotlib); it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing junction: J436 ===\n",
      "⏳ Training stacking model for junction J436...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:44:00] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007786 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.275829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:44:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:44:36] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:44:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:45:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:45:20] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004559 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.277946\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001776 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.275623\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001712 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.272806\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.276270\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.276500\n",
      "✅ Finished training J436 in 129.47 sec\n",
      "\n",
      "=== Processing junction: J27 ===\n",
      "⏳ Training stacking model for junction J27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:46:10] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2829\n",
      "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.260135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:46:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:46:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:46:52] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:47:07] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:47:33] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2829\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.263080\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2829\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.259387\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2829\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.257349\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2829\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.260972\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2829\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.259884\n",
      "✅ Finished training J27 in 112.65 sec\n",
      "\n",
      "=== Processing junction: J101 ===\n",
      "⏳ Training stacking model for junction J101...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:48:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002110 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.302005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:48:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:48:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:48:27] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:48:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:48:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.302497\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.301896\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000618 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.299848\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.302440\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.303347\n",
      "✅ Finished training J101 in 61.35 sec\n",
      "\n",
      "=== Processing junction: J309 ===\n",
      "⏳ Training stacking model for junction J309...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:49:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2319\n",
      "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.265820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:49:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:49:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:49:28] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:49:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:49:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2319\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.267119\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2319\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.264915\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011408 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2319\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.264519\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2319\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.266550\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2319\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.265997\n",
      "✅ Finished training J309 in 67.87 sec\n",
      "\n",
      "=== Processing junction: J280 ===\n",
      "⏳ Training stacking model for junction J280...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:50:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005867 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.281857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:50:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:50:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:50:44] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:50:48] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:50:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.283291\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.280926\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.280684\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.282844\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.281538\n",
      "✅ Finished training J280 in 58.55 sec\n",
      "\n",
      "=== Processing junction: J83 ===\n",
      "⏳ Training stacking model for junction J83...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:51:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004026 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3084\n",
      "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.293839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:51:24] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:51:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:51:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:51:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:51:53] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3084\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.296106\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003737 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3084\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.291976\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3084\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.291806\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3084\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.294609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3084\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.294700\n",
      "✅ Finished training J83 in 59.18 sec\n",
      "\n",
      "=== Processing junction: J90 ===\n",
      "⏳ Training stacking model for junction J90...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:52:14] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.272839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:52:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:52:26] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:52:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:52:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:52:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.275721\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.272305\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.270355\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.274212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.271602\n",
      "✅ Finished training J90 in 41.78 sec\n",
      "\n",
      "=== Processing junction: J303 ===\n",
      "⏳ Training stacking model for junction J303...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:52:56] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.313765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:53:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:53:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:53:12] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:53:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:53:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001807 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.314430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.313462\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.312387\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.314765\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.313781\n",
      "✅ Finished training J303 in 42.43 sec\n",
      "\n",
      "=== Processing junction: J227 ===\n",
      "⏳ Training stacking model for junction J227...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:53:39] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2829\n",
      "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.293702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:53:47] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:53:51] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:53:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:53:59] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:54:04] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2829\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.295464\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2829\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.293069\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2829\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.290455\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2829\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.294926\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2829\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 0.294593\n",
      "✅ Finished training J227 in 42.18 sec\n",
      "\n",
      "=== Processing junction: J498 ===\n",
      "⏳ Training stacking model for junction J498...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:54:22] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.325759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:54:29] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:54:34] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:54:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:54:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [12:54:46] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.326460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.324822\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.323558\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.327286\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2574\n",
      "[LightGBM] [Info] Number of data points in the train set: 15360, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 0.326668\n",
      "✅ Finished training J498 in 41.52 sec\n",
      "✅ Stacking ensemble training completed with MLflow logging\n"
     ]
    }
   ],
   "source": [
    "import os, time, pickle, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = np.abs(y_true)+np.abs(y_pred)\n",
    "    denominator = np.where(denominator==0, np.finfo(float).eps, denominator)\n",
    "    return 100/len(y_true)*np.sum(2*np.abs(y_pred-y_true)/denominator)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_val = np.percentile(y_true, percentile)\n",
    "    peak_idx = y_true>=peak_val\n",
    "    if np.sum(peak_idx)==0: return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_idx]-y_pred[peak_idx]))\n",
    "\n",
    "# -----------------------------\n",
    "# Paths & parameters\n",
    "# -----------------------------\n",
    "root = '/mnt/data/home/zayd/Digital_twin_project/machine_learning/dataset/Ctown/junctions/'\n",
    "version = '0.1.0'\n",
    "MODEL_DIR = f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/stacking_model_{version}'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(f'{MODEL_DIR}/scalers', exist_ok=True)\n",
    "\n",
    "lag_steps = [1,2,3]\n",
    "rolling_windows = [3,6,12]\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=['junction','MAE','RMSE','SMAPE','R2','Peak_Error'])\n",
    "feature_and_target = {}\n",
    "global_y_true, global_y_pred = [], []\n",
    "\n",
    "mlflow.set_experiment(\"Digital_Twin_Experiments\")\n",
    "\n",
    "# -----------------------------\n",
    "# Parent MLflow run\n",
    "# -----------------------------\n",
    "with mlflow.start_run(run_name=f\"stacking_model_{version}\"):\n",
    "\n",
    "    for filename in os.listdir(root)[:10]:\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        df = df.sort_values(by=[\"scenario_id\", \"time_id\"]).reset_index(drop=True)\n",
    "        junction_cols = [col for col in df.columns if col.startswith('J')]\n",
    "\n",
    "        # Convert from m³/s to L/s\n",
    "        df[junction_cols] = df[junction_cols] * 1000\n",
    "        junction = os.path.splitext(filename)[0]\n",
    "        print(f\"\\n=== Processing junction: {junction} ===\")\n",
    "        \n",
    "\n",
    "        with mlflow.start_run(run_name=f\"{junction}\", nested=True):\n",
    "\n",
    "            # -----------------------------\n",
    "            # Lag & rolling features\n",
    "            # -----------------------------\n",
    "            for lag in lag_steps:\n",
    "                df[f'{junction}_lag{lag}'] = df[junction].shift(lag)\n",
    "            for w in rolling_windows:\n",
    "                df[f'{junction}_rollmean{w}'] = df[junction].rolling(window=w, min_periods=1).mean()\n",
    "            df = df.fillna(0)\n",
    "\n",
    "            features = [c for c in df.columns if c!=junction]\n",
    "            target = junction\n",
    "            feature_and_target[junction] = {\"target\":target, \"features\":features, \"lags\":lag_steps}\n",
    "\n",
    "            # -----------------------------\n",
    "            # Scaling\n",
    "            # -----------------------------\n",
    "            feature_scaler = MinMaxScaler().fit(df[features])\n",
    "            target_scaler = MinMaxScaler().fit(df[[target]])\n",
    "            df[features] = feature_scaler.transform(df[features])\n",
    "            df[[target]] = target_scaler.transform(df[[target]])\n",
    "\n",
    "            # Save scalers\n",
    "            with open(f'{MODEL_DIR}/scalers/{junction}_feature_scaler.save','wb') as f: pickle.dump(feature_scaler,f)\n",
    "            with open(f'{MODEL_DIR}/scalers/{junction}_target_scaler.save','wb') as f: pickle.dump(target_scaler,f)\n",
    "            mlflow.log_artifact(f'{MODEL_DIR}/scalers/{junction}_feature_scaler.save', artifact_path=f\"{junction}/scalers\")\n",
    "            mlflow.log_artifact(f'{MODEL_DIR}/scalers/{junction}_target_scaler.save', artifact_path=f\"{junction}/scalers\")\n",
    "\n",
    "            # -----------------------------\n",
    "            # Train/test split by time\n",
    "            # -----------------------------\n",
    "            scenario_ids = df['scenario_id'].unique()\n",
    "            split = int(0.8 * len(scenario_ids))\n",
    "            train_scenarios = scenario_ids[:split]\n",
    "            test_scenarios = scenario_ids[split:]\n",
    "\n",
    "            train_df = df[df['scenario_id'].isin(train_scenarios)]\n",
    "            test_df = df[df['scenario_id'].isin(test_scenarios)]\n",
    "\n",
    "            # Extract X and y\n",
    "            X_train = train_df[features]\n",
    "            y_train = train_df[target]\n",
    "            X_test = test_df[features]\n",
    "            y_test = test_df[target]\n",
    "\n",
    "            # -----------------------------\n",
    "            # Stacking Regressor\n",
    "            # -----------------------------\n",
    "            estimators = [\n",
    "                ('xgb', xgb.XGBRegressor(\n",
    "                    n_estimators=1000, learning_rate=0.01, max_depth=6, random_state=42, verbose=1\n",
    "                )),\n",
    "                ('lgbm', lgb.LGBMRegressor(\n",
    "                    n_estimators=1000, learning_rate=0.01, num_leaves=31, random_state=42, verbose=1\n",
    "                ))\n",
    "            ]\n",
    "            stacking_model = StackingRegressor(estimators=estimators, final_estimator=Ridge(), cv=5)\n",
    "\n",
    "            print(f\"⏳ Training stacking model for junction {junction}...\")\n",
    "            start_time = time.time()\n",
    "            stacking_model.fit(X_train, y_train)\n",
    "            elapsed_sec = time.time() - start_time\n",
    "            print(f\"✅ Finished training {junction} in {elapsed_sec:.2f} sec\")\n",
    "            mlflow.log_param(f\"{junction}_training_time_sec\", elapsed_sec)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Predictions & metrics\n",
    "            # -----------------------------\n",
    "            y_pred = stacking_model.predict(X_test)\n",
    "            y_test_inv = target_scaler.inverse_transform(y_test.values.reshape(-1,1))\n",
    "            y_pred_inv = target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
    "            global_y_true.append(y_test_inv)\n",
    "            global_y_pred.append(y_pred_inv)\n",
    "\n",
    "            mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "            smape_val = smape(y_test_inv, y_pred_inv)\n",
    "            r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "            peak_err = peak_error(y_test_inv, y_pred_inv)\n",
    "\n",
    "            mlflow.log_params({\n",
    "                \"algo\": f\"Stacking_ensemble_{version}\",\n",
    "                \"junction\": junction,\n",
    "                \"window\": rolling_windows,\n",
    "                \"stride\": lag_steps,\n",
    "                \"n_estimators_xgb\": 1000,\n",
    "                \"learning_rate_xgb\": 0.01,\n",
    "                \"max_depth_xgb\": 6,\n",
    "                \"subsample_xgb\": 0.8,\n",
    "                \"colsample_bytree_xgb\": 0.8,\n",
    "                \"n_estimators_lgbm\": 1000,\n",
    "                \"learning_rate_lgbm\": 0.01,\n",
    "                \"num_leaves_lgbm\": 31,\n",
    "                \"training_time_sec\": elapsed_sec\n",
    "            })\n",
    "\n",
    "            mlflow.log_metrics({\n",
    "                'MAE':mae,'RMSE':rmse,'SMAPE':smape_val,'R2_score':r2,'peak_error':peak_err\n",
    "            })\n",
    "\n",
    "            # Save model\n",
    "            model_path = f\"{MODEL_DIR}/{junction}.pkl\"\n",
    "            with open(model_path,'wb') as f: pickle.dump(stacking_model,f)\n",
    "            mlflow.log_artifact(model_path)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Global metrics\n",
    "    # -----------------------------\n",
    "    all_y_true = np.vstack(global_y_true)\n",
    "    all_y_pred = np.vstack(global_y_pred)\n",
    "    global_metrics = {\n",
    "        'MAE': mean_absolute_error(all_y_true, all_y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(all_y_true, all_y_pred)),\n",
    "        'SMAPE': smape(all_y_true, all_y_pred),\n",
    "        'R2_score': r2_score(all_y_true, all_y_pred),\n",
    "        'peak_error': peak_error(all_y_true, all_y_pred)\n",
    "    }\n",
    "    mlflow.log_metrics(global_metrics)\n",
    "\n",
    "    # Save feature/target mapping\n",
    "    with open(f'{MODEL_DIR}/feature_and_target.json','w') as f: json.dump(feature_and_target,f,indent=4)\n",
    "    mlflow.log_artifact(f'{MODEL_DIR}/feature_and_target.json')\n",
    "\n",
    "print(\"✅ Stacking ensemble training completed with MLflow logging\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c6e4d",
   "metadata": {},
   "source": [
    "stacking ensemble (Cross validation + hyperparametere_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ff45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def smape(y_true, y_pred):\n",
    "    denom = np.abs(y_true)+np.abs(y_pred)\n",
    "    denom = np.where(denom==0, np.finfo(float).eps, denom)\n",
    "    return 100/len(y_true)*np.sum(2*np.abs(y_pred-y_true)/denom)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_val = np.percentile(y_true, percentile)\n",
    "    peak_idx = y_true>=peak_val\n",
    "    if np.sum(peak_idx)==0: return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_idx]-y_pred[peak_idx]))\n",
    "\n",
    "# -----------------------------\n",
    "# Paths & parameters\n",
    "# -----------------------------\n",
    "root = '/mnt/data/home/zayd/Digital_twin_project/machine_learning/dataset/Ctown/junctions/'\n",
    "version = '0.0.5_best'\n",
    "MODEL_DIR = f'/mnt/data/home/zayd/Digital_twin_project/machine_learning/model_trained/stacking_model_{version}'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(f'{MODEL_DIR}/scalers', exist_ok=True)\n",
    "\n",
    "lag_steps = [1,2,3]\n",
    "rolling_windows = [3,6,12]\n",
    "\n",
    "feature_and_target = {}\n",
    "mlflow.set_experiment(\"Digital_Twin_Experiments\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"stacking_model_{version}\"):\n",
    "\n",
    "    for filename in os.listdir(root):\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        df = df.sort_values(by=[\"scenario_id\", \"time_id\"]).reset_index(drop=True)\n",
    "        junction_cols = [col for col in df.columns if col.startswith('J')]\n",
    "\n",
    "        # Convert from m³/s to L/s\n",
    "        df[junction_cols] = df[junction_cols] * 1000\n",
    "        junction = os.path.splitext(filename)[0]\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"{junction}\", nested=True):\n",
    "            # Lag & rolling features\n",
    "            for lag in lag_steps:\n",
    "                df[f'{junction}_lag{lag}'] = df[junction].shift(lag)\n",
    "            for w in rolling_windows:\n",
    "                df[f'{junction}_rollmean{w}'] = df[junction].rolling(window=w, min_periods=1).mean()\n",
    "            df = df.fillna(0)\n",
    "\n",
    "            features = [c for c in df.columns if c != junction]\n",
    "            target = junction\n",
    "            feature_and_target[junction] = {\"target\": target, \"features\": features, \"lags\": lag_steps}\n",
    "\n",
    "            # Scaling\n",
    "            feature_scaler = MinMaxScaler().fit(df[features])\n",
    "            target_scaler = MinMaxScaler().fit(df[[target]])\n",
    "            df[features] = feature_scaler.transform(df[features])\n",
    "            df[[target]] = target_scaler.transform(df[[target]])\n",
    "\n",
    "            # Save scalers\n",
    "            for scaler_name, scaler_obj in zip(['feature','target'], [feature_scaler,target_scaler]):\n",
    "                path = f'{MODEL_DIR}/scalers/{junction}_{scaler_name}_scaler.save'\n",
    "                with open(path,'wb') as f: pickle.dump(scaler_obj,f)\n",
    "                mlflow.log_artifact(path, artifact_path=f\"{junction}/scalers\")\n",
    "\n",
    "            X, y = df[features].values, df[target].values\n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "            # Stacking + hyperparameter tuning\n",
    "            estimators = [\n",
    "                ('xgb', xgb.XGBRegressor(random_state=42, verbosity=1)), \n",
    "                ('lgbm', lgb.LGBMRegressor(random_state=42))\n",
    "            ]\n",
    "            stacking_model = StackingRegressor(estimators=estimators, final_estimator=Ridge(), cv=5)\n",
    "            param_grid = {\n",
    "                'xgb__n_estimators': [500, 1000],\n",
    "                'xgb__learning_rate': [0.01, 0.05],\n",
    "                'lgbm__n_estimators': [500, 1000],\n",
    "                'lgbm__learning_rate': [0.01, 0.05]\n",
    "            }\n",
    "            gsearch = GridSearchCV(\n",
    "                stacking_model, param_grid, cv=tscv,\n",
    "                scoring='neg_mean_squared_error', verbose=0\n",
    "            )\n",
    "            gsearch.fit(X, y)\n",
    "\n",
    "            # Best params\n",
    "            best_model_params = gsearch.best_params_\n",
    "            xgb_params = {k.replace(\"xgb__\", \"\"): v for k, v in best_model_params.items() if k.startswith(\"xgb__\")}\n",
    "            lgbm_params = {k.replace(\"lgbm__\", \"\"): v for k, v in best_model_params.items() if k.startswith(\"lgbm__\")}\n",
    "\n",
    "            # Refit with best params\n",
    "            estimators_refit = [\n",
    "                ('xgb', xgb.XGBRegressor(random_state=42, verbosity=1, **xgb_params)),\n",
    "                ('lgbm', lgb.LGBMRegressor(random_state=42, **lgbm_params))\n",
    "            ]\n",
    "            model = StackingRegressor(estimators=estimators_refit, final_estimator=Ridge())\n",
    "            model.fit(X, y)\n",
    "\n",
    "            # Evaluate on last fold\n",
    "            train_idx, test_idx = list(tscv.split(X))[-1]\n",
    "            X_test, y_test = X[test_idx], y[test_idx]\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_test_inv = target_scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "            y_pred_inv = target_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
    "\n",
    "            mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "            smape_val = smape(y_test_inv, y_pred_inv)\n",
    "            r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "            peak_err = peak_error(y_test_inv, y_pred_inv)\n",
    "\n",
    "            # Log to MLflow\n",
    "            model_path = f\"{MODEL_DIR}/{junction}_{version}.pkl\"\n",
    "            with open(model_path,'wb') as f: pickle.dump(model,f)\n",
    "            mlflow.log_artifact(model_path)\n",
    "\n",
    "            mlflow.log_params({\n",
    "                \"algo\": f\"Stacking_{version}\",\n",
    "                \"junction\": junction,\n",
    "                \"window\": rolling_windows,\n",
    "                \"stride\": lag_steps,\n",
    "            })\n",
    "\n",
    "            mlflow.log_metrics({\n",
    "                \"MAE\": mae, \"RMSE\": rmse, \"SMAPE\": smape_val, \"R2\": r2, \"peak_error\": peak_err\n",
    "            })\n",
    "\n",
    "    # Save feature/target mapping\n",
    "    feature_target_path = f'{MODEL_DIR}/feature_and_target.json'\n",
    "    with open(feature_target_path,'w') as f:\n",
    "        json.dump(feature_and_target, f, indent=4)\n",
    "    mlflow.log_artifact(feature_target_path)\n",
    "\n",
    "print(\"✅ Best stacking model per junction logged with MLflow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284d6a1",
   "metadata": {},
   "source": [
    "Prophet (single junction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d882854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/Desktop/Digital_twin_project/digitaltwin/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_11861/2692068839.py:68: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_prophet = train_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "/tmp/ipykernel_11861/2692068839.py:69: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_prophet = test_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "13:09:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:09:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/tmp/ipykernel_11861/2692068839.py:68: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_prophet = train_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "/tmp/ipykernel_11861/2692068839.py:69: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_prophet = test_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "13:09:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:09:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/tmp/ipykernel_11861/2692068839.py:68: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_prophet = train_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "/tmp/ipykernel_11861/2692068839.py:69: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_prophet = test_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "13:09:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:09:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/tmp/ipykernel_11861/2692068839.py:68: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_prophet = train_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "/tmp/ipykernel_11861/2692068839.py:69: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_prophet = test_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "13:09:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:09:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/tmp/ipykernel_11861/2692068839.py:68: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_prophet = train_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "/tmp/ipykernel_11861/2692068839.py:69: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_prophet = test_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "13:09:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:09:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/tmp/ipykernel_11861/2692068839.py:68: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_prophet = train_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "/tmp/ipykernel_11861/2692068839.py:69: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_prophet = test_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "13:09:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:09:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/tmp/ipykernel_11861/2692068839.py:68: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_prophet = train_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "/tmp/ipykernel_11861/2692068839.py:69: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_prophet = test_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "13:09:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:09:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/tmp/ipykernel_11861/2692068839.py:68: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_prophet = train_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "/tmp/ipykernel_11861/2692068839.py:69: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_prophet = test_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "13:09:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:09:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/tmp/ipykernel_11861/2692068839.py:68: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_prophet = train_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "/tmp/ipykernel_11861/2692068839.py:69: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_prophet = test_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "13:09:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:09:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/tmp/ipykernel_11861/2692068839.py:68: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_prophet = train_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "/tmp/ipykernel_11861/2692068839.py:69: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_prophet = test_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
      "13:09:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:09:53 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prophet training with detailed parameter logging completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TMPDIR\"] = \"/mnt/data/home/zayd/tmp\"\n",
    "os.environ[\"TEMP\"] = \"/mnt/data/home/zayd/tmp\"\n",
    "os.environ[\"TMP\"] = \"/mnt/data/home/zayd/tmp\"\n",
    "\n",
    "import os, time, pickle, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import mlflow\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true = np.where(y_true==0, np.finfo(float).eps, y_true)\n",
    "    return np.mean(np.abs((y_true - y_pred)/y_true))*100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    denominator = np.where(denominator==0, np.finfo(float).eps, denominator)\n",
    "    return 100/len(y_true) * np.sum(2*np.abs(y_pred-y_true)/denominator)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_val = np.percentile(y_true, percentile)\n",
    "    peak_idx = y_true >= peak_val\n",
    "    if np.sum(peak_idx)==0: return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_idx]-y_pred[peak_idx]))\n",
    "\n",
    "# -----------------------------\n",
    "# Paths & parameters\n",
    "# -----------------------------\n",
    "root = '/mnt/data/home/zayd/Digital_twin_project/machine_learning/dataset/Ctown/junctions/'\n",
    "version = '0.1.0'\n",
    "MODEL_DIR = f'./model_trained/Prophet_{version}'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=['junction','MAE','RMSE','SMAPE','R2','Peak_Error'])\n",
    "feature_and_target = {}\n",
    "\n",
    "mlflow.set_experiment(\"Digital_Twin_Experiments\")\n",
    "\n",
    "# -----------------------------\n",
    "# Parent MLflow run\n",
    "# -----------------------------\n",
    "with mlflow.start_run(run_name=f\"Prophet_{version}\"):\n",
    "\n",
    "    for filename in os.listdir(root)[:10]:\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        df = df.sort_values(by=[\"scenario_id\", \"time_id\"]).reset_index(drop=True)\n",
    "        junction_cols = [col for col in df.columns if col.startswith('J')]\n",
    "\n",
    "        # Convert from m³/s to L/s\n",
    "        df[junction_cols] = df[junction_cols] * 1000\n",
    "        junction = os.path.splitext(filename)[0]\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"{junction}\", nested=True):\n",
    "            # -----------------------------\n",
    "            # Scenario-based split\n",
    "            # -----------------------------\n",
    "            scenario_ids = df['scenario_id'].unique()\n",
    "            split_idx = int(0.8*len(scenario_ids))\n",
    "            train_df = df[df['scenario_id'].isin(scenario_ids[:split_idx])]\n",
    "            test_df = df[df['scenario_id'].isin(scenario_ids[split_idx:])]\n",
    "\n",
    "            # Prepare Prophet data\n",
    "            train_prophet = train_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
    "            test_prophet = test_df[['time_id', junction]].rename(columns={'time_id':'ds', junction:'y'}).fillna(method='ffill')\n",
    "            feature_and_target[junction] = {\"target\": junction}\n",
    "\n",
    "            # -----------------------------\n",
    "            # Train Prophet\n",
    "            # -----------------------------\n",
    "            model = Prophet(daily_seasonality=False)\n",
    "            start_time = time.time()\n",
    "            model.fit(train_prophet)\n",
    "            elapsed_sec = time.time()-start_time\n",
    "\n",
    "            # Log detailed parameters\n",
    "            mlflow.log_params({\n",
    "                \"algo\": f\"Prophet_{version}\",\n",
    "                \"daily_seasonality\": False,\n",
    "                \"yearly_seasonality\": model.yearly_seasonality,\n",
    "                \"weekly_seasonality\": model.weekly_seasonality,\n",
    "                \"holidays\": bool(model.holidays),\n",
    "                \"changepoint_prior_scale\": model.changepoint_prior_scale,\n",
    "                \"seasonality_prior_scale\": model.seasonality_prior_scale,\n",
    "                \"training_time_sec\": elapsed_sec\n",
    "            })\n",
    "\n",
    "            # Forecast\n",
    "            future = test_prophet[['ds']]\n",
    "            forecast = model.predict(future)\n",
    "            y_pred = forecast['yhat'].values\n",
    "            y_true = test_prophet['y'].values\n",
    "\n",
    "            # Metrics\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "            smape_val = smape(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            peak_err = peak_error(y_true, y_pred)\n",
    "\n",
    "            # Log metrics for this junction\n",
    "            mlflow.log_metrics({\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                # 'MAPE': mape,\n",
    "                'SMAPE': smape_val,\n",
    "                'R2_score': r2,\n",
    "                'peak_error': peak_err\n",
    "            })\n",
    "\n",
    "            # Save model\n",
    "            model_path = os.path.join(MODEL_DIR, f\"{junction}.pkl\")\n",
    "            with open(model_path,'wb') as f: pickle.dump(model,f)\n",
    "            mlflow.log_artifact(model_path)\n",
    "\n",
    "    # Save feature/target mapping\n",
    "    with open(os.path.join(MODEL_DIR, 'feature_and_target.json'),'w') as f: json.dump(feature_and_target,f,indent=4)\n",
    "    mlflow.log_artifact(os.path.join(MODEL_DIR,'feature_and_target.json'))\n",
    "\n",
    "print(\"✅ Prophet training with detailed parameter logging completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c14358",
   "metadata": {},
   "source": [
    "Prophet (zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e9109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import os\n",
    "import pickle\n",
    "import mlflow\n",
    "import json\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true = np.where(y_true == 0, np.finfo(float).eps, y_true)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    denominator = np.where(denominator == 0, np.finfo(float).eps, denominator)\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / denominator)\n",
    "\n",
    "def peak_error(y_true, y_pred, percentile=95):\n",
    "    peak_value = np.percentile(y_true, percentile)\n",
    "    peak_indices = y_true >= peak_value\n",
    "    if np.sum(peak_indices) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_true[peak_indices] - y_pred[peak_indices]))\n",
    "\n",
    "# -----------------------------\n",
    "# Paths and parameters\n",
    "# -----------------------------\n",
    "root = '/mnt/data/home/zayd/Digital_twin_project/machine_learning/dataset/junctions_new/'\n",
    "version = '0.0.1'\n",
    "os.makedirs(f'./model_trained/Prophet_{version}', exist_ok=True)\n",
    "mlflow.set_tracking_uri(\"file:model_trained/mlruns\")\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=['junction', 'MAE', 'RMSE', 'MAPE', 'SMAPE', 'R2', 'Peak_Error'])\n",
    "feature_and_target = {}\n",
    "\n",
    "global_y_true, global_y_pred = [], []\n",
    "\n",
    "with mlflow.start_run(run_name=f\"Prophet_{version}\"):\n",
    "\n",
    "    for filename in os.listdir(root)[:70]:\n",
    "        df = pd.read_parquet(os.path.join(root, filename))\n",
    "        junction = os.path.splitext(filename)[0]\n",
    "\n",
    "        # -----------------------------\n",
    "        # Scenario-based train/test split\n",
    "        # -----------------------------\n",
    "        scenario_ids = df['scenario_id'].unique()\n",
    "        split_idx = int(0.8 * len(scenario_ids))\n",
    "        train_scenarios = scenario_ids[:split_idx]\n",
    "        test_scenarios = scenario_ids[split_idx:]\n",
    "\n",
    "        train_df = df[df['scenario_id'].isin(train_scenarios)]\n",
    "        test_df = df[df['scenario_id'].isin(test_scenarios)]\n",
    "\n",
    "        # Prepare for Prophet\n",
    "        train_prophet = train_df[['time_id', junction]].rename(columns={'time_id': 'ds', junction: 'y'})\n",
    "        test_prophet = test_df[['time_id', junction]].rename(columns={'time_id': 'ds', junction: 'y'})\n",
    "        train_prophet['y'] = train_prophet['y'].fillna(method='ffill')\n",
    "        test_prophet['y'] = test_prophet['y'].fillna(method='ffill')\n",
    "\n",
    "        feature_and_target[junction] = {\"target\": junction}\n",
    "\n",
    "        # -----------------------------\n",
    "        # Train Prophet\n",
    "        # -----------------------------\n",
    "        model = Prophet(daily_seasonality=False)\n",
    "        start_time = time.time()\n",
    "        model.fit(train_prophet)\n",
    "        elapsed_sec = time.time() - start_time\n",
    "        print(f\"{junction} training time: {elapsed_sec:.2f} sec\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Forecast test scenarios\n",
    "        # -----------------------------\n",
    "        future = test_prophet[['ds']]\n",
    "        forecast = model.predict(future)\n",
    "        y_pred = forecast['yhat'].values\n",
    "        y_true = test_prophet['y'].values\n",
    "\n",
    "        global_y_true.append(y_true)\n",
    "        global_y_pred.append(y_pred)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Metrics per junction\n",
    "        # -----------------------------\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "        smape_val = smape(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        peak_err = peak_error(y_true, y_pred)\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
    "            'junction': [junction],\n",
    "            'MAE': [mae],\n",
    "            'RMSE': [rmse],\n",
    "            'MAPE': [mape],\n",
    "            'SMAPE': [smape_val],\n",
    "            'R2': [r2],\n",
    "            'Peak_Error': [peak_err]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "        # Save model\n",
    "        with open(f'model_trained/Prophet_{version}/{junction}.pkl', 'wb') as f:\n",
    "            pickle.dump(model,f)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Global metrics\n",
    "    # -----------------------------\n",
    "    all_y_true = np.concatenate(global_y_true)\n",
    "    all_y_pred = np.concatenate(global_y_pred)\n",
    "\n",
    "    global_metrics = {\n",
    "        'MAE': mean_absolute_error(all_y_true, all_y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(all_y_true, all_y_pred)),\n",
    "        'MAPE': mean_absolute_percentage_error(all_y_true, all_y_pred),\n",
    "        'SMAPE': smape(all_y_true, all_y_pred),\n",
    "        'R2': r2_score(all_y_true, all_y_pred),\n",
    "        'Peak_Error': peak_error(all_y_true, all_y_pred)\n",
    "    }\n",
    "\n",
    "    print(\"\\n✅ Global Metrics:\")\n",
    "    for k,v in global_metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    mlflow.log_metrics(global_metrics)\n",
    "\n",
    "# Save feature/target mapping\n",
    "with open(f'model_trained/Prophet_{version}/feature_and_target.json', 'w') as f:\n",
    "    json.dump(feature_and_target, f, indent=4)\n",
    "\n",
    "print(\"✅ Prophet training completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digitaltwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
